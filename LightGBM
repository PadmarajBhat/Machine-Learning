* What is LightGBM?
   * it is from Microsoft (2017).
   * claims bettern than xgboost
      * it creates bin of the continuous variable and uses it to find the best split.
         * this not only increases the speed but also addresses the sparsity in the data.
* How is it different from xgboost? or is it better to xgboost ?
    * xgboost with parameter tree method="hist" can be comparable to that of lightGBM.
    * xgboost seems to have low prediction time but high training time when compared to that of lightGBM.
    
    * There is also a catboost (categorical boosting) but that has slow training time but outperforms in prediction time.
        *  https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/
        
    * nice article on parameter tuning of LGBM : https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc
    
    * Scikit API details : https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier.fit
        * supports evaluation set
        * class weight to address the case of imbalance in training data
        * categorical_feature: to indicate the categorical columns through the indices to the feature list. If NOT then the panda category information is inferred.

    * should the categorical features to be one hot encoded before being fed to LGBM ?
        * **Ans**:
        * in the traditional approach to create machine learning model, encoding is common practice. However, LGBM does not wants expects the same. However, the categorical columns has to be explicitly specified. : https://lightgbm.readthedocs.io/en/latest/Python-Intro.html
                * ```train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3'])```  

    
    * LightGBM supports libsvm, tsv, csv, txt, numpy (2d), panda, H2O DF and scipy sparse matrix
        * what is libsvm format? (I m guessing condensed for integer dataset)
            * **Ans**: Yes I was right in my guess. https://qr.ae/TWyxE1: It is format with which sparse entry can be done for a sample/observation. Missing Indexes are assumed to be 0.
                ```-1 5:1 7:1 14:1 19:1 39:1 40:1 51:1 63:1 67:1 73:1 74:1 76:1 78:1 83:1 ```
                    * Note that it starts with label and in the above case it is -1.
                    
        * what are differences between panda df and H2O df ?
          * **Ans** : they are almost the same except the fact that H2O df are subjected to lazy evaluation (like that of dask)
          
* Open ended question:
    * can xgboost be distributed ?
    * 
    * how does LGBM scale out?
        * single machine with multiple cpu ?
        * single machine with multiple cpu and gpu
        * multiple machines with multiple cpu and gpu
            * **Ans**:
            * Need to dig deep into https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html
            * Bit into theory:
                * data parallelism: data is split horizontally, so as to create bins. best split for the tree growth is taken from the recursive best fit and hence reducing the "reduce" efforts. Best suited for the large data model builiding.
                * feature parallelism: features are distributed among workers who communicate best splits with others. Suitable when features are considerable large. Hope there is a ratio which we determines the kind of parallelism to choose.
            
    * when it comes to catboost performance, how far is LGBM when data has categorical features ?
    
    * when do we use lightgbm client?

    * what is most voted, hottest and latest queries on lightgbm ?
