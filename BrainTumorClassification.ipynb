{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BrainTumorClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Machine-Learning/blob/master/BrainTumorClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCyCln9DwLS",
        "colab_type": "text"
      },
      "source": [
        "# Detection of 3 Brain Tumors (Meningioma, Glioma and Pituitary) in T1-weighted contrast enhanced images\n",
        "\n",
        "### - Revisitng the Udacity Capstone Project in pursuit of better accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExaNYOAl3nSH",
        "colab_type": "text"
      },
      "source": [
        "# What is the problem statement?\n",
        "  * predict the tumor class given a MRI image\n",
        "  * OR predict the tumor class when both MRI and Tumor region is given !!!\n",
        "      * tumor region is identified and put in input dataset by experts\n",
        "          * can we have Image Segmentation problem ?\n",
        "\n",
        "\n",
        "  * I think this is the order of problem from easy level to difficult level\n",
        "    * Identify the tumor class from raw MRI image (here accuracy may be low)\n",
        "    * Identify the tumor class from raw MRI image with tumor region identified info (here accuracy may be better)\n",
        "    * Auto detect the tumor segment in a MRI image and classify the tumor (ideal application for a radiologist)\n",
        "\n",
        "    Let us try all the 3 !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47aIRBi1lPdJ",
        "colab_type": "text"
      },
      "source": [
        "# Import Packages\n",
        "* read the input MRI images (.mat) files through ***h5py***\n",
        "* **bokeh** plot for the zoomed in analysis of a tumor and neighbors\n",
        "* ***pandas*** for data analysis and preprocessing\n",
        "* ***tensorflow*** for modelling and predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzYp6q2F5Doj",
        "colab_type": "code",
        "outputId": "32719178-6dc0-450f-ed12-471976c0ec49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XamypXiCEdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from bokeh.io import output_notebook, show\n",
        "from bokeh.layouts import row\n",
        "from bokeh.plotting import figure\n",
        "output_notebook()\n",
        "\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import random # for radom selection from a list\n",
        "import datetime\n",
        "import itertools\n",
        "import time\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quwfr-qMPrrs",
        "colab_type": "text"
      },
      "source": [
        "### Most important line in the program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqclQBxuMQ9m",
        "colab_type": "text"
      },
      "source": [
        " print(tf.keras.backend.floatx())\n",
        " tf.keras.backend.set_floatx(\"float64\")\n",
        " print(tf.keras.backend.floatx())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8abLHDC4ut4",
        "colab_type": "code",
        "outputId": "4fe978cb-5ea8-40ed-f9fd-01f20e3219d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E7e82UO_cLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a Global variable\n",
        "tumor_names = [\"\",\"Meningioma\",\"Glioma\",\"Pituitary\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68mR7i5xla0f",
        "colab_type": "text"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "* **Iteration 1**:\n",
        "    * Mount Google Drive\n",
        "    * Unzip it in colab disk\n",
        "    * load mat attributes to list of tuples ( with mri and tumor 5 point summary details)\n",
        "    * create a panda dataframe for analysis\n",
        "\n",
        "    ##### Issues Faced:\n",
        "    1. loading to panda with image took half(6GB) of RAM\n",
        "    * loading tumor along with mri image (as in mat file) crashed the colab\n",
        "      * Solution: let us load image but save only 5 point summary for both mri image and tumor\n",
        "\n",
        "    2. How do we scale/normalize the data?\n",
        "      * would tumor region have 0 in it ?\n",
        "        * only way to know is through the value present in the binary indicator == 1\n",
        "            * implementation through 2 for loops takes forever !!!\n",
        "              * need to implement throuhg np.where...a[a == 1] !!!\n",
        "            \n",
        "\n",
        "    3. Some images are less than 512\n",
        "        * pad the difference with 0s.\n",
        "        \n",
        "    4. Should tumor image be scaled between 0 -1? For now, brightness values are relative to that of the whole image to which it belongs to.\n",
        "\n",
        "    5. Epoch run failed due to no data generated by the custom generator.\n",
        "      * Going to try the ImageGenerator from the TF.\n",
        "\n",
        "\n",
        "* **Iteration 2** :\n",
        "  * ImageGenerator worked fine but the np to image conversion had used dtype of np.uint8 to avoid warning during saving to image. However, that lead to corrupt image and hence loss was more and accuracy was less.\n",
        "  * Generator built for the iteration is correct ?\n",
        "      * are *flips* is not damaging data\n",
        "\n",
        "* **Iteration 3** :\n",
        "  * validate the image augmentation in the ImageGenerator\n",
        "  * initial load the data was fine. zip based train and test failed\n",
        "\n",
        "* **Iteration 4**:\n",
        "  * open all mat(hdf5) files and load 5 point summary of mri and tumor to a panda df\n",
        "  * save all the numpy mri image array to training_data directory\n",
        "  * split the df to training : testing = 80 :20\n",
        "  * move the 20% of the testing to testing_data\n",
        "  * out of the 80% training data, move the 10% for the validation during training\n",
        "      * download\n",
        "        * 5\n",
        "        * b*.zip\n",
        "        * *.txt\n",
        "        * mat\n",
        "          * *.mat\n",
        "\n",
        "      * training_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "      * validation_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "      * testing_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6trYXPZPJ54V",
        "colab_type": "text"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWr8OsJ1ar6P",
        "colab_type": "text"
      },
      "source": [
        "##### Google Drive File Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWQcmRvRFD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf training_data validation_data testing_data download"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCtee7XQbSv",
        "colab_type": "code",
        "outputId": "c6a76d78-8036-4c68-a4ef-1f578e1288aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def loadMatFiles(dir=\"training_data/\"):\n",
        "  if not os.path.isfile(\"download/mat/1.mat\") and not os.path.isfile(\"training_data/npz/0.npz\"):\n",
        "    cmds = [\n",
        "            \"rm -rf download data training_data validation_data testing_data\"\n",
        "            ,\"wget https://ndownloader.figshare.com/articles/1512427/versions/5 -P download\"\n",
        "            ,\"unzip download/5 -d download\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_1-766.zip -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_1533-2298.zip -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_767-1532.zip   -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_2299-3064.zip  -d download/mat\"]\n",
        "\n",
        "    for c in cmds:\n",
        "      os.system(c)\n",
        "    time.sleep(5)\n",
        "  else:\n",
        "    print(\"mat files are loaded into download/mat directory\")\n",
        "\n",
        "loadMatFiles()\n",
        "print(\"Total Data : \", len(os.listdir(\"download/mat\")))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Data :  3064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUa5H7fsQkxk",
        "colab_type": "text"
      },
      "source": [
        "if not os.path.isdir(\"training_data/\"):\n",
        "\n",
        "  if not os.path.isdir(\"download/mat\") or not len(os.listdir(\"download/mat\")) :\n",
        "    loadMatFiles()\n",
        "\n",
        "  os.system(\"mkdir training_data validation_data testing_data\")\n",
        "\n",
        "  files = os.listdir(\"download/mat\").copy()\n",
        "  training_files = random.sample(files, k=round(len(files) *.7))\n",
        "\n",
        "  for file in training_files:\n",
        "    try:\n",
        "      os.rename(\"download/mat/\"+file, \"training_data/\"+file)\n",
        "    except:\n",
        "      print(\"Skipping :\", file)\n",
        "  time.sleep(5)\n",
        "\n",
        "print(\"Training Data:\", len(os.listdir(\"training_data/\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijgDKtrSQsgs",
        "colab_type": "text"
      },
      "source": [
        "files = os.listdir(\"download/mat\").copy()\n",
        "validation_files = random.sample(files, k=round(len(files) *.1))\n",
        "\n",
        "for file in validation_files:\n",
        "  try:\n",
        "    os.rename(\"download/mat/\"+file, \"validation_data/\"+file)\n",
        "  except:\n",
        "    print(\"Skipping :\", file)\n",
        "time.sleep(5)\n",
        "print(\"Validation Data:\", len(os.listdir(\"validation_data/\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_9jSTFhQyeR",
        "colab_type": "text"
      },
      "source": [
        "for file in os.listdir(\"download/mat/\"):\n",
        "      if not (file in os.listdir(\"training_data/\") or file in os.listdir(\"validation_data\")):\n",
        "        try:\n",
        "          os.rename(\"download/mat/\"+file, \"testing_data/\"+file)\n",
        "        except:\n",
        "          print(\"Skipping :\", file)\n",
        "time.sleep(5)\n",
        "print(\"Testing Data:\", len(os.listdir(\"testing_data/\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrXoZw8BxUR9",
        "colab_type": "text"
      },
      "source": [
        "def returnImageLabel(loc, file_list):\n",
        "  image_list=[]\n",
        "  label_list=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "\n",
        "  if not os.path.isdir(loc+\"images/\"):\n",
        "    os.system(\"mkdir \"+loc+\"images/\")\n",
        "    os.system(\"mkdir \"+loc+\"images/1/\")\n",
        "    os.system(\"mkdir \"+loc+\"images/2/\")\n",
        "    os.system(\"mkdir \"+loc+\"images/3/\")\n",
        "  for file_name in file_list:\n",
        "    \n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          \n",
        "          #image_list.append((image_array,  label_transform[label]))\n",
        "          image_list.append(image_array)\n",
        "          label_list.append(label_transform[label])\n",
        "          imageio.imwrite(loc+\"images/\"+str(label)+\"/\"+file_name.split(\".\")[0]+'.png', image_array)\n",
        "  return np.array(image_list), np.array(label_list)\n",
        "\n",
        "def npSaver(loc, batch_size=16):\n",
        "  \n",
        "  files = os.listdir(loc).copy()\n",
        "  os.system(\"mkdir \"+loc+\"npz/\")\n",
        "  for i in range(0,len(files),batch_size):\n",
        "    x,y = returnImageLabel(loc, files[i:i+batch_size])\n",
        "    print(loc+\"npz/\"+str(i)+\".npz\")\n",
        "    np.savez(loc+\"/npz/\"+str(i), x=x, y=y)\n",
        "    #time.sleep(1)\n",
        "    #np.save(loc+str(i),np.array(returnImageLabel(loc, files[i:i+32])))\n",
        "\n",
        "if not os.path.isfile(\"training_data/npz/0.npz\"):\n",
        "  npSaver(\"training_data/\")\n",
        "print(\"Training Batch Files :\", len(os.listdir(\"training_data/npz\")))\n",
        "\n",
        "if not os.path.isfile(\"validation_data/npz/0.npz\"):\n",
        "  npSaver(\"validation_data/\")\n",
        "print(\"Validation Batch Files :\", len(os.listdir(\"validation_data/npz\")))\n",
        "\n",
        "if not os.path.isfile(\"testing_data/npz/0.npz\"):\n",
        "  npSaver(\"testing_data/\")\n",
        "print(\"Testing Batch Files :\", len(os.listdir(\"testing_data/npz\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx62oEM3sneI",
        "colab_type": "text"
      },
      "source": [
        "#### MyGenerators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiGOCo1vjPS",
        "colab_type": "text"
      },
      "source": [
        "def myTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myValidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myCNNTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512,512,1)), data['y']\n",
        "\n",
        "def myCNNValidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512,512,1)), data['y']\n",
        "\n",
        "def myCNNTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512,512,1)), data['y']\n",
        "\n",
        "\n",
        "def myCNNtfrTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    images=[]\n",
        "    for i in range(data['x'].shape[0]):\n",
        "      image = np.stack((data['x'][i],)*3, axis=-1)\n",
        "      images.append(image)\n",
        "    #np.fromiter((np.stack((data['x'][i],np.zeros((512,512)),np.zeros((512,512))), axis=-1) for xi in data['x']), x.dtype)\n",
        "    yield np.array(images), data['y']\n",
        "\n",
        "def myCNNVtfralidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    images=[]\n",
        "    for i in range(data['x'].shape[0]):\n",
        "      #image = np.stack((data['x'][i],np.zeros((512,512)),np.zeros((512,512))), axis=-1)\n",
        "      image = np.stack((data['x'][i],)*3, axis=-1)\n",
        "      images.append(image)\n",
        "\n",
        "    yield np.array(images), data['y']\n",
        "\n",
        "def myCNNtfrTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    images=[]\n",
        "    for i in range(data['x'].shape[0]):\n",
        "      #image = np.stack((data['x'][i],np.zeros((512,512)),np.zeros((512,512))), axis=-1)\n",
        "      image = np.stack((data['x'][i],)*3, axis=-1)\n",
        "      images.append(image)\n",
        "\n",
        "    yield np.array(images), data['y']\n",
        "\n",
        "\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "train_data_gen = image_generator.flow_from_directory(directory=\"training_data/images\",\n",
        "                                                     batch_size=32,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(512, 512),\n",
        "                                                     #classes = list(CLASS_NAMES)\n",
        "                                                     )\n",
        "\n",
        "valid_data_gen = image_generator.flow_from_directory(directory=\"validation_data/images\",\n",
        "                                                     batch_size=32,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(512, 512),\n",
        "                                                     #classes = list(CLASS_NAMES)\n",
        "                                                     )\n",
        "\n",
        "test_data_gen = image_generator.flow_from_directory(directory=\"testing_data/images\",\n",
        "                                                     batch_size=32,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(512, 512),\n",
        "                                                     #classes = list(CLASS_NAMES)\n",
        "                                                     )\n",
        "\n",
        "\n",
        "\n",
        "def myBstrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield {\"image_array\":data['x'].reshape((data['x'].shape[0],512,512,1))}, data['y']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKq1Mw9UPaG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf training_data/npz validation_data/npz testing_Data/npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yFVzZWatfpR",
        "colab_type": "text"
      },
      "source": [
        "for i in myCNNtfrTestGenerator(16):\n",
        "  print(i[0].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv8YafkX9zoz",
        "colab_type": "text"
      },
      "source": [
        "print(plt.imread(\"training_data/images/1/1.png\").shape, type(plt.imread(\"training_data/images/1/1.png\")),plt.imread(\"training_data/images/1/1.png\").dtype)\n",
        "plt.imshow(plt.imread(\"training_data/images/1/1.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJ7KGrx9ZdE",
        "colab_type": "text"
      },
      "source": [
        "for i in train_data_gen:\n",
        "  print(i[0].shape)\n",
        "  print(i[0][0].max())\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNKyXmtUT5uW",
        "colab_type": "text"
      },
      "source": [
        "# TensorFunctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctgxgPDTaGGr",
        "colab_type": "code",
        "outputId": "6f8590ee-3efa-47e0-de5d-d33ae12108f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "with h5py.File(\"download/mat/1.mat\",'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          #image_array = image_array/image_array.max()\n",
        "          plt.imshow(image_array,cmap=\"bone\")\n",
        "          plt.show()\n",
        "          #print(np.resize(image_array,(128,128)).shape)\n",
        "          #imageio.imwrite(\"aaa.png\",np.resize(image_array,(128,128)))\n",
        "          #print(list(plt.imread(\"aaa.png\")[64]))\n",
        "          #print(list(np.resize(image_array,(128,128))))\n",
        "          imageio.imwrite(\"aaa.png\",np.array(tf.image.resize(np.stack((image_array,)*3,axis=-1),(128,128),method=\"nearest\")))\n",
        "          plt.imshow(plt.imread(\"aaa.png\"))\n",
        "          print(\"max value = \",np.array(tf.image.resize(np.stack((image_array,)*3,axis=-1),(128,128),method=\"nearest\")).max())\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWcE/O1PN1rH0T5YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0.0, 3317.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max value =  3317.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdJPCepdw1u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir cache cache/training_data/ cache/validation_data/ cache/testing_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOMgYzeAQmbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHAPE = 224\n",
        "batch_size = 64\n",
        "def loadImage(loc, file_name):\n",
        "    label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "    loc = loc.numpy().decode('utf-8')\n",
        "    file_name = file_name.numpy().decode('utf-8')\n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          \n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          image_array = np.array(tf.image.resize(np.stack((image_array,)*3,axis=-1),(SHAPE, SHAPE),method=\"nearest\"))\n",
        "          image_array = image_array/image_array.max()\n",
        "          return image_array, label_transform[label]\n",
        "\n",
        "dataset= tf.data.Dataset.from_tensor_slices(os.listdir(\"download/mat/\")).\\\n",
        "            map(lambda x: tf.py_function(func=loadImage,\n",
        "      inp=[\"download/mat/\",x], Tout=(tf.float64, tf.int16)))\n",
        "training_data = dataset.shuffle(3000).take(2500).batch(batch_size).prefetch(2)\n",
        "dataset = dataset.skip(2500)\n",
        "validation_data = dataset.take(200).batch(batch_size).prefetch(2)\n",
        "dataset = dataset.skip(200)\n",
        "testing_data = dataset.take(-1).repeat(1).batch(batch_size).prefetch(2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMPPBNmBlglA",
        "colab_type": "code",
        "outputId": "9afdd388-be7d-4132-bb10-8c7cd5bd2043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in testing_data.take(1):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: id=24514, shape=(16, 224, 224, 3), dtype=float64, numpy=\n",
            "array([[[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.00169722, 0.00169722, 0.00169722],\n",
            "         [0.00373388, 0.00373388, 0.00373388],\n",
            "         [0.00373388, 0.00373388, 0.00373388],\n",
            "         ...,\n",
            "         [0.00373388, 0.00373388, 0.00373388],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.00135777, 0.00135777, 0.00135777],\n",
            "         [0.00678887, 0.00678887, 0.00678887],\n",
            "         [0.00577054, 0.00577054, 0.00577054],\n",
            "         ...,\n",
            "         [0.00509165, 0.00509165, 0.00509165],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.00203666, 0.00203666, 0.00203666],\n",
            "         [0.00577054, 0.00577054, 0.00577054],\n",
            "         [0.00746775, 0.00746775, 0.00746775],\n",
            "         ...,\n",
            "         [0.00610998, 0.00610998, 0.00610998],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00441276, 0.00441276, 0.00441276],\n",
            "         [0.00543109, 0.00543109, 0.00543109],\n",
            "         ...,\n",
            "         [0.00848608, 0.00848608, 0.00848608],\n",
            "         [0.00475221, 0.00475221, 0.00475221],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00441276, 0.00441276, 0.00441276],\n",
            "         [0.00678887, 0.00678887, 0.00678887],\n",
            "         ...,\n",
            "         [0.01086219, 0.01086219, 0.01086219],\n",
            "         [0.00339443, 0.00339443, 0.00339443],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00271555, 0.00271555, 0.00271555],\n",
            "         [0.00305499, 0.00305499, 0.00305499],\n",
            "         ...,\n",
            "         [0.00339443, 0.00339443, 0.00339443],\n",
            "         [0.00169722, 0.00169722, 0.00169722],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.00041841, 0.00041841, 0.00041841],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.00041841, 0.00041841, 0.00041841],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0.        , 0.        , 0.        ],\n",
            "         [0.00929152, 0.00929152, 0.00929152],\n",
            "         [0.01045296, 0.01045296, 0.01045296],\n",
            "         ...,\n",
            "         [0.01006581, 0.01006581, 0.01006581],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00658149, 0.00658149, 0.00658149],\n",
            "         [0.01664731, 0.01664731, 0.01664731],\n",
            "         ...,\n",
            "         [0.01316299, 0.01316299, 0.01316299],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.01122726, 0.01122726, 0.01122726],\n",
            "         [0.01277584, 0.01277584, 0.01277584],\n",
            "         ...,\n",
            "         [0.01355014, 0.01355014, 0.01355014],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.0116144 , 0.0116144 , 0.0116144 ],\n",
            "         [0.01509872, 0.01509872, 0.01509872],\n",
            "         ...,\n",
            "         [0.01316299, 0.01316299, 0.01316299],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00774293, 0.00774293, 0.00774293],\n",
            "         [0.01509872, 0.01509872, 0.01509872],\n",
            "         ...,\n",
            "         [0.01548587, 0.01548587, 0.01548587],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00619435, 0.00619435, 0.00619435],\n",
            "         [0.01045296, 0.01045296, 0.01045296],\n",
            "         ...,\n",
            "         [0.00929152, 0.00929152, 0.00929152],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]]])>, <tf.Tensor: id=24515, shape=(16, 3), dtype=int16, numpy=\n",
            "array([[1, 0, 0],\n",
            "       [0, 0, 1],\n",
            "       [0, 0, 1],\n",
            "       [1, 0, 0],\n",
            "       [0, 1, 0],\n",
            "       [1, 0, 0],\n",
            "       [1, 0, 0],\n",
            "       [0, 1, 0],\n",
            "       [0, 1, 0],\n",
            "       [0, 1, 0],\n",
            "       [0, 0, 1],\n",
            "       [1, 0, 0],\n",
            "       [1, 0, 0],\n",
            "       [1, 0, 0],\n",
            "       [0, 1, 0],\n",
            "       [1, 0, 0]], dtype=int16)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21SFjMn3g7ue",
        "colab_type": "text"
      },
      "source": [
        "print(os.listdir(\"training_data\"))\n",
        "train_data= tf.data.Dataset.from_tensor_slices(os.listdir(\"training_data\"))\n",
        "for i in train_data:\n",
        "  print(i)\n",
        "  break\n",
        "train_data = train_data.skip(1000)\n",
        "for i in train_data:\n",
        "  print(i)\n",
        "  break\n",
        "train_data = train_data.skip(1000)\n",
        "for i in train_data:\n",
        "  print(i)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZfJOqJ16mbg",
        "colab_type": "text"
      },
      "source": [
        "[os.system(\"rm -rf \"+\"training_data/\"+f) for f in os.listdir(\"training_data/\") if \".np\" in f or \".csv\" in f]\n",
        "[os.system(\"rm -rf \"+\"validation_data/\"+f) for f in os.listdir(\"validation_data/\") if \".np\" in f or \".csv\" in f]\n",
        "[os.system(\"rm -rf \"+\"testing_data/\"+f) for f in os.listdir(\"testing_data/\") if \".np\" in f or \".csv\" in f]\n",
        "!rm -rf \"training_data/npz validation_data/npz testing_data/npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyh_sPtY7UMy",
        "colab_type": "text"
      },
      "source": [
        "### Load Image Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoGqScRpu7F5",
        "colab_type": "text"
      },
      "source": [
        "def retrieveImage(file_name):\n",
        "  f = h5py.File(file_name,'r')\n",
        "  mri_image = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return mri_image/mri_image.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_gT4oC7bYb",
        "colab_type": "text"
      },
      "source": [
        "### Load Tumor Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcPw_HqJ7pkX",
        "colab_type": "text"
      },
      "source": [
        "def retrieveTumorImage(file_name):\n",
        "  f = h5py.File(file_name,'r')\n",
        "  mri_image = np.array(f['cjdata']['tumorMask'],dtype=np.float128)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return mri_image/mri_image.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-c5y3LXGnCE",
        "colab_type": "text"
      },
      "source": [
        "### Create Directories for ImageGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry3jlMuXKdmD",
        "colab_type": "text"
      },
      "source": [
        "!mkdir \"training_data/images/\"\n",
        "!mkdir \"training_data/images/1\"\n",
        "!mkdir \"training_data/images/2\"\n",
        "!mkdir \"training_data/images/3\"\n",
        "\n",
        "!mkdir \"testing_data/images/\"\n",
        "!mkdir \"testing_data/images/1\"\n",
        "!mkdir \"testing_data/images/2\"\n",
        "!mkdir \"testing_data/images/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF_-ayUh7jlS",
        "colab_type": "text"
      },
      "source": [
        "### Load Image and Tumor Statistics to Panda\n",
        "\n",
        "*   Panda df would have 5 point summary of both mri and tumor\n",
        "*   data directory will have label wise subdirectories for ImageGenerator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUvYbhjEzet",
        "colab_type": "text"
      },
      "source": [
        "def return_imageInfo_from_mat_file(dir,file_name):\n",
        "    f = h5py.File(dir+file_name,'r')\n",
        "\n",
        "    mri_image = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "    #scaler = MinMaxScaler(feature_range=(1,2))\n",
        "    #mri_image = scaler.fit(mri_image)\n",
        "    mri_image = mri_image/mri_image.max()\n",
        "\n",
        "    if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "    \n",
        "    temp_mri_image = np.copy(mri_image)\n",
        "    temp_mri_image[temp_mri_image == 0 ] = 2\n",
        "\n",
        "    mri_quartiles = np.percentile(mri_image[mri_image > 0], [25, 50, 75,80,85,90,95,96,97,98,99])\n",
        "\n",
        "    tumor_image = np.array(f['cjdata']['tumorMask'], dtype=np.float128)\n",
        "    if tumor_image.shape[0] < 512:\n",
        "      print(\"Shape of the tumor image : \", tumor_image.shape)\n",
        "      tumor_image = np.pad(tumor_image,(512 - tumor_image.shape[0])//2,'constant',constant_values=0)\n",
        "    \n",
        "    tumor_image = temp_mri_image * tumor_image\n",
        "    tumor_image = tumor_image[tumor_image > 0]\n",
        "    tumor_image[tumor_image == 2] = 0\n",
        "\n",
        "    '''tumor_array =[]\n",
        "    for i in range(0,512):\n",
        "      for j in range(0,512):\n",
        "        if tumor_image[i][j]:\n",
        "          tumor_array.append(mri_image[i][j])\n",
        "\n",
        "    tumor_image = np.array(tumor_array, dtype=np.float)'''\n",
        "\n",
        "    tumor_quartiles = np.percentile(tumor_image, [25, 50, 75,80,85,90,95,96,97,98,99])\n",
        "\n",
        "    label=np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "    imageio.imwrite(dir+\"images/\"+str(label)+\"/\"+file_name.split(\".\")[0]+'.jpg', np.array(f['cjdata']['image'],dtype=np.int16))\n",
        "\n",
        "    return np.array(f['cjdata']['PID'],dtype=np.int)[0][0] \\\n",
        "            ,mri_image.min() \\\n",
        "            ,mri_image.max() \\\n",
        "            ,mri_quartiles[0] \\\n",
        "            ,mri_quartiles[1] \\\n",
        "            ,mri_quartiles[2] \\\n",
        "            ,mri_quartiles[3] \\\n",
        "            ,mri_quartiles[4] \\\n",
        "            ,mri_quartiles[5] \\\n",
        "            ,mri_quartiles[6] \\\n",
        "            ,mri_quartiles[7] \\\n",
        "            ,mri_quartiles[8] \\\n",
        "            ,mri_quartiles[9] \\\n",
        "            ,mri_quartiles[10] \\\n",
        "            ,tumor_image.min() \\\n",
        "            ,tumor_image.max() \\\n",
        "            ,tumor_quartiles[0] \\\n",
        "            ,tumor_quartiles[1] \\\n",
        "            ,tumor_quartiles[2] \\\n",
        "            ,tumor_quartiles[3] \\\n",
        "            ,tumor_quartiles[4] \\\n",
        "            ,tumor_quartiles[5] \\\n",
        "            ,tumor_quartiles[6] \\\n",
        "            ,tumor_quartiles[7] \\\n",
        "            ,tumor_quartiles[8] \\\n",
        "            ,tumor_quartiles[9] \\\n",
        "            ,tumor_quartiles[10] \\\n",
        "            ,tumor_image.shape \\\n",
        "            ,file_name\\\n",
        "            ,np.array(f['cjdata']['label'], dtype=np.int)[0][0] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h9X8MvPNy50",
        "colab_type": "text"
      },
      "source": [
        "def loadDf(dir=\"training_data/\"):\n",
        "  patients_details = []\n",
        "  '''for root, dirs, files in os.walk(\"/content/drive/My Drive/1512427/\", topdown = False):\n",
        "    for f in files:\n",
        "      if \".zip\" in f:\n",
        "          file = zipfile.ZipFile(root+f, \"r\")\n",
        "          for name in file.namelist():\n",
        "            file.extract(name,\".\")\n",
        "            patients_details.append(return_imageInfo_from_mat_file(name))\n",
        "          #break\n",
        "      #break  '''   \n",
        "  \n",
        "  for f in getFileNames(dir):\n",
        "    if \".mat\" in f:\n",
        "      patients_details.append(return_imageInfo_from_mat_file(dir,f))\n",
        "  mri_col_names = [\"mri_min\",\"mri_max\",\"mri_1q\",\"mri_median\", \"mri_3q\",\"mri_80\",\"mri_85\",\"mri_90\",\"mri_95\",\"mri_96\",\"mri_97\",\"mri_98\",\"mri_99\"]\n",
        "  tumor_col_names = [\"t_min\",\"t_max\",\"t_1q\",\"t_median\",\"t_3q\",\"t_80\",\"t_85\",\"t_90\",\"t_95\",\"t_96\",\"t_97\",\"t_98\",\"t_99\",\"tumor_size\"]\n",
        "  col_names = [\"pid\"] + mri_col_names + tumor_col_names+ [\"file_name\",\"label\"]\n",
        "  return pd.DataFrame(patients_details,columns=col_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooal44AcufXG",
        "colab_type": "text"
      },
      "source": [
        "### Load Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCSo34a4Vlta",
        "colab_type": "text"
      },
      "source": [
        "df = loadDf()\n",
        "df[\"square_shape\"] = df.tumor_size.apply(lambda x: np.sqrt(x[0]))\n",
        "df.sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-G7l5uvulcY",
        "colab_type": "text"
      },
      "source": [
        "### Load Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upUrUpLMunt1",
        "colab_type": "text"
      },
      "source": [
        "df_test = loadDf(\"testing_data/\")\n",
        "df_test[\"square_shape\"] = df_test.tumor_size.apply(lambda x: np.sqrt(x[0]))\n",
        "df_test.sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCGj95Q6u1eC",
        "colab_type": "text"
      },
      "source": [
        "### Test the loaded data (both Training and Testing Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBtoXDYUu7Zc",
        "colab_type": "text"
      },
      "source": [
        "def displayNpImages(dir=\"training_data/\"):\n",
        "  for i in range(1,4):\n",
        "    print(\"2 samples for \",tumor_names[i])\n",
        "    for j in random.choices( os.listdir(dir+str(i)),k=2):\n",
        "      plt.imshow(plt.imread(dir+str(i)+\"/\"+str(j)))\n",
        "      plt.show()\n",
        "\n",
        "#displayNpImages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udE3comcx3Oe",
        "colab_type": "text"
      },
      "source": [
        "#### Visual Testing Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHWSr83x8mO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displayNpImages(\"testing_data/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJc8C31m9Efe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displayNpImages(\"validation_data/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeU2lq6Lzb_-",
        "colab_type": "text"
      },
      "source": [
        "def analyzeZipDir(df,start,end):\n",
        "  df[\"file_num\"] = df.file_name.apply(lambda x: x.split(\".\")[0])\n",
        "  df[\"file_num\"] = df.file_num.astype(np.int)\n",
        "  \n",
        "  return df[df.file_num.isin(list(range(start,end)))]\n",
        "analyzeZipDir(df.copy(),1,766).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UnHytYu1RhE",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df.copy(),1533,2298).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0By0gpPh1W8R",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df_test.copy(),2299,3064).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHedgA0R2cy9",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df.copy(),767,1532).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-XfW7wbi2n",
        "colab_type": "text"
      },
      "source": [
        "### Create Test directory for validation through generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpZ1DnxSFdz",
        "colab_type": "text"
      },
      "source": [
        "!rm -rf \"test\"\n",
        "!mkdir \"test\"\n",
        "!mkdir \"test/1\"\n",
        "!mkdir \"test/2\"\n",
        "!mkdir \"test/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPpIuurTUKt",
        "colab_type": "text"
      },
      "source": [
        "!ls -l /content/data/2/3046.jpg\n",
        "!ls -l /content/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhVfTlhdXtG-",
        "colab_type": "text"
      },
      "source": [
        "!ls -l /content/data/2/2404.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GLeYdm-cWAU",
        "colab_type": "text"
      },
      "source": [
        "### Move the test set to test directory from data directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMyBzfxxP07c",
        "colab_type": "text"
      },
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/data\", topdown = False):\n",
        "  \n",
        "    \n",
        "    if len(files) > 0:\n",
        "      print(root, dirs, files)\n",
        "\n",
        "      #indices = np.random.randint(0,len(files),size=round(len(files)*.2))\n",
        "      rand_files = random.choices(files,k=round(len(files)*.2))\n",
        "      \n",
        "      for f in rand_files:\n",
        "        #print(f)\n",
        "        try:\n",
        "          shutil.move(root+\"/\"+f, \"/content/test/\"+root.split(\"/\")[-1]+\"/\"+f)\n",
        "        except :\n",
        "          print(\"Ignoring : \",f)\n",
        "\n",
        "#list(os.walk(\"/content/data\")) /content/test /content/training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-eDxHYafKUK",
        "colab_type": "text"
      },
      "source": [
        "### ImageGenerators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pyYghx5gdCe",
        "colab_type": "text"
      },
      "source": [
        "### Load tf.data\n",
        "* inspired from: https://stackoverflow.com/questions/48309631/tensorflow-tf-data-dataset-reading-large-hdf5-files\n",
        "* extract all files\n",
        "* save file names in a tensor\n",
        "* write a generator function to read a file and return numpy mri image array and its label\n",
        "* using tf.dataset.interleave function, read the file concurrently\n",
        "* apply batch and shuffle\n",
        "* feed it to model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77irdWdhtaxN",
        "colab_type": "text"
      },
      "source": [
        "##### Download the MRI image zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgd4_g2h7rYa",
        "colab_type": "text"
      },
      "source": [
        "def mygenerator(file_name, dir=\"training_data/\"):\n",
        "  #print(file_name)\n",
        "  f = h5py.File(dir+file_name,'r')\n",
        "  #print(f['cjdata']['image'].dtype)\n",
        "  mri_image = np.array(f['cjdata']['image'],dtype=np.int16)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      #print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return (mri_image, np.array(f['cjdata']['label'], dtype=np.int)[0][0])\n",
        "\n",
        "\n",
        "df_temp = pd.DataFrame([mygenerator(f) for f in getFileNames()],columns=[\"image\",\"label\"])\n",
        "\n",
        "'''ds = tf.data.Dataset.from_tensor_slices([  tf.data.Dataset.from_generator(\n",
        "        mygenerator(filename), \n",
        "        (tf.uint8,tf.int8), \n",
        "        (tf.TensorShape([]), tf.TensorShape([None]))) for filename in ds])'''\n",
        "df_temp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4-RohAP6hK",
        "colab_type": "text"
      },
      "source": [
        "imageio.imwrite(\"test.jpg\",df_temp.iloc[0][0])\n",
        "plt.imshow(imageio.imread(\"test.jpg\"),cmap='bone')\n",
        "plt.show()\n",
        "plt.imshow(plt.imread(\"test.jpg\"))\n",
        "plt.show()\n",
        "plt.imshow(df_temp.iloc[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgjK1-K4N8Ue",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(np.array(df_temp.iloc[0][0],dtype=np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLreCMdqiiAq",
        "colab_type": "text"
      },
      "source": [
        "class ToTensor:\n",
        "  def __init__(self):\n",
        "    print(\"Obj Created\")\n",
        "  \n",
        "  def unzipData(self,source_dirname, dest_dir=\"data/\"):\n",
        "    try:\n",
        "      shutil.rmtree(dest_dir)\n",
        "    except:\n",
        "      pass\n",
        "    os.mkdir(dest_dir)\n",
        "    print(\"Exploring \",source_dirname, \"directory\")\n",
        "    for root, dirs, files in os.walk(source_dirname, topdown = False):\n",
        "      for f in files:\n",
        "        print(\"Found file \",f)\n",
        "        if \".zip\" in f:\n",
        "          print(\"Unzipping \", f)\n",
        "          with zipfile.ZipFile(f) as zf:\n",
        "            zf.extractall(dest_dir)\n",
        "    print(\"Files Loaded !!!\")\n",
        "    return self\n",
        "\n",
        "  def getFileName(self,dir_name=\"data/\"):\n",
        "    return os.listdir(dir_name)\n",
        "\n",
        "toTensor = ToTensor()\n",
        "toTensor.unzipData(\"/content/drive/My Drive/1512427/\").getFileName()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0KYEepMovPx",
        "colab_type": "text"
      },
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tqRGoUK5APW",
        "colab_type": "text"
      },
      "source": [
        "## Statistical Analysis\n",
        "* Number of patients in the dataset\n",
        "* Patient wise distribution of tumor classes\n",
        "* Comparison of below attributes for the 3 tumor classes\n",
        "  * 1st quantile of MRI image\n",
        "  * Median of the MRI image\n",
        "  * 3rd quantile of the MRI image\n",
        "  * min value distribution of the Tumor\n",
        "  * max value distribution of the Tumor\n",
        "  * 1st quantile of the Tumor\n",
        "  * median of the Tumor\n",
        "  * 3rd quantile of the Tumor\n",
        "    * Analysis: \n",
        "      * All tumors have darkest area which may indicate the tumor itself\n",
        "      * All tumors have uniform distribution of brightness (apart from the dark area)\n",
        "      * MRI images have darker area outside the skull (non scan area)\n",
        "          * will this influence the model ?\n",
        "          * should the color of the tumor and the non important area of the MRI scan be different ?\n",
        "          \n",
        "* 256x256 image size distribution (any bias in there ?)\n",
        "\n",
        "##### Issues Faced:\n",
        "* Bokeh plots are interactive but they consume a lot of space(>100mb) in the notebook \n",
        "  * markdown for now, when interested can be seen by enabling it as code cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aumAqfm3VymM",
        "colab_type": "text"
      },
      "source": [
        "df.pid.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXwJqH_jnfor",
        "colab_type": "text"
      },
      "source": [
        "There are only 5 patients info present !!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvBti1CInkCG",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"pid\").agg(\"count\").reset_index()[['pid','mri_min']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-95548oIsq",
        "colab_type": "text"
      },
      "source": [
        "df.groupby([\"pid\",\"label\"]).agg(\"count\").reset_index()[['pid','label','mri_min']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z0gzrC-qPjs",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"label\").agg(\"count\").reset_index()[['label','pid']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCP1RnBrn2n",
        "colab_type": "text"
      },
      "source": [
        "def plotStatistics(df, tumor_name):\n",
        "  df = df[[\"mri_1q\",\"mri_median\",\"mri_3q\",\"t_min\",\"t_1q\",\"t_median\",\"t_3q\",\"t_max\"]]\n",
        "  df=(df-df.min())/(df.max()-df.min())\n",
        "  fig, ax = plt.subplots(1, 8,sharex=True,sharey=True,tight_layout=True)\n",
        "  fig.set_figheight(4)\n",
        "  fig.set_figwidth(13)\n",
        "  \n",
        "  fig.suptitle(tumor_name+\" Tumor\")\n",
        "  #plt.subplot(1,8,1)\n",
        "  ax[0].hist(df.mri_1q.tolist())\n",
        "  ax[0].set_title(\"mri_1q\")\n",
        "  #plt.subplot(1,8,2)\n",
        "  ax[1].hist(df.mri_median.tolist())\n",
        "  ax[1].set_title(\"mri_median\")\n",
        "  #plt.subplot(1,8,3)\n",
        "  ax[2].hist(df.mri_3q.tolist())\n",
        "  ax[2].set_title(\"mri_3q\")\n",
        "  #plt.subplot(1,8,4)\n",
        "  ax[3].hist(df.t_min.tolist())\n",
        "  ax[3].set_title(\"t_min\")\n",
        "  #plt.subplot(1,8,5)\n",
        "  ax[4].hist(df.t_1q.tolist())\n",
        "  ax[4].set_title(\"t_1q\")\n",
        "  #plt.subplot(1,8,6)\n",
        "  ax[5].hist(df.t_median.tolist())\n",
        "  ax[5].set_title(\"t_median\")\n",
        "  #plt.subplot(1,8,7)\n",
        "  ax[6].hist(df.t_3q.tolist())\n",
        "  ax[6].set_title(\"t_3q\")\n",
        "  #plt.subplot(1,8,8)\n",
        "  ax[7].hist(df.t_max.tolist())\n",
        "  ax[7].set_title(\"t_max\")\n",
        "  plt.show()\n",
        "\n",
        "plotStatistics(df[df.label ==1], tumor_names[1])\n",
        "plotStatistics(df[df.label ==2], tumor_names[2])\n",
        "plotStatistics(df[df.label ==3], tumor_names[3])\n",
        "plotStatistics(df, \"All\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW6obIAoho0m",
        "colab_type": "text"
      },
      "source": [
        "## Can we reduce the image size ?\n",
        "* Why?\n",
        "  * faster model building\n",
        "  * lower convolution experiment iterations\n",
        "  * lower ram usage and hence higher batch size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQYMkEwLlfPv",
        "colab_type": "text"
      },
      "source": [
        "### Approach 1 : Can we segregate the skull ?\n",
        "  * removing the unwanted area \n",
        "    * percentile approach: identify the percentile and see if any of the tumor percentile is always less that the MRI percentile. \n",
        "        i.e. to prove mri_99 > t_99. This failed as indicated below\n",
        "    * brightness based skull identification:\n",
        "      * nearest neighbor ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H8qcdkeIFyo",
        "colab_type": "text"
      },
      "source": [
        "df[df.mri_99 < df.t_99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAI0eeqfjUZi",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(\"120.mat\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCNwwdLSoLjt",
        "colab_type": "text"
      },
      "source": [
        "### Approach 2: PCA\n",
        "* Note that if we do the PCA transformation, we 2D image will be reduced to 1D. Therefore, we can not use it for the Convolution approach.\n",
        "  * we can note here that just by 2 features (components) and the trained PCA model, we are able to recreate the image with not much difference. [See the last 3 images] This showcases the PCA strength.\n",
        "\n",
        "* Withhelding the PCA approach as we are going to pursuit the Convolution \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOkU29MBoSzg",
        "colab_type": "text"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=5,whiten=True)\n",
        "image=[]\n",
        "image.append(retrieveImage(\"120.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"1.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"2.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"3.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"4.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"5.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"6.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"7.mat\").reshape(-1))\n",
        "\n",
        "#print(image.shape)\n",
        "\n",
        "pca.fit(image)\n",
        "plt.imshow(pca.mean_.reshape((512,512)),\n",
        "           cmap=plt.cm.bone)\n",
        "plt.show()\n",
        "\n",
        "print(pca.noise_variance_)\n",
        "print(image[0].reshape((1,-1)).shape)\n",
        "pca.transform(image[0].reshape((1,-1)))\n",
        "\n",
        "#plt.imshow(pca.transform(image[1].reshape(1,-1)).reshape((512,512)),cmap=plt.cm.bone)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ptnmf6M4Phe",
        "colab_type": "text"
      },
      "source": [
        "components = pca.transform(image[0].reshape(1,-1))\n",
        "projected = pca.inverse_transform(components)\n",
        "plt.imshow(projected.reshape((512,512)))\n",
        "plt.show()\n",
        "plt.imshow(image[0].reshape((512,512)))\n",
        "plt.show()\n",
        "plt.imshow(retrieveImage(\"120.mat\"))\n",
        "plt.show()\n",
        "print(\"doe it match :\", projected.reshape((512,512)) == retrieveImage(\"120.mat\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmKL2wU4eHoA",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(image[0].reshape((256,256)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NZjperm5HpZ",
        "colab_type": "text"
      },
      "source": [
        "## Visual Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYCnbIgH360L",
        "colab_type": "text"
      },
      "source": [
        "### Smallest Tumor Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH9v4orNwTFk",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(list(df[df.tumor_size == df.tumor_size.min()]['file_name'])[0]));\n",
        "plt.imshow(retrieveTumorImage(list(df[df.tumor_size == df.tumor_size.min()]['file_name'])[0]),alpha=0.5);\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj_utuj44fUp",
        "colab_type": "text"
      },
      "source": [
        "### Biggest Tumor in the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0btsuHfkxfty",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]));\n",
        "plt.imshow(retrieveTumorImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]),alpha=0.5);\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Jgv6oj30uI",
        "colab_type": "text"
      },
      "source": [
        "### Numpy Resize failed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aYJNq38zCRL",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(np.resize(retrieveImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]),(256,256)));\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJy0WXR3A_Td",
        "colab_type": "text"
      },
      "source": [
        "### Bokeh Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqrwFc48BCh9",
        "colab_type": "text"
      },
      "source": [
        "def bokehPlot(file_name, tumor_label):\n",
        "  tumor_names = [\"\",\"Meningioma\",\"Glioma\",\"Pituitary\"]\n",
        "  im = retrieveImage(file_name)\n",
        "  s1 = figure(width=512, plot_height=512, title=tumor_names[tumor_label]+\" MRI Image\")\n",
        "  s1.image([im],x=[0],y=[0],dw=[512],dh=[512])\n",
        "\n",
        "  im2 = retrieveTumorImage(file_name)\n",
        "\n",
        "  s2 = figure(width=500, plot_height=500, title=tumor_names[tumor_label]+\" MRI Image with Tumor Highlighted\")\n",
        "  s2.image([im2],x=[0],y=[0],dw=[512],dh=[512])\n",
        "  s2.image([im],x=[0],y=[0],dw=[512],dh=[512],global_alpha=0.5)\n",
        "\n",
        "  show(row(s1,s2))\n",
        "\n",
        "bokehPlot(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0], list(df[df.tumor_size == df.tumor_size.max()]['label'])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ0e1zAWWE4l",
        "colab_type": "text"
      },
      "source": [
        "#### Meningioma Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9dliKZMWINQ",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 1].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pP6euXXWqpU",
        "colab_type": "text"
      },
      "source": [
        "#### Glioma Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22qcn6KiWvMJ",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 2].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_3qNVQqV9A8",
        "colab_type": "text"
      },
      "source": [
        "#### Pituitary Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8P5A-cQO79W",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 3].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdvYCpyo2vC",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6QgmzMKv6Hw",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing ideas:\n",
        "\n",
        "1.  Dataset has tumor region indicator which would allow us to get the average brightness of the area.\n",
        "\n",
        "2. It is said that brightest region is skull and skull is not important for the tumor detection. It is only brain position determines the tumor class. If we remove skull remaining image is brain ?\n",
        "\n",
        "3. if we start with a window of image which would maximize the presence of tumor and expand to include some brain region around the tumor then i guess it is the best data for training(and predicting). Because tumor position in brain is THE factor that decides the tumor class.\n",
        "\n",
        "4. what is the optimum batch size for training?\n",
        "\n",
        "5. what is the overall Image augumented training dataset size ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDE9zmipLimp",
        "colab_type": "text"
      },
      "source": [
        "## Train & Test split\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dA-tbvOLt9b",
        "colab_type": "text"
      },
      "source": [
        "def getSplit(df):\n",
        "  df_test=df.sample(frac=.2)\n",
        "  df = df.drop(df_test.index)\n",
        "  return df, df_test\n",
        "\n",
        "df_orig = df.copy()\n",
        "df,df_test = getSplit(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGWNu-Zpk04v",
        "colab_type": "text"
      },
      "source": [
        "## Batch Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvedxzrclan5",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"label\").agg(\"count\").reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81fcRggmMy5",
        "colab_type": "text"
      },
      "source": [
        "def returnBatchIndices(df,batch_size):\n",
        "  label_1 = df[df.label == 1].index.tolist()\n",
        "  label_2 = df[df.label == 2].index.tolist()\n",
        "  label_3 = df[df.label == 3].index.tolist()\n",
        "\n",
        "  label_list = []\n",
        "  #print(len(label_1), len(label_2),len(label_3),list(range(0,max(len(label_1),len(label_2),len(label_3)),batch_size)))\n",
        "  for i in range(0,max(len(label_1),len(label_2),len(label_3)),batch_size):\n",
        "    label_list.append(label_1[i:i+batch_size] + label_2[i:i+batch_size] + label_3[i:i+batch_size])\n",
        "  return label_list\n",
        "\n",
        "#yieldbatch(df,5)\n",
        "for batch in returnBatchIndices(df,5):\n",
        "  print(batch)\n",
        "  break\n",
        "\n",
        "print(\"Total Number of Batches: \", len(returnBatchIndices(df,5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk60VHloMRo3",
        "colab_type": "text"
      },
      "source": [
        "### For Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexzUUlVk3pi",
        "colab_type": "text"
      },
      "source": [
        "def returnABatch(df,batch_size):\n",
        "  #returns a balanced label mri images\n",
        "  index_list = returnBatchIndices(df,batch_size)\n",
        "  #print(\"index list\",len(list(index_list)))\n",
        "  df2 = pd.get_dummies(df['label'], prefix = 'label')\n",
        "  df = pd.concat([df,df2],axis=1)\n",
        "  for j in index_list:\n",
        "    batch_images=[]\n",
        "    batch_labels=[]\n",
        "    #print(\"j\",j)\n",
        "    for i in j:\n",
        "      #print(\"i\",i)\n",
        "      label_list=[]\n",
        "      image = retrieveImage(list(df[df.index == i]['file_name'])[0])\n",
        "      transformed_image = image.reshape((512,512,1))\n",
        "      batch_images.append(transformed_image)\n",
        "      label_list.append(df[df.index == i]['label_1'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_2'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_3'].tolist()[0])\n",
        "      batch_labels.append(label_list)\n",
        "      #print(\"Batches :\",len(batch_images),len(batch_labels))\n",
        "\n",
        "    #from keras.utils import to_categorical\n",
        "    #batch_labels = to_categorical(batch_labels)\n",
        "    yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "for i in returnABatch(df.reset_index(),2)  :\n",
        "  if i[1].shape[0] < 6:\n",
        "    print(len(i),len(i[1]))\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql7EHi_cMYYP",
        "colab_type": "text"
      },
      "source": [
        "### For Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhi_DvKXMXYN",
        "colab_type": "text"
      },
      "source": [
        "def returnABatch1d(df,batch_size):\n",
        "  #returns a balanced label mri images\n",
        "  index_list = returnBatchIndices(df,batch_size)\n",
        "  #print(\"index list\",len(list(index_list)))\n",
        "  df2 = pd.get_dummies(df['label'], prefix = 'label')\n",
        "  df = pd.concat([df,df2],axis=1)\n",
        "  for j in index_list:\n",
        "    batch_images=[]\n",
        "    batch_labels=[]\n",
        "    #print(\"j\",j)\n",
        "    for i in j:\n",
        "      #print(\"i\",i)\n",
        "      label_list=[]\n",
        "      image = retrieveImage(list(df[df.index == i]['file_name'])[0])\n",
        "      transformed_image = image.reshape(512*512)\n",
        "      batch_images.append(transformed_image)\n",
        "      label_list.append(df[df.index == i]['label_1'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_2'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_3'].tolist()[0])\n",
        "      batch_labels.append(label_list)\n",
        "      #print(\"Batches :\",len(batch_images),len(batch_labels))\n",
        "\n",
        "    #from keras.utils import to_categorical\n",
        "    #batch_labels = to_categorical(batch_labels)\n",
        "    yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "for i in returnABatch1d(df.reset_index(),2)  :\n",
        "  if i[1].shape[0] < 6:\n",
        "    print(len(i),len(i[1]))\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFyXNbgckbCt",
        "colab_type": "text"
      },
      "source": [
        "#Model Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAFFed2pEX8W",
        "colab_type": "text"
      },
      "source": [
        "## CNN Approach using Tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcguDqrmyuR-",
        "colab_type": "code",
        "outputId": "1b5f884a-4c26-4094-ced0-8081b6cf5593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def CNN():\n",
        "  SHAPE = 224\n",
        "  batch_size = 64\n",
        "  def loadImage(loc, file_name):\n",
        "      label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "      loc = loc.numpy().decode('utf-8')\n",
        "      file_name = file_name.numpy().decode('utf-8')\n",
        "      with h5py.File(loc+file_name,'r') as f:\n",
        "            image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "            \n",
        "            label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "            if image_array.shape[0] != 512:\n",
        "              image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "            #image_array = np.array(tf.image.resize(np.stack((image_array,)*3,axis=-1),(SHAPE, SHAPE),method=\"nearest\"))\n",
        "            image_array = image_array/image_array.max()\n",
        "            return image_array.reshape(512,512,1), label_transform[label]\n",
        "\n",
        "  dataset= tf.data.Dataset.from_tensor_slices(os.listdir(\"download/mat/\")).\\\n",
        "              map(lambda x: tf.py_function(func=loadImage,\n",
        "        inp=[\"download/mat/\",x], Tout=(tf.float64, tf.int16)))\n",
        "  training_data = dataset.shuffle(3000).take(2500).cache().batch(batch_size).prefetch(2)\n",
        "  dataset = dataset.skip(2500)\n",
        "  validation_data = dataset.take(200).cache().batch(batch_size).prefetch(2)\n",
        "  dataset = dataset.skip(200)\n",
        "  testing_data = dataset.take(-1).repeat(1).batch(batch_size).prefetch(2)\n",
        "  depth = 8\n",
        "  model = tf.keras.models.Sequential([\n",
        "              tf.keras.layers.Conv2D(8, (3,3), activation=\"relu\", input_shape=(512,512,1))\n",
        "              ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "              ,tf.keras.layers.Conv2D(16, (3,3), activation=\"relu\")\n",
        "              ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "              ,tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\")\n",
        "              ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "              ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "              ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "              ,tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\")\n",
        "              ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "              ,tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\")\n",
        "              ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "              ,tf.keras.layers.Flatten()\n",
        "              ,tf.keras.layers.Dropout(0.5)\n",
        "              ,tf.keras.layers.Dense(256, activation=\"relu\")\n",
        "              ,tf.keras.layers.Dropout(0.5)\n",
        "              ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "  ])\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\"])#,\"Precision\",\"Recall\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  history = model.fit_generator(training_data\n",
        "                                ,validation_data=validation_data\n",
        "                                ,epochs=20\n",
        "                                ,use_multiprocessing=True\n",
        "                                )\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  result = model.evaluate_generator(testing_data,steps=1)\n",
        "  print(result)\n",
        "\n",
        "CNN()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 510, 510, 8)       80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 255, 255, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 253, 253, 16)      1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 124, 124, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 60, 60, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2359552   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 2,753,731\n",
            "Trainable params: 2,753,731\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "40/40 [==============================] - 155s 4s/step - loss: 0.8544 - accuracy: 0.6128 - val_loss: 0.6154 - val_accuracy: 0.7350\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.6269 - accuracy: 0.7284 - val_loss: 0.5169 - val_accuracy: 0.7850\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.4961 - accuracy: 0.7780 - val_loss: 0.3519 - val_accuracy: 0.8350\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.4577 - accuracy: 0.8032 - val_loss: 0.3670 - val_accuracy: 0.8200\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.3804 - accuracy: 0.8364 - val_loss: 0.2460 - val_accuracy: 0.8650\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.3137 - accuracy: 0.8684 - val_loss: 0.1640 - val_accuracy: 0.9200\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 107s 3s/step - loss: 0.2581 - accuracy: 0.8916 - val_loss: 0.1353 - val_accuracy: 0.9600\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 107s 3s/step - loss: 0.2224 - accuracy: 0.9048 - val_loss: 0.1219 - val_accuracy: 0.9550\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.1944 - accuracy: 0.9268 - val_loss: 0.1018 - val_accuracy: 0.9650\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.1601 - accuracy: 0.9352 - val_loss: 0.1415 - val_accuracy: 0.9350\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.1552 - accuracy: 0.9344 - val_loss: 0.0870 - val_accuracy: 0.9550\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.1375 - accuracy: 0.9456 - val_loss: 0.1376 - val_accuracy: 0.9400\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 107s 3s/step - loss: 0.1368 - accuracy: 0.9472 - val_loss: 0.0874 - val_accuracy: 0.9550\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.0980 - accuracy: 0.9672 - val_loss: 0.0414 - val_accuracy: 0.9850\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.1260 - accuracy: 0.9624 - val_loss: 0.0763 - val_accuracy: 0.9750\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.2309 - accuracy: 0.9028 - val_loss: 0.0843 - val_accuracy: 0.9850\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.1185 - accuracy: 0.9512 - val_loss: 0.0460 - val_accuracy: 0.9850\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 108s 3s/step - loss: 0.0885 - accuracy: 0.9644 - val_loss: 0.0380 - val_accuracy: 0.9900\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.0654 - accuracy: 0.9760 - val_loss: 0.0617 - val_accuracy: 0.9700\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 110s 3s/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 0.0219 - val_accuracy: 0.9950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEkDRAiIpL0N9lVw19rcldRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.022312136366963387, 0.984375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZHApFUtxgH",
        "colab_type": "text"
      },
      "source": [
        "img = plt.imread(\"/content/data/1/1.jpg\")\n",
        "plt.imshow(img,cmap=\"bone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AGrENCXpN1",
        "colab_type": "text"
      },
      "source": [
        "target = df_temp.pop('label')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df_temp.values, target.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecS_esHM67zr",
        "colab_type": "text"
      },
      "source": [
        "history=model.fit_generator(myCNNTrainGenerator(16)\n",
        "                  ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "                  , epochs=10\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=myCNNValidateGenerator(16)\n",
        "                  ,validation_steps = len(os.listdir(\"validation_data/npz\"))\n",
        "                  #,workers=8\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBueyG7UzkVi",
        "colab_type": "text"
      },
      "source": [
        "model.save('./final_model.h5', include_optimizer=True)\n",
        "from tensorflow.keras.models import load_model\n",
        "model2 = load_model('./final_model.h5')\n",
        "history2=model2.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=8\n",
        "                  )\n",
        "\n",
        "result2 = model2.evaluate(test_generator)\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ex2GT1x-6SG",
        "colab_type": "text"
      },
      "source": [
        "result = model.evaluate(myCNNTestGenerator(16))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uljl3NAf8Jos",
        "colab_type": "text"
      },
      "source": [
        "history=model.fit_generator(validate_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=test_generator\n",
        "                  ,workers=8\n",
        "                  )\n",
        "result = model.evaluate(test_generator)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJhyb__OadUy",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7OMaFAT5_hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4srxIYwTagJ5",
        "colab_type": "code",
        "outputId": "11a04b3e-fbab-4e4e-e155-d961677a28b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "def preTrain():\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "      https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "      -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "  local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "  pre_trained_model = InceptionV3(input_shape = (512,512,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  \n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('mixed7')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  #x = layers.Flatten()(last_output)\n",
        "  x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='sigmoid')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "  return model\n",
        "imageNet_model = preTrain()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 13:16:08--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.126.128, 2607:f8b0:4001:c14::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M   123MB/s    in 0.7s    \n",
            "\n",
            "2019-10-15 13:16:09 (123 MB/s) - /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 saved [87910968/87910968]\n",
            "\n",
            "last layer output shape:  (None, 30, 30, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSF1E-NkAZr",
        "colab_type": "text"
      },
      "source": [
        "imageNet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQPFkf4k7jhJ",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPEHY67206Y",
        "colab_type": "code",
        "outputId": "9849bc17-2986-4b60-b18b-715bfe9c9c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def pretrain2():\n",
        "  #https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "     https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "     -O /tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "\n",
        "  # https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5 \\\n",
        "  # -O /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
        "  from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "  from tensorflow.keras.applications.densenet import DenseNet121\n",
        "\n",
        "  local_weights_file = '/tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "  #'/tmp/mobilenet_1_0_128_tf_no_top.h5'\n",
        "\n",
        "  pre_trained_model = DenseNet121(input_shape = (512,512,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('relu')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  '''from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  '''\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  x = layers.Flatten()(last_output)\n",
        "  #x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='softmax')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  history=model.fit_generator(myCNNtfrTrainGenerator(16)\n",
        "                  ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "                  , epochs=10\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=myCNNVtfralidateGenerator(16)\n",
        "                  ,validation_steps = len(os.listdir(\"validation_data/npz\"))\n",
        "                  #,workers=8\n",
        "                  )\n",
        "pretrain2()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 14:43:31--  https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f5b4b85e-fa1e-11e7-9a46-5fbe25b60245?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191015%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191015T144331Z&X-Amz-Expires=300&X-Amz-Signature=f0f73b26c79ace8c8affab8e66397e615f83e8a97c1028fa05be4e485e9f4e67&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddensenet121_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-10-15 14:43:31--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f5b4b85e-fa1e-11e7-9a46-5fbe25b60245?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191015%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191015T144331Z&X-Amz-Expires=300&X-Amz-Signature=f0f73b26c79ace8c8affab8e66397e615f83e8a97c1028fa05be4e485e9f4e67&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddensenet121_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.147.67\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.147.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30011760 (29M) [application/octet-stream]\n",
            "Saving to: /tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\n",
            "/tmp/densenet121_we 100%[===================>]  28.62M  49.0MB/s    in 0.6s    \n",
            "\n",
            "2019-10-15 14:43:32 (49.0 MB/s) - /tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5 saved [30011760/30011760]\n",
            "\n",
            "last layer output shape:  (None, 16, 16, 1024)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 518, 518, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 256, 256, 64) 9408        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 256, 256, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 256, 256, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 258, 258, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 128, 128, 64) 0           zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 128, 128, 64) 256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 128, 128, 64) 0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 128, 128, 128 8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 128, 128, 128 0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 128, 128, 96) 0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 128, 128, 96) 384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 128, 128, 96) 0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 128, 128, 128 12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 128, 128, 128 0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 128, 128, 128 0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 128, 128, 128 512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 128, 128, 128 0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 128, 128, 128 16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 128, 128, 128 0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 128, 128, 160 0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 128, 128, 160 640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 128, 128, 160 0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 128, 128, 128 20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 128, 128, 128 0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 128, 128, 192 0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 128, 128, 192 768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 128, 128, 192 0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 128, 128, 128 24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 128, 128, 128 0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 128, 128, 224 0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 128, 128, 224 896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 128, 128, 224 0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 128, 128, 128 28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 128, 128, 128 0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 128, 128, 256 0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 128, 128, 256 1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 128, 128, 256 0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 128, 128, 128 32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 64, 64, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 64, 64, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 64, 64, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 64, 64, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 64, 64, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 64, 64, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 64, 64, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 64, 64, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 64, 64, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 64, 64, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 64, 64, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 64, 64, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 64, 64, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 64, 64, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 64, 64, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 64, 64, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 64, 64, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 64, 64, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 64, 64, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 64, 64, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 64, 64, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 64, 64, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 64, 64, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 64, 64, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 64, 64, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 64, 64, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 64, 64, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 64, 64, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 64, 64, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 64, 64, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 64, 64, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 64, 64, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 64, 64, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 64, 64, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 64, 64, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 64, 64, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 64, 64, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 64, 64, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 64, 64, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 64, 64, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 64, 64, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 64, 64, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 64, 64, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 64, 64, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 64, 64, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 64, 64, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 64, 64, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 64, 64, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 64, 64, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 64, 64, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 64, 64, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 64, 64, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 64, 64, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 64, 64, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 64, 64, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 64, 64, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 64, 64, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 64, 64, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 64, 64, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 64, 64, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 64, 64, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 64, 64, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 64, 64, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 64, 64, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 64, 64, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 32, 32, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 32, 32, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 32, 32, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 32, 32, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 32, 32, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 32, 32, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 32, 32, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 32, 32, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 32, 32, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 32, 32, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 32, 32, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 32, 32, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 32, 32, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 32, 32, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 32, 32, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 32, 32, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 32, 32, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 32, 32, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 32, 32, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 32, 32, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 32, 32, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 32, 32, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 32, 32, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 32, 32, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 32, 32, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 32, 32, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 32, 32, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 32, 32, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 32, 32, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 32, 32, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 32, 32, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 32, 32, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 32, 32, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 32, 32, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 32, 32, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 32, 32, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 32, 32, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 32, 32, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 32, 32, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 32, 32, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 32, 32, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 32, 32, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 32, 32, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 32, 32, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 32, 32, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 32, 32, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 32, 32, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 32, 32, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 32, 32, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 32, 32, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 32, 32, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 32, 32, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 32, 32, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 32, 32, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 32, 32, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 32, 32, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 32, 32, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 32, 32, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 32, 32, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 32, 32, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 32, 32, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 32, 32, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 32, 32, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 32, 32, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 32, 32, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 32, 32, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 32, 32, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 32, 32, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 32, 32, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 32, 32, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 32, 32, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 32, 32, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 32, 32, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 32, 32, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 32, 32, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 32, 32, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 32, 32, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 32, 32, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 32, 32, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 32, 32, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 32, 32, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 32, 32, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 32, 32, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 32, 32, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 32, 32, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 32, 32, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 32, 32, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 32, 32, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 32, 32, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 32, 32, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 32, 32, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 32, 32, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 32, 32, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 32, 32, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 32, 32, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 32, 32, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 32, 32, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 32, 32, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 32, 32, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 32, 32, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 32, 32, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 32, 32, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 32, 32, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 32, 32, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 32, 32, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 16, 16, 512)  0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 16, 16, 512)  0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 16, 16, 128)  65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 16, 16, 128)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 16, 16, 544)  0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 16, 16, 544)  2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 16, 16, 544)  0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 16, 16, 128)  69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 16, 16, 128)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 16, 16, 576)  0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 16, 16, 576)  2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 16, 16, 576)  0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 16, 16, 128)  73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 16, 16, 128)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 16, 16, 608)  0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 16, 16, 608)  2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 16, 16, 608)  0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 16, 16, 128)  77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 16, 16, 128)  0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 16, 16, 640)  0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 16, 16, 640)  2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 16, 16, 640)  0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 16, 16, 128)  81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 16, 16, 128)  0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 16, 16, 672)  0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 16, 16, 672)  2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 16, 16, 672)  0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 16, 16, 128)  86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 16, 16, 128)  0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 16, 16, 704)  0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 16, 16, 704)  2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 16, 16, 704)  0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 16, 16, 128)  90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 16, 16, 128)  0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 16, 16, 736)  0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 16, 16, 736)  2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 16, 16, 736)  0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 16, 16, 128)  94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 16, 16, 128)  0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 16, 16, 768)  0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 16, 16, 768)  3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 16, 16, 768)  0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 16, 16, 128)  98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 16, 16, 128)  0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 16, 16, 800)  0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 16, 16, 800)  3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 16, 16, 800)  0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 16, 16, 128)  102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 16, 16, 832)  0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 16, 16, 832)  3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 16, 16, 832)  0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 16, 16, 128)  106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 16, 16, 864)  0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 16, 16, 864)  3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 16, 16, 864)  0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 16, 16, 128)  110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 16, 16, 896)  0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 16, 16, 896)  3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 16, 16, 896)  0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 16, 16, 128)  114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 16, 16, 928)  0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 16, 16, 928)  3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 16, 16, 928)  0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 16, 16, 128)  118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 16, 16, 960)  0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 16, 16, 960)  3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 16, 16, 960)  0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 16, 16, 128)  122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 16, 16, 992)  0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 16, 16, 992)  3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 16, 16, 992)  0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 16, 16, 128)  126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 16, 16, 1024) 0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 16, 16, 1024) 4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 16, 16, 1024) 0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 262144)       0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          134218240   flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            1539        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 141,257,283\n",
            "Trainable params: 134,219,779\n",
            "Non-trainable params: 7,037,504\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "135/135 [==============================] - 2363s 18s/step - loss: 11.2456 - accuracy: 0.3016 - val_loss: 11.3610 - val_accuracy: 0.2935\n",
            "Epoch 2/10\n",
            "  5/135 [>.............................] - ETA: 40:08 - loss: 9.6709 - accuracy: 0.4000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Keras_worker_ForkPoolWorker-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d33498b76a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                   \u001b[0;31m#,workers=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                   )\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mpretrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d33498b76a66>\u001b[0m in \u001b[0;36mpretrain2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyCNNVtfralidateGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_data/npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                   \u001b[0;31m#,workers=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                   )\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7SCLpTz7uxK",
        "colab_type": "text"
      },
      "source": [
        "### MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVKk8Hn37pb_",
        "colab_type": "code",
        "outputId": "7e2bf6dd-982d-409d-e25c-e094af7bb36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def mobilenet():\n",
        "  #https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "     https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5 \\\n",
        "     -O /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
        "  from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "  \n",
        "  local_weights_file = '/tmp/mobilenet_1_0_128_tf_no_top.h5'\n",
        "  \n",
        "\n",
        "  pre_trained_model = MobileNet(input_shape = (255,255,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('conv_pw_13_relu')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  '''from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  \n",
        "  # Flatten the output layer to 1 dimension\n",
        "  #x = layers.\n",
        "  x = layers.Flatten()(last_output)\n",
        "  x = layers.Dropout(0.5)(x)  \n",
        "  #x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='softmax')(x)  '''         \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Conv2D(3, (3,3), activation=\"relu\", input_shape=(512,512,1))\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,pre_trained_model\n",
        "            ,tf.keras.layers.AveragePooling2D(7,7)\n",
        "            ,tf.keras.layers.Flatten()\n",
        "            ,tf.keras.layers.Dense(1000, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(512, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "          ])\n",
        "  #model = Model( pre_trained_model.input, x) \n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  history=model.fit_generator(myCNNTrainGenerator(16)\n",
        "                  ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "                  , epochs=20\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=myCNNValidateGenerator(16)\n",
        "                  ,validation_steps = len(os.listdir(\"validation_data/npz\"))\n",
        "                  #,workers=8\n",
        "                  )\n",
        "  print(model.evaluate(myCNNtfrTestGenerator(16)))\n",
        "mobilenet()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-16 13:54:35--  https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191016T135435Z&X-Amz-Expires=300&X-Amz-Signature=f3875f203cf242225c9e0964f055064b84bd9262a9e517f44849c6a01bbb3b09&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-10-16 13:54:35--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191016T135435Z&X-Amz-Expires=300&X-Amz-Signature=f3875f203cf242225c9e0964f055064b84bd9262a9e517f44849c6a01bbb3b09&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.225.144\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.225.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17225924 (16M) [application/octet-stream]\n",
            "Saving to: /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
            "\n",
            "/tmp/mobilenet_1_0_ 100%[===================>]  16.43M  46.8MB/s    in 0.4s    \n",
            "\n",
            "2019-10-16 13:54:36 (46.8 MB/s) - /tmp/mobilenet_1_0_128_tf_no_top.h5 saved [17225924/17225924]\n",
            "\n",
            "Model: \"mobilenet_1.00_255\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 255, 255, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 127, 127, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 127, 127, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 127, 127, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 127, 127, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 127, 127, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 127, 127, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 127, 127, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 63, 63, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 63, 63, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 63, 63, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 63, 63, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 63, 63, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 63, 63, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 31, 31, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 31, 31, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 31, 31, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 31, 31, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 31, 31, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 15, 15, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 15, 15, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 15, 15, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 15, 15, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "=================================================================\n",
            "Total params: 3,228,864\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "None\n",
            "last layer output shape:  (None, 7, 7, 1024)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 510, 510, 3)       30        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 255, 255, 3)       0         \n",
            "_________________________________________________________________\n",
            "mobilenet_1.00_255 (Model)   (None, 7, 7, 1024)        3228864   \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1000)              1025000   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               512512    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 4,767,945\n",
            "Trainable params: 1,539,081\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "135/135 [==============================] - 175s 1s/step - loss: 1.1465 - accuracy: 0.5492 - val_loss: 1.3695 - val_accuracy: 0.2609\n",
            "Epoch 2/20\n",
            "135/135 [==============================] - 171s 1s/step - loss: 0.6840 - accuracy: 0.6965 - val_loss: 2.3565 - val_accuracy: 0.2609\n",
            "Epoch 3/20\n",
            "135/135 [==============================] - 170s 1s/step - loss: 0.6144 - accuracy: 0.7436 - val_loss: 3.3268 - val_accuracy: 0.2609\n",
            "Epoch 4/20\n",
            "135/135 [==============================] - 172s 1s/step - loss: 0.5226 - accuracy: 0.7702 - val_loss: 4.0483 - val_accuracy: 0.2609\n",
            "Epoch 5/20\n",
            "135/135 [==============================] - 171s 1s/step - loss: 0.5185 - accuracy: 0.7758 - val_loss: 4.1357 - val_accuracy: 0.2609\n",
            "Epoch 6/20\n",
            "  2/135 [..............................] - ETA: 2:43 - loss: 0.3427 - accuracy: 0.9062"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Keras_worker_ForkPoolWorker-9:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0c0a098c3492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                   )\n\u001b[1;32m     75\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyCNNtfrTestGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mmobilenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-0c0a098c3492>\u001b[0m in \u001b[0;36mmobilenet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyCNNValidateGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_data/npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                   \u001b[0;31m#,workers=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                   )\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by4izwHZAL4z",
        "colab_type": "text"
      },
      "source": [
        "#### MobileNet v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjKu5QJAueF6",
        "colab_type": "code",
        "outputId": "9e22afb9-cebd-4093-8d05-3d032eb3a417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SkipDataset shapes: (<unknown>, <unknown>), types: (tf.float64, tf.int16)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w6VtzpgAPah",
        "colab_type": "code",
        "outputId": "73e4d899-657d-4539-e141-9b55724f26b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def mobilenet2():\n",
        "  #https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "     https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5 \\\n",
        "     -O /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
        "  from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "  \n",
        "  local_weights_file = '/tmp/mobilenet_1_0_128_tf_no_top.h5'\n",
        "  \n",
        "\n",
        "  pre_trained_model = MobileNet(input_shape = (224,224,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  print(pre_trained_model.summary())\n",
        "\n",
        "  #last_layer = pre_trained_model.get_layer('conv_pw_13_relu')\n",
        "  last_layer = pre_trained_model.get_layer('conv_pad_12')\n",
        "  last_output = last_layer.output\n",
        "  #print('last layer output shape: ', last_layer.output_shape)\n",
        "  #last_output = last_layer.output\n",
        "  x = tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\n",
        "  #x = tf.keras.layers.AveragePooling2D((6,6))(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(3,activation=\"softmax\")(x)\n",
        "\n",
        "  model = Model( pre_trained_model.input, x)\n",
        "  '''\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.2)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (1, activation='sigmoid')(x) \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            #pre_trained_model\n",
        "            last_layer\n",
        "            ,tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D()\n",
        "            ,tf.keras.layers.AveragePooling2D((7,7))\n",
        "            ,tf.keras.layers.Flatten()\n",
        "            ,tf.keras.layers.Dense(1024, activation=\"relu\")\n",
        "            #,tf.keras.layers.Conv2D(512, (3,3), activation=\"relu\")\n",
        "            #,tf.keras.layers.MaxPooling2D()\n",
        "            #,tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\")\n",
        "            #,tf.keras.layers.MaxPooling2D()\n",
        "            #,tf.keras.layers.Flatten()\n",
        "            #,tf.keras.layers.Dense(1000, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            #,tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "            #,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "          ])\n",
        "  #model = Model( pre_trained_model.input, x) '''\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.summary()\n",
        "  #print(model.evaluate_generator(testing_data,steps=1))\n",
        "  history=model.fit_generator(\n",
        "                  training_data\n",
        "                  ,epochs=20\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validation_data\n",
        "                  )\n",
        "  print(model.evaluate_generator(testing_data,steps=1))\n",
        "mobilenet2()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-24 05:51:44--  https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191024%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191024T055145Z&X-Amz-Expires=300&X-Amz-Signature=0b05705798e28ea56d2bf842ca26195d00686eda181eb01513ff52eae185ca52&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-10-24 05:51:45--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191024%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191024T055145Z&X-Amz-Expires=300&X-Amz-Signature=0b05705798e28ea56d2bf842ca26195d00686eda181eb01513ff52eae185ca52&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.86.203\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.86.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17225924 (16M) [application/octet-stream]\n",
            "Saving to: /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
            "\n",
            "/tmp/mobilenet_1_0_ 100%[===================>]  16.43M  47.4MB/s    in 0.3s    \n",
            "\n",
            "2019-10-24 05:51:45 (47.4 MB/s) - /tmp/mobilenet_1_0_128_tf_no_top.h5 saved [17225924/17225924]\n",
            "\n",
            "Model: \"mobilenet_1.00_224\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "=================================================================\n",
            "Total params: 3,228,864\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 13, 13, 64)        294976    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                147520    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 2,070,531\n",
            "Trainable params: 442,691\n",
            "Non-trainable params: 1,627,840\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "40/40 [==============================] - 143s 4s/step - loss: 1.2883 - accuracy: 0.5360 - val_loss: 0.8375 - val_accuracy: 0.6800\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 143s 4s/step - loss: 0.6277 - accuracy: 0.6940 - val_loss: 0.6008 - val_accuracy: 0.7250\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 148s 4s/step - loss: 0.4601 - accuracy: 0.7884 - val_loss: 0.6575 - val_accuracy: 0.6750\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 148s 4s/step - loss: 0.3840 - accuracy: 0.8428 - val_loss: 0.6284 - val_accuracy: 0.6750\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 151s 4s/step - loss: 0.3628 - accuracy: 0.8556 - val_loss: 0.6095 - val_accuracy: 0.7150\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 151s 4s/step - loss: 0.3039 - accuracy: 0.8736 - val_loss: 0.3681 - val_accuracy: 0.8050\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 145s 4s/step - loss: 0.2484 - accuracy: 0.9024 - val_loss: 0.8260 - val_accuracy: 0.6550\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 142s 4s/step - loss: 0.2051 - accuracy: 0.9092 - val_loss: 0.6315 - val_accuracy: 0.7600\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 140s 3s/step - loss: 0.1647 - accuracy: 0.9332 - val_loss: 0.2760 - val_accuracy: 0.8650\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 140s 4s/step - loss: 0.1201 - accuracy: 0.9512 - val_loss: 0.1728 - val_accuracy: 0.9100\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 141s 4s/step - loss: 0.1023 - accuracy: 0.9624 - val_loss: 0.4116 - val_accuracy: 0.8200\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 140s 3s/step - loss: 0.0786 - accuracy: 0.9724 - val_loss: 0.2530 - val_accuracy: 0.9000\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 139s 3s/step - loss: 0.0494 - accuracy: 0.9856 - val_loss: 0.3639 - val_accuracy: 0.8600\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 138s 3s/step - loss: 0.0652 - accuracy: 0.9776 - val_loss: 0.1501 - val_accuracy: 0.9000\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 138s 3s/step - loss: 0.0962 - accuracy: 0.9676 - val_loss: 0.1700 - val_accuracy: 0.9200\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 139s 3s/step - loss: 0.0698 - accuracy: 0.9768 - val_loss: 0.1987 - val_accuracy: 0.9100\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 139s 3s/step - loss: 0.0441 - accuracy: 0.9852 - val_loss: 0.1712 - val_accuracy: 0.9100\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 138s 3s/step - loss: 0.0325 - accuracy: 0.9888 - val_loss: 0.2625 - val_accuracy: 0.9100\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 145s 4s/step - loss: 0.0335 - accuracy: 0.9864 - val_loss: 0.2352 - val_accuracy: 0.9100\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 145s 4s/step - loss: 0.0268 - accuracy: 0.9904 - val_loss: 0.2020 - val_accuracy: 0.9200\n",
            "[0.19222292304039001, 0.90625]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S3IXXqNnUOE",
        "colab_type": "code",
        "outputId": "9564969c-13d7-4802-c8c5-23f78ecd54d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "def pretrain1():\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  module_selection = (\"mobilenet_v2\", 224) #@param [\"(\\\"mobilenet_v2\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "  handle_base, pixels = module_selection\n",
        "  #MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "  MODULE_HANDLE = \"https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3\"\n",
        "  IMAGE_SIZE = (pixels, pixels)\n",
        "  print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
        "\n",
        "  BATCH_SIZE = 16 #@param {type:\"integer\"}\n",
        "\n",
        "  print(\"Building model with\", MODULE_HANDLE)\n",
        "  model = tf.keras.Sequential([\n",
        "      hub.Module(MODULE_HANDLE, tags={\"train\"}, trainable=False),\n",
        "      tf.keras.layers.Dropout(rate=0.2),\n",
        "      tf.keras.layers.Dense(3, activation='softmax',\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "  ])\n",
        "  model.build((None,)+IMAGE_SIZE+(3,))\n",
        "  model.summary()\n",
        "\n",
        "pretrain1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3 with input size (224, 224)\n",
            "Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4af54639eb34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpretrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-4af54639eb34>\u001b[0m in \u001b[0;36mpretrain1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HANDLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   model = tf.keras.Sequential([\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HANDLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       tf.keras.layers.Dense(3, activation='softmax',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m           tags=self._tags)\n\u001b[0m\u001b[1;32m    171\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[1;32m    404\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         import_scope=relative_scope_name)\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Build a list from the variable name in the module definition to the actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[1;32m   1452\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                                  **kwargs)[0]\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m   \u001b[0;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[1;32m   1464\u001b[0m                        \u001b[0;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m                        \"execution is enabled.\")\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlUhmzy2380C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageNet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxHeuBcA4TVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageNet_model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUHrCKk4ZMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator, validate_generator, test_generator = getGenerators(32,(512,512))\n",
        "history=imageNet_model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=3\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  #,workers=2\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cIJzWiY4dkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = imageNet_model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgY15ze8jZet",
        "colab_type": "text"
      },
      "source": [
        "## Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rieJMvjwtIy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bstData(loc = \"download/mat/\",SHAPE = 64):\n",
        "  print(datetime.datetime.now())\n",
        "  image_list = []\n",
        "  label_list = []\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  #label_transform = [[],[0,0,1],[0,1,0],[1,0,0]]\n",
        "  #ddd = pd.DataFrame(columns=[str(x) for x in range(512*512)]+[\"label\"])\n",
        "  i =0\n",
        "  image_stacks = np.ndarray((4500,(SHAPE*SHAPE)),dtype=np.float64)\n",
        "  print(image_stacks.shape)\n",
        "  for file_name in [f  for f in os.listdir(loc) if \".mat\" in f]:\n",
        "    if i % 200 == 0:\n",
        "      print(i, end=\" \")\n",
        "      #break\n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          \n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          \n",
        "          image_array = np.array(tf.image.resize(image_array.reshape(512,512,1),(SHAPE,SHAPE), method=\"nearest\")).reshape((SHAPE,SHAPE))\n",
        "          image_array = image_array/image_array.max()\n",
        "          #image_list.append((image_array,  label_transform[label]))\n",
        "          #image_list.append(image_array.reshape(-1))\n",
        "          #label_list.append(list(label_transform[label]))\n",
        "          label_list.append(label)\n",
        "          #label_list.append(str(label))\n",
        "          #image_array_label = np.concatenate((image_array.reshape(-1), [label]))\n",
        "          #print(image_array.reshape(-1).shape, image_array_label.shape)\n",
        "          #image_stacks = np.stack((image_stacks,image_array_label))\n",
        "          image_stacks[i] = image_array.reshape(-1)\n",
        "          i=i+1\n",
        "\n",
        "  #df = pd.DataFrame({\"image_array\":image_list,\"label\":label_list})\n",
        "  df = pd.DataFrame(image_stacks[:i],columns=[str(x) for x in range(SHAPE*SHAPE)])\n",
        "  df['label'] = label_list\n",
        "  print(\"Load Data End Time: \", datetime.datetime.now())\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kfFLA7jH1jC",
        "colab_type": "code",
        "outputId": "50108834-be0f-4a3f-8bef-f5ead806a69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "df_bst = bstData()\n",
        "def returnMap(x):\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  return label_transform[int(x)]\n",
        "#df_bst['label'] = df_bst.label.apply(lambda x: returnMap(x))\n",
        "df_bst.head()\n",
        "df_bst.info()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-24 04:26:38.893904\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:26:57.289822\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3064 entries, 0 to 3063\n",
            "Columns: 4097 entries, 0 to label\n",
            "dtypes: float64(4096), int64(1)\n",
            "memory usage: 95.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJj7BpHjDnx8",
        "colab_type": "code",
        "outputId": "cfda3bf4-ecae-4773-8df1-f0d736ccbf65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "df_bst.info()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3064 entries, 0 to 3063\n",
            "Columns: 4097 entries, 0 to label\n",
            "dtypes: float64(4096), int64(1)\n",
            "memory usage: 95.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wwl_dvAf1VI",
        "colab_type": "code",
        "outputId": "47622c5e-158a-42f5-c9a5-c2326ba4593c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "df_bst_eval = df_bst.sample(frac=0.1)\n",
        "df_bst =df_bst.drop(df_bst_eval.index)\n",
        "df_bst_eval.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>4057</th>\n",
              "      <th>4058</th>\n",
              "      <th>4059</th>\n",
              "      <th>4060</th>\n",
              "      <th>4061</th>\n",
              "      <th>4062</th>\n",
              "      <th>4063</th>\n",
              "      <th>4064</th>\n",
              "      <th>4065</th>\n",
              "      <th>4066</th>\n",
              "      <th>4067</th>\n",
              "      <th>4068</th>\n",
              "      <th>4069</th>\n",
              "      <th>4070</th>\n",
              "      <th>4071</th>\n",
              "      <th>4072</th>\n",
              "      <th>4073</th>\n",
              "      <th>4074</th>\n",
              "      <th>4075</th>\n",
              "      <th>4076</th>\n",
              "      <th>4077</th>\n",
              "      <th>4078</th>\n",
              "      <th>4079</th>\n",
              "      <th>4080</th>\n",
              "      <th>4081</th>\n",
              "      <th>4082</th>\n",
              "      <th>4083</th>\n",
              "      <th>4084</th>\n",
              "      <th>4085</th>\n",
              "      <th>4086</th>\n",
              "      <th>4087</th>\n",
              "      <th>4088</th>\n",
              "      <th>4089</th>\n",
              "      <th>4090</th>\n",
              "      <th>4091</th>\n",
              "      <th>4092</th>\n",
              "      <th>4093</th>\n",
              "      <th>4094</th>\n",
              "      <th>4095</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2199</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011652</td>\n",
              "      <td>0.021590</td>\n",
              "      <td>0.021590</td>\n",
              "      <td>0.020219</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.028444</td>\n",
              "      <td>0.028787</td>\n",
              "      <td>0.017135</td>\n",
              "      <td>0.022618</td>\n",
              "      <td>0.000685</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010281</td>\n",
              "      <td>0.067169</td>\n",
              "      <td>0.039753</td>\n",
              "      <td>0.031186</td>\n",
              "      <td>0.031871</td>\n",
              "      <td>0.021933</td>\n",
              "      <td>0.028787</td>\n",
              "      <td>0.025360</td>\n",
              "      <td>0.040781</td>\n",
              "      <td>0.013365</td>\n",
              "      <td>0.023989</td>\n",
              "      <td>0.025017</td>\n",
              "      <td>0.022276</td>\n",
              "      <td>0.022618</td>\n",
              "      <td>0.020219</td>\n",
              "      <td>0.021933</td>\n",
              "      <td>0.019877</td>\n",
              "      <td>0.012680</td>\n",
              "      <td>0.023646</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.009253</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2245</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>0.013258</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.016472</td>\n",
              "      <td>0.021695</td>\n",
              "      <td>0.018481</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.023303</td>\n",
              "      <td>0.020490</td>\n",
              "      <td>0.020490</td>\n",
              "      <td>0.016874</td>\n",
              "      <td>0.028927</td>\n",
              "      <td>0.017276</td>\n",
              "      <td>0.020892</td>\n",
              "      <td>0.023704</td>\n",
              "      <td>0.032141</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044998</td>\n",
              "      <td>0.027320</td>\n",
              "      <td>0.023303</td>\n",
              "      <td>0.021294</td>\n",
              "      <td>0.027722</td>\n",
              "      <td>0.024508</td>\n",
              "      <td>0.022097</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.028927</td>\n",
              "      <td>0.022097</td>\n",
              "      <td>0.020088</td>\n",
              "      <td>0.022499</td>\n",
              "      <td>0.020490</td>\n",
              "      <td>0.016874</td>\n",
              "      <td>0.020892</td>\n",
              "      <td>0.018883</td>\n",
              "      <td>0.021695</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.014464</td>\n",
              "      <td>0.010446</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1878</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003359</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.009405</td>\n",
              "      <td>0.008398</td>\n",
              "      <td>0.008398</td>\n",
              "      <td>0.012429</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.009070</td>\n",
              "      <td>0.010749</td>\n",
              "      <td>0.017803</td>\n",
              "      <td>0.006718</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005375</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.006718</td>\n",
              "      <td>0.00739</td>\n",
              "      <td>0.003359</td>\n",
              "      <td>0.002687</td>\n",
              "      <td>0.002015</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.002687</td>\n",
              "      <td>0.004031</td>\n",
              "      <td>0.005039</td>\n",
              "      <td>0.006718</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.009405</td>\n",
              "      <td>0.00739</td>\n",
              "      <td>0.012093</td>\n",
              "      <td>0.015452</td>\n",
              "      <td>0.010749</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.006718</td>\n",
              "      <td>0.013100</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>0.010077</td>\n",
              "      <td>0.008398</td>\n",
              "      <td>0.009070</td>\n",
              "      <td>0.009741</td>\n",
              "      <td>0.008062</td>\n",
              "      <td>0.016124</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.005710</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.009070</td>\n",
              "      <td>0.006046</td>\n",
              "      <td>0.007390</td>\n",
              "      <td>0.004703</td>\n",
              "      <td>0.013436</td>\n",
              "      <td>0.004031</td>\n",
              "      <td>0.004367</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  4097 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3  ...      4093      4094      4095  label\n",
              "2199  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000      2\n",
              "752   0.0  0.011652  0.021590  0.021590  ...  0.023646  0.018163  0.009253      2\n",
              "2245  0.0  0.006027  0.013258  0.017678  ...  0.014464  0.010446  0.000000      2\n",
              "1878  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000      1\n",
              "1054  0.0  0.003359  0.008734  0.008734  ...  0.013436  0.004031  0.004367      3\n",
              "\n",
              "[5 rows x 4097 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XttH4EoiMI_E",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
        "from sklearn.datasets import load_iris, load_digits, load_boston\n",
        "\n",
        "rng = np.random.RandomState(31337)\n",
        "\n",
        "print(\"Zeros and Ones from the Digits dataset: binary classification\")\n",
        "digits = load_digits(2)\n",
        "y = digits['target']\n",
        "X = digits['data']\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    xgb_model = xgb.XGBClassifier().fit(X[train_index], y[train_index])\n",
        "    predictions = xgb_model.predict(X[test_index])\n",
        "    actuals = y[test_index]\n",
        "    print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "print(\"Iris: multiclass classification\")\n",
        "iris = load_iris()\n",
        "y = iris['target']\n",
        "X = iris['data']\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    xgb_model = xgb.XGBClassifier().fit(X[train_index], y[train_index])\n",
        "    predictions = xgb_model.predict(X[test_index])\n",
        "    actuals = y[test_index]\n",
        "    print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "print(\"Boston Housing: regression\")\n",
        "boston = load_boston()\n",
        "y = boston['target']\n",
        "X = boston['data']\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    xgb_model = xgb.XGBRegressor().fit(X[train_index], y[train_index])\n",
        "    predictions = xgb_model.predict(X[test_index])\n",
        "    actuals = y[test_index]\n",
        "    print(mean_squared_error(actuals, predictions))\n",
        "\n",
        "print(\"Parameter optimization\")\n",
        "y = boston['target']\n",
        "X = boston['data']\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "clf = GridSearchCV(xgb_model,\n",
        "                   {'max_depth': [2,4,6],\n",
        "                    'n_estimators': [50,100,200]}, verbose=1)\n",
        "clf.fit(X,y)\n",
        "print(clf.best_score_)\n",
        "print(clf.best_params_)\n",
        "\n",
        "# The sklearn API models are picklable\n",
        "print(\"Pickling sklearn API models\")\n",
        "# must open in binary format to pickle\n",
        "pickle.dump(clf, open(\"best_boston.pkl\", \"wb\"))\n",
        "clf2 = pickle.load(open(\"best_boston.pkl\", \"rb\"))\n",
        "print(np.allclose(clf.predict(X), clf2.predict(X)))\n",
        "\n",
        "# Early-stopping\n",
        "\n",
        "X = digits['data']\n",
        "y = digits['target']\n",
        "\n",
        "print(type(X),np.unique(y))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "clf = xgb.XGBClassifier()\n",
        "clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"auc\",\n",
        "        eval_set=[(X_test, y_test)])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdrTcbZMJfdO",
        "colab_type": "code",
        "outputId": "17102c38-6279-4edf-e442-fc7fb67dfc0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "def xgboost_exp(shape):\n",
        "  df_bst = bstData(SHAPE=shape)\n",
        "  df_bst_eval = df_bst.sample(frac=0.3)\n",
        "  df_bst =df_bst.drop(df_bst_eval.index)\n",
        "  import xgboost as xgb\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  print(datetime.datetime.now())\n",
        "  xgb_model = xgb.XGBClassifier(n_estimators=100).fit(df_bst[df_bst.columns[:-1]],df_bst['label'])\n",
        "  print(datetime.datetime.now())\n",
        "  print(\"When image is of shape\",shape,\"Accuracy :\",accuracy_score(df_bst_eval['label'], xgb_model.predict(df_bst_eval[df_bst_eval.columns[:-1]])))\n",
        "  #print( accuracy_score(df_))\n",
        "\n",
        "xgboost_exp(64)\n",
        "xgboost_exp(64)\n",
        "xgboost_exp(64)\n",
        "xgboost_exp(32)\n",
        "xgboost_exp(32)\n",
        "xgboost_exp(32)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-24 04:26:58.194264\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:27:16.478122\n",
            "2019-10-24 04:27:16.689397\n",
            "2019-10-24 04:29:39.983010\n",
            "When image is of shape 64 Accuracy : 0.9368879216539717\n",
            "2019-10-24 04:29:40.115946\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:29:57.779183\n",
            "2019-10-24 04:29:57.807268\n",
            "2019-10-24 04:32:19.852404\n",
            "When image is of shape 64 Accuracy : 0.9129488574537541\n",
            "2019-10-24 04:32:20.004790\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:32:38.076467\n",
            "2019-10-24 04:32:38.104795\n",
            "2019-10-24 04:35:00.319867\n",
            "When image is of shape 64 Accuracy : 0.9227421109902068\n",
            "2019-10-24 04:35:00.454485\n",
            "(4500, 1024)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:35:18.457347\n",
            "2019-10-24 04:35:18.468407\n",
            "2019-10-24 04:35:54.274200\n",
            "When image is of shape 32 Accuracy : 0.9107725788900979\n",
            "2019-10-24 04:35:54.319008\n",
            "(4500, 1024)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:36:12.178843\n",
            "2019-10-24 04:36:12.188048\n",
            "2019-10-24 04:36:47.918078\n",
            "When image is of shape 32 Accuracy : 0.8933623503808488\n",
            "2019-10-24 04:36:47.962399\n",
            "(4500, 1024)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:37:05.844432\n",
            "2019-10-24 04:37:05.853330\n",
            "2019-10-24 04:37:41.504317\n",
            "When image is of shape 32 Accuracy : 0.9020674646354734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7zZv82fo8nX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "338d3f5e-916b-41ba-aae2-bef9e9ada41c"
      },
      "source": [
        "xgboost_exp(128)\n",
        "xgboost_exp(128)\n",
        "xgboost_exp(128)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-23 12:42:45.364320\n",
            "(4500, 16384)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-23 12:43:07.270429\n",
            "2019-10-23 12:43:07.505608\n",
            "2019-10-23 12:52:38.742299\n",
            "When image is of shape 128 Accuracy : 0.9129488574537541\n",
            "2019-10-23 12:52:39.269810\n",
            "(4500, 16384)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-23 12:53:00.841379\n",
            "2019-10-23 12:53:00.938270\n",
            "2019-10-23 13:02:43.226755\n",
            "When image is of shape 128 Accuracy : 0.9325353645266594\n",
            "2019-10-23 13:02:43.765483\n",
            "(4500, 16384)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-23 13:03:05.481727\n",
            "2019-10-23 13:03:05.586474\n",
            "2019-10-23 13:12:38.021653\n",
            "When image is of shape 128 Accuracy : 0.9249183895538629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVyCF3VeX88_",
        "colab_type": "text"
      },
      "source": [
        "## Will the accuracy change if only nearby tumor area is considered?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l01pbhnmYE-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "873b07f9-3c41-4945-8927-7e6ea3ac1dae"
      },
      "source": [
        "def get_tumor_neighbor(a,o,k1):\n",
        "    \n",
        "    k = k1[:]\n",
        "    #print(\"shape of k : \", len(k))\n",
        "    b = np.zeros((len(k),a.shape[0], a.shape[1]))\n",
        "    b[0] = o\n",
        "    del k[-1]\n",
        "    for i in range(a.shape[0] ):\n",
        "        if np.count_nonzero(a[i]) > 0:\n",
        "            for j in range(a.shape[1] ):\n",
        "                if a[i][j] != 0 :#and b[i][j] == 0:\n",
        "                    l = 1\n",
        "                    for m in k:\n",
        "                        if i-m >= 0 and j+m <= 512 :\n",
        "                            b[l, i-m:i, j:j+m] = o[i-m:i, j:j+m]\n",
        "                        if i-m >= 0 and j-m >= 0:\n",
        "                            b[l, i-m:i,j-m:j] = o[i-m:i,j-m:j]\n",
        "                        if i+m <=512 and j-m >= 0:\n",
        "                            b[l, i:i+m, j-m:j] = o[i:i+m, j-m:j]\n",
        "                        if i+m <=512 and j+m <=512:\n",
        "                            b[l, i:i+m, j:j+m] = o[i:i+m, j:j+m]\n",
        "                        l+=1\n",
        "    \n",
        "    return b\n",
        "\n",
        "\n",
        "def bstTumorData(loc = \"download/mat/\",SHAPE = 64,neighbors=[64]):\n",
        "  print(datetime.datetime.now())\n",
        "  image_list = []\n",
        "  label_list = []\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  #label_transform = [[],[0,0,1],[0,1,0],[1,0,0]]\n",
        "  #ddd = pd.DataFrame(columns=[str(x) for x in range(512*512)]+[\"label\"])\n",
        "  i =0\n",
        "  image_stacks = np.ndarray((4500,(SHAPE*SHAPE)),dtype=np.float64)\n",
        "  print(image_stacks.shape)\n",
        "  for file_name in [f  for f in os.listdir(loc) if \".mat\" in f]:\n",
        "    if i % 200 == 0:\n",
        "      print(i, end=\" \")\n",
        "      #break\n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          orig_image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          if orig_image_array.shape[0] != 512:\n",
        "            orig_image_array = np.pad(orig_image_array,(512 - orig_image_array.shape[0])//2,'constant',constant_values=0)\n",
        "\n",
        "          tumor_array = np.array(f['cjdata']['tumorMask'],dtype=np.float64)\n",
        "          if tumor_array.shape[0] != 512:\n",
        "            tumor_array = np.pad(tumor_array,(512 - tumor_array.shape[0])//2,'constant',constant_values=0)\n",
        "          #print(\"tumor size\", tumor_array.shape)\n",
        "          \n",
        "          image_array = get_tumor_neighbor(tumor_array,orig_image_array,neighbors)\n",
        "\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          \n",
        "          image_array = np.array(tf.image.resize(image_array.reshape(512,512,1),(SHAPE,SHAPE), method=\"nearest\")).reshape((SHAPE,SHAPE))\n",
        "          image_array = image_array/image_array.max()\n",
        "          #image_list.append((image_array,  label_transform[label]))\n",
        "          #image_list.append(image_array.reshape(-1))\n",
        "          #label_list.append(list(label_transform[label]))\n",
        "          #print(image_array.max(),image_array.min())\n",
        "          label_list.append(label)\n",
        "          #label_list.append(str(label))\n",
        "          #image_array_label = np.concatenate((image_array.reshape(-1), [label]))\n",
        "          #print(image_array.reshape(-1).shape, image_array_label.shape)\n",
        "          #image_stacks = np.stack((image_stacks,image_array_label))\n",
        "          image_stacks[i] = image_array.reshape(-1)\n",
        "          i=i+1\n",
        "\n",
        "  #df = pd.DataFrame({\"image_array\":image_list,\"label\":label_list})\n",
        "  df = pd.DataFrame(image_stacks[:i],columns=[str(x) for x in range(SHAPE*SHAPE)])\n",
        "  df['label'] = label_list\n",
        "  print(\"Load Data End Time: \", datetime.datetime.now())\n",
        "  return df\n",
        "\n",
        "df_bst = bstTumorData()\n",
        "df_bst.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-24 04:24:58.354159\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 04:26:37.169667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>4057</th>\n",
              "      <th>4058</th>\n",
              "      <th>4059</th>\n",
              "      <th>4060</th>\n",
              "      <th>4061</th>\n",
              "      <th>4062</th>\n",
              "      <th>4063</th>\n",
              "      <th>4064</th>\n",
              "      <th>4065</th>\n",
              "      <th>4066</th>\n",
              "      <th>4067</th>\n",
              "      <th>4068</th>\n",
              "      <th>4069</th>\n",
              "      <th>4070</th>\n",
              "      <th>4071</th>\n",
              "      <th>4072</th>\n",
              "      <th>4073</th>\n",
              "      <th>4074</th>\n",
              "      <th>4075</th>\n",
              "      <th>4076</th>\n",
              "      <th>4077</th>\n",
              "      <th>4078</th>\n",
              "      <th>4079</th>\n",
              "      <th>4080</th>\n",
              "      <th>4081</th>\n",
              "      <th>4082</th>\n",
              "      <th>4083</th>\n",
              "      <th>4084</th>\n",
              "      <th>4085</th>\n",
              "      <th>4086</th>\n",
              "      <th>4087</th>\n",
              "      <th>4088</th>\n",
              "      <th>4089</th>\n",
              "      <th>4090</th>\n",
              "      <th>4091</th>\n",
              "      <th>4092</th>\n",
              "      <th>4093</th>\n",
              "      <th>4094</th>\n",
              "      <th>4095</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.008176</td>\n",
              "      <td>0.007315</td>\n",
              "      <td>0.010327</td>\n",
              "      <td>0.007745</td>\n",
              "      <td>0.008606</td>\n",
              "      <td>0.018503</td>\n",
              "      <td>0.009036</td>\n",
              "      <td>0.011188</td>\n",
              "      <td>0.006885</td>\n",
              "      <td>0.013339</td>\n",
              "      <td>0.011188</td>\n",
              "      <td>0.012478</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007315</td>\n",
              "      <td>0.009897</td>\n",
              "      <td>0.008606</td>\n",
              "      <td>0.007745</td>\n",
              "      <td>0.011188</td>\n",
              "      <td>0.009036</td>\n",
              "      <td>0.009466</td>\n",
              "      <td>0.013339</td>\n",
              "      <td>0.006885</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.012909</td>\n",
              "      <td>0.009897</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>0.01506</td>\n",
              "      <td>0.009897</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>0.010757</td>\n",
              "      <td>0.012478</td>\n",
              "      <td>0.009036</td>\n",
              "      <td>0.008176</td>\n",
              "      <td>0.013769</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.016351</td>\n",
              "      <td>0.01506</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.009466</td>\n",
              "      <td>0.017642</td>\n",
              "      <td>0.022375</td>\n",
              "      <td>0.010757</td>\n",
              "      <td>0.010757</td>\n",
              "      <td>0.013339</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.012909</td>\n",
              "      <td>0.016351</td>\n",
              "      <td>0.011188</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>0.01506</td>\n",
              "      <td>0.009897</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  4097 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0         1         2         3  ...      4093     4094      4095  label\n",
              "0  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000      1\n",
              "1  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000      1\n",
              "2  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000      3\n",
              "3  0.0  0.006454  0.008176  0.007315  ...  0.015491  0.01506  0.009897      3\n",
              "4  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.00000  0.000000      3\n",
              "\n",
              "[5 rows x 4097 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioNrrGaReaTl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "07e2421f-dbe7-481c-f9ce-ab540f60816c"
      },
      "source": [
        "def xgboost_exp_2(shape):\n",
        "  df_bst = bstTumorData(SHAPE=shape)\n",
        "  df_bst_eval = df_bst.sample(frac=0.3)\n",
        "  df_bst =df_bst.drop(df_bst_eval.index)\n",
        "  import xgboost as xgb\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  print(datetime.datetime.now())\n",
        "  xgb_model = xgb.XGBClassifier(n_estimators=100).fit(df_bst[df_bst.columns[:-1]],df_bst['label'])\n",
        "  print(datetime.datetime.now())\n",
        "  print(\"When image is of shape\",shape,\"Accuracy :\",accuracy_score(df_bst_eval['label'], xgb_model.predict(df_bst_eval[df_bst_eval.columns[:-1]])))\n",
        "  #print( accuracy_score(df_))\n",
        "\n",
        "xgboost_exp_2(64)\n",
        "xgboost_exp_2(64)\n",
        "xgboost_exp_2(64)\n",
        "xgboost_exp_2(32)\n",
        "xgboost_exp_2(32)\n",
        "xgboost_exp_2(32)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-24 01:58:32.282550\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 02:00:11.403796\n",
            "2019-10-24 02:00:11.720013\n",
            "2019-10-24 02:02:35.237065\n",
            "When image is of shape 64 Accuracy : 0.9216539717083787\n",
            "2019-10-24 02:02:35.378934\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 02:04:10.731203\n",
            "2019-10-24 02:04:10.761232\n",
            "2019-10-24 02:06:33.661863\n",
            "When image is of shape 64 Accuracy : 0.9096844396082698\n",
            "2019-10-24 02:06:33.799114\n",
            "(4500, 4096)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 02:08:10.297622\n",
            "2019-10-24 02:08:10.322989\n",
            "2019-10-24 02:10:33.038658\n",
            "When image is of shape 64 Accuracy : 0.9129488574537541\n",
            "2019-10-24 02:10:33.178044\n",
            "(4500, 1024)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 02:12:08.806022\n",
            "2019-10-24 02:12:08.815485\n",
            "2019-10-24 02:12:44.670485\n",
            "When image is of shape 32 Accuracy : 0.9140369967355821\n",
            "2019-10-24 02:12:44.715389\n",
            "(4500, 1024)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 02:14:19.765889\n",
            "2019-10-24 02:14:19.775051\n",
            "2019-10-24 02:14:55.377524\n",
            "When image is of shape 32 Accuracy : 0.9162132752992383\n",
            "2019-10-24 02:14:55.422950\n",
            "(4500, 1024)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 Load Data End Time:  2019-10-24 02:16:30.118240\n",
            "2019-10-24 02:16:30.127866\n",
            "2019-10-24 02:17:05.786412\n",
            "When image is of shape 32 Accuracy : 0.9096844396082698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sD3SC236ICb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54KzALIUIrmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORICAL_COLUMNS = []\n",
        "NUMERIC_COLUMNS = list(df_bst)[:-1]\n",
        "\n",
        "dftrain = df_bst.copy()\n",
        "#dftrain['label']= dftrain.label.astype(np.int)\n",
        "#y_train = dftrain.pop('label')\n",
        "  \n",
        "def one_hot_cat_column(feature_name, vocab):\n",
        "  return tf.feature_column.indicator_column(\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n",
        "                                                                vocab))\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  # Need to one-hot encode categorical features.\n",
        "  vocabulary = dftrain[feature_name].unique()\n",
        "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
        "  \n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
        "                                                          dtype=tf.float64))\n",
        "y_train = dftrain.pop('label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXBdnoIg6VC-",
        "colab_type": "code",
        "outputId": "14ca40f9-eb06-4b8e-d66c-c64869cced48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use entire batch since this is such a small dataset.\n",
        "\n",
        "NUM_EXAMPLES = len(y_train)\n",
        "print(NUM_EXAMPLES)\n",
        "\n",
        "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
        "  def input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
        "    if shuffle:\n",
        "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
        "    # For training, cycle thru dataset as many times as need (n_epochs=None).    \n",
        "    dataset = dataset.repeat(n_epochs)\n",
        "    # In memory training doesn't use batching.\n",
        "    dataset = dataset.batch(NUM_EXAMPLES)\n",
        "    return dataset\n",
        "  return input_fn\n",
        "\n",
        "# Training and evaluation input functions.\n",
        "train_input_fn = make_input_fn(dftrain, y_train, n_epochs=1)\n",
        "dfeval = df_bst_eval.copy()\n",
        "y_eval = dfeval.pop(\"label\")\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMnBdBLEjds7",
        "colab_type": "code",
        "outputId": "1c563f6e-a977-49ff-8d88-1fa58a15502f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "print(datetime.datetime.now())\n",
        "n_batches = 1\n",
        "est = tf.estimator.BoostedTreesClassifier(n_trees=2, n_classes=3,train_in_memory=True, feature_columns=feature_columns, n_batches_per_layer=n_batches, )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-22 00:11:42.564274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-e3aaa9f5f349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoostedTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches_per_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_columns, n_batches_per_layer, model_dir, n_classes, weight_column, label_vocabulary, n_trees, max_depth, learning_rate, l1_regularization, l2_regularization, tree_complexity, min_node_weight, config, center_bias, pruning_mode, quantile_sketch_epsilon, train_in_memory)\u001b[0m\n\u001b[1;32m   1871\u001b[0m       \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpruning_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For now pruning is not supported with multi class.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m     head, closed_form = _create_classification_head_and_closed_form(\n",
            "\u001b[0;31mValueError\u001b[0m: For now pruning is not supported with multi class."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4i8XN6CWIYX",
        "colab_type": "code",
        "outputId": "44cee9bb-8fba-40eb-ed66-2738caf6b04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "\n",
        "est.train(train_input_fn,max_steps=2)\n",
        "print(datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_rank_at_least\u001b[0;34m(x, rank, data, summarize, message, name)\u001b[0m\n\u001b[1;32m   1355\u001b[0m       assert_op = _assert_rank_condition(x, rank, static_condition,\n\u001b[0;32m-> 1356\u001b[0;31m                                          dynamic_condition, data, summarize)\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/check_ops.py\u001b[0m in \u001b[0;36m_assert_rank_condition\u001b[0;34m(x, rank, static_condition, dynamic_condition, data, summarize)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1160\u001b[0;31m             'Static rank condition failed', x_rank_static, rank_static)\n\u001b[0m\u001b[1;32m   1161\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'static_checks_determined_all_ok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ('Static rank condition failed', 1, array(2, dtype=int32))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-cef99f6fe0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1188\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1190\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1191\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1893\u001b[0m           \u001b[0mclosed_form_grad_and_hess_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosed_form\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m           \u001b[0mweight_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1895\u001b[0;31m           train_in_memory=train_in_memory)\n\u001b[0m\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m     super(BoostedTreesClassifier, self).__init__(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/boosted_trees.py\u001b[0m in \u001b[0;36m_bt_model_fn\u001b[0;34m(features, labels, mode, head, feature_columns, tree_hparams, n_batches_per_layer, config, closed_form_grad_and_hess_fn, example_id_column_name, weight_column, train_in_memory, name)\u001b[0m\n\u001b[1;32m   1335\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mtrain_op_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_train_op_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m       logits=logits)\n\u001b[0m\u001b[1;32m   1338\u001b[0m   \u001b[0;31m# Add an early stop hook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m   estimator_spec = estimator_spec._replace(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/head.py\u001b[0m in \u001b[0;36mcreate_estimator_spec\u001b[0;34m(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)\u001b[0m\n\u001b[1;32m    239\u001b[0m           self._create_tpu_estimator_spec(\n\u001b[1;32m    240\u001b[0m               \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m               regularization_losses))\n\u001b[0m\u001b[1;32m    242\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtpu_estimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_estimator_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/head.py\u001b[0m in \u001b[0;36m_create_tpu_estimator_spec\u001b[0;34m(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)\u001b[0m\n\u001b[1;32m   1266\u001b[0m       (training_loss, unreduced_loss, weights, processed_labels) = (\n\u001b[1;32m   1267\u001b[0m           self.create_loss(\n\u001b[0;32m-> 1268\u001b[0;31m               features=features, mode=mode, logits=logits, labels=labels))\n\u001b[0m\u001b[1;32m   1269\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mregularization_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularization_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/head.py\u001b[0m in \u001b[0;36mcreate_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     labels = _check_dense_labels_match_logits_and_reshape(\n\u001b[0;32m-> 1164\u001b[0;31m         labels=labels, logits=logits, expected_labels_dimension=1)\n\u001b[0m\u001b[1;32m   1165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_vocabulary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m       labels = lookup_ops.index_table_from_tensor(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/head.py\u001b[0m in \u001b[0;36m_check_dense_labels_match_logits_and_reshape\u001b[0;34m(labels, logits, expected_labels_dimension)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m'Suggested Fix: check your n_classes argument to the estimator '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         'and/or the shape of your label.'.format(expected_labels_dimension))\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0massert_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_rank_at_least\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0mstatic_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_rank_at_least\u001b[0;34m(x, rank, data, summarize, message, name)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         raise ValueError(\n\u001b[1;32m   1361\u001b[0m             \u001b[0;34m'%s.  Tensor %s must have rank at least %d.  Received rank %d, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             'shape %s' % (message, name, e.args[2], e.args[1], x.get_shape()))\n\u001b[0m\u001b[1;32m   1363\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: labels shape must be [D0, D1, ... DN, 1]. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label..  Tensor IteratorGetNext:4096 must have rank at least 2.  Received rank 1, shape (None,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU5y5lt9rorA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  print(datetime.datetime.now())\n",
        "  result = est.evaluate(eval_input_fn)\n",
        "  print(result)\n",
        "  print(datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTN_rMHhGlgR",
        "colab_type": "text"
      },
      "source": [
        "#### TFT PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TkirrD6FFv5",
        "colab_type": "code",
        "outputId": "560f94a1-c8aa-4dfd-8814-e75971fb9513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tfx\n",
        "import tensorflow_transform as tft\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/2b/2063d0520cfedf4f2008917f95cd46018f980be49d838d0359e552ca839e/tfx-0.14.0-py3-none-any.whl (384kB)\n",
            "\u001b[K     || 389kB 3.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-data-validation<0.15,>=0.14.1 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/86/77ec6a7c5c91ac69798fc3a9c911ff225a4c2833a42fb59d63c8162679e7/tensorflow_data_validation-0.14.1-cp36-cp36m-manylinux2010_x86_64.whl (2.4MB)\n",
            "\u001b[K     || 2.4MB 47.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.15,>=0.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/5e/e56ccce03d780d49755148978693f74c3da12f08e0b903417f5c711e0c12/tensorflow_model_analysis-0.14.0-py3-none-any.whl (777kB)\n",
            "\u001b[K     || 778kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.7 in /tensorflow-2.0.0/python3.6 (from tfx) (3.10.0)\n",
            "Requirement already satisfied: six<2,>=1.10 in /tensorflow-2.0.0/python3.6 (from tfx) (1.12.0)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.6/dist-packages (from tfx) (1.7.11)\n",
            "Collecting ml-metadata<0.15,>=0.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/15/902b399494dcb3d96f09795d55f72a1191ba33f3ae737600fd1d5db7f471/ml_metadata-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (4.8MB)\n",
            "\u001b[K     || 4.8MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7.0 in /usr/local/lib/python3.6/dist-packages (from tfx) (7.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b3/b6dcbd94bf8a5ae6a0be5fc988bdfb0b0dfb87ea37e788dc4dcc039a3aee/apache_beam-2.16.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     || 3.0MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<1,>=0.1.6 in /tensorflow-2.0.0/python3.6 (from tfx) (0.8.1)\n",
            "Collecting tensorflow-transform<0.15,>=0.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/84/c8770b330a3fbe4e6a727e3e922a04d3a755a79870e4ee090b959cb01983/tensorflow-transform-0.14.0.tar.gz (221kB)\n",
            "\u001b[K     || 225kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (5.5.0)\n",
            "Requirement already satisfied: pandas<1,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.24.2)\n",
            "Requirement already satisfied: tensorflow-metadata<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.14.0)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /tensorflow-2.0.0/python3.6 (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (1.17.2)\n",
            "Collecting scikit-learn<0.21,>=0.18 (from tensorflow-data-validation<0.15,>=0.14.1->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/5b/5da31a6572dc6b7b2846a7cfcbe2e060a0e6af0e1059a6516965e40371b7/scikit_learn-0.20.4-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K     || 5.4MB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<0.15.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.14.1)\n",
            "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.14.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.15,>=0.14->tfx) (7.5.1)\n",
            "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.15,>=0.14->tfx) (1.0.0)\n",
            "Collecting scipy==1.1.0 (from tensorflow-model-analysis<0.15,>=0.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     || 31.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /tensorflow-2.0.0/python3.6 (from protobuf<4,>=3.7->tfx) (41.4.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.4.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.11.3)\n",
            "Requirement already satisfied: grpcio<2,>=1.12.1 in /tensorflow-2.0.0/python3.6 (from apache-beam[gcp]<3,>=2.14->tfx) (1.24.1)\n",
            "Collecting dill<0.3.1,>=0.3.0 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
            "\u001b[K     || 153kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (0.16.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (3.9.0)\n",
            "Requirement already satisfied: pyyaml<4.0.0,>=3.12 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (3.13)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.7)\n",
            "Collecting avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b2/98a736a31213d3e281a62bcae5572cf297d2546bc429accf36f9ee1604bf/avro-python3-1.9.1.tar.gz\n",
            "Collecting mock<3.0.0,>=1.0.1 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     || 61kB 24.0MB/s \n",
            "\u001b[?25hCollecting oauth2client<4,>=2.0.1 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
            "\u001b[K     || 81kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (2018.9)\n",
            "Collecting fastavro<0.22,>=0.21.4 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/28/0206330c0002b1e28e21473117d0dc813defbd5891562d27af5c68c93899/fastavro-0.21.24-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     || 1.2MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.3.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     || 51kB 24.3MB/s \n",
            "\u001b[?25hCollecting python-dateutil<3,>=2.8.0 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     || 235kB 44.2MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/af/0ef7d097a1d5ad0c843867600e86de915e8ab8864740f49a4636cfb51af6/google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232kB)\n",
            "\u001b[K     || 235kB 56.8MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/32/df3e36fd705a00092f1ffa9f41ce1df8dcb594ae313d239b87861a41fc2e/google-apitools-0.5.28.tar.gz (172kB)\n",
            "\u001b[K     || 174kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.0.3)\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (3.1.1)\n",
            "Requirement already satisfied: google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.14.1)\n",
            "Collecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/07a82945a7396ea34debafd476724bb5fc267c292790fdf2138c693f95c5/google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118kB)\n",
            "\u001b[K     || 122kB 52.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/aa/29cbcf8cf7d08ce2d55b9dce858f7c632b434cb6451bed17cb4275804217/google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     || 92kB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (4.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (4.7.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (1.0.18)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.15,>=0.14->tensorflow-data-validation<0.15,>=0.14.1->tfx) (1.6.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.2.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.5.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.6.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (0.2.6)\n",
            "Collecting pbr>=0.11 (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/a4/d5c83831a3452713e4b4f126149bc4fbda170f7cb16a86a00ce57ce0e9ad/pbr-5.4.3-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     || 112kB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.14->tfx) (0.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]<3,>=2.14->tfx) (2.4.2)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (0.6.2)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (2.21.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx) (1.14.2)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3 (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Collecting fasteners>=0.14 (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx) (0.4.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.1.7)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.5.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (2.6.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (2.10.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.8.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.4.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (2.8)\n",
            "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (17.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.5.1)\n",
            "Building wheels for collected packages: tensorflow-transform, dill, avro-python3, oauth2client, hdfs, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for tensorflow-transform (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-transform: filename=tensorflow_transform-0.14.0-cp36-none-any.whl size=282799 sha256=5b9cba85c45a647c79318373af4fe5f4806c65994f0ce1958d27bcba2947cb43\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/8f/19/808f4a2d4d23a13b6ec44682fc2662646e8d9193b49f4a5f93\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.0-cp36-none-any.whl size=77513 sha256=b2bd9893b2674fd60c6f5a45d2ccab504b36d7f9d7d2e1c0349f2387a21a1ce8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.1-cp36-none-any.whl size=43199 sha256=1710edc59d19900898e43b5a0f5407108a5948f3e1830f8ffc468eef3ce3e9af\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/54/6f/a5df680fd3224aa45145686f3b1b02a878a90ea769fcf9daaf\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp36-none-any.whl size=106382 sha256=1965988aa8a6d928fb081e16e556a4f446b8a0773200d30d34d1b361ec03e3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33214 sha256=a00111aba82e515a7c2e6f61570c6e7b16c4342f5c6c2c87f4358a185babcd7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.28-cp36-none-any.whl size=130111 sha256=fa14ddf357d8df5a959bfc84770b42cf410337a1f0c70568befed78c0a5a226b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/c2/92/837e8a4d649a209dff85b38d7fbb576b4b480738be70865f29\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp36-none-any.whl size=18499 sha256=f7f803ce84f2c3d4e5fa7b361aaf1cd2eae892109f728ea6fcd61b7cc446a66f\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built tensorflow-transform dill avro-python3 oauth2client hdfs google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.9 has requirement dill>=0.3.1, but you'll have dill 0.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, scikit-learn, dill, avro-python3, pbr, mock, oauth2client, fastavro, hdfs, python-dateutil, grpc-google-iam-v1, google-cloud-bigtable, monotonic, fasteners, google-apitools, google-cloud-pubsub, google-cloud-datastore, apache-beam, tensorflow-transform, tensorflow-data-validation, tensorflow-model-analysis, ml-metadata, tfx\n",
            "  Found existing installation: scipy 1.3.1\n",
            "    Uninstalling scipy-1.3.1:\n",
            "      Successfully uninstalled scipy-1.3.1\n",
            "  Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "  Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "  Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Found existing installation: python-dateutil 2.5.3\n",
            "    Uninstalling python-dateutil-2.5.3:\n",
            "      Successfully uninstalled python-dateutil-2.5.3\n",
            "  Found existing installation: google-cloud-datastore 1.8.0\n",
            "    Uninstalling google-cloud-datastore-1.8.0:\n",
            "      Successfully uninstalled google-cloud-datastore-1.8.0\n",
            "Successfully installed apache-beam-2.16.0 avro-python3-1.9.1 dill-0.3.0 fastavro-0.21.24 fasteners-0.15 google-apitools-0.5.28 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-pubsub-1.0.2 grpc-google-iam-v1-0.12.3 hdfs-2.5.8 ml-metadata-0.14.0 mock-2.0.0 monotonic-1.5 oauth2client-3.0.0 pbr-5.4.3 python-dateutil-2.8.0 scikit-learn-0.20.4 scipy-1.1.0 tensorflow-data-validation-0.14.1 tensorflow-model-analysis-0.14.0 tensorflow-transform-0.14.0 tfx-0.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "google",
                  "oauth2client",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8b0329e58a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tfx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_transform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_transform/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspect_preprocessing_fn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosted_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_quantile_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosted_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantile_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusltMTwF9NZ",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression using Tensorflow Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb8bFUocuERf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "             tf.keras.layers.Flatten(input_shape=(512, 512,3))\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\")            \n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTbDTPvPNtRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=5\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=2\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDTHRX8Q7_2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V_RPsdvE4dP",
        "colab_type": "text"
      },
      "source": [
        "batch_size = 64\n",
        "steps_per_epoch = round(df.groupby(\"label\").agg(\"count\").reset_index()['pid'].max()/batch_size)\n",
        "\n",
        "print(\"Total Training Dataset : \", df.shape[0])\n",
        "print(\"Batch Size : \", batch_size)\n",
        "print(\"Steps per epoch : \", steps_per_epoch)\n",
        "print(\"Test Datasize shape : \", df_test.shape[0])\n",
        "\n",
        "history=model.fit_generator(returnABatch1d(df,batch_size)\n",
        "                  ,steps_per_epoch=steps_per_epoch\n",
        "                  , epochs=5\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_FIgewtHwKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qfFq4-MXHXs",
        "colab_type": "text"
      },
      "source": [
        "# Model Testing\n",
        "* is the model bias for 256 size images ?\n",
        "* is there any imbalance in 256 size images ?\n",
        "* converting 512x512 to 256x256 size would definitely speed up the process but would it impact the accuracy ?\n",
        "* is the model has better accuracy for any type of tumor class? (as we have imbalanced set ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym6uQGG8gydZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFTO4bLSGRBk",
        "colab_type": "text"
      },
      "source": [
        "# Observations / Lesson Learnt:\n",
        "\n",
        "* Iteration 1:\n",
        "  * CNN of 512x512 took half an hour even on TPU\n",
        "  * more and more convolution layer decreases the neurons required for training( duh!!!) and hence the batch size can be increased.\n",
        "  * testing result was 49%. Not Acceptable.\n",
        "\n",
        "* Iteration 2:\n",
        "  * tried PCA but realized transformation is not suited for convolution\n",
        "  * tried to use imagegenerator by converting the numpy to image (jpg) file. But found that numpy to image was not successfull.\n",
        "\n",
        "* Iteration 3:\n",
        "  * experimented with ImageGenerator for training, validation and testing\n",
        "  * time is still a worry for CNN approach; it is 20 mins\n",
        "  * though it is fast for the logistic regression the epochs are not helping (epoch = 5)\n",
        "\n",
        "\n",
        "* Iteration 4:\n",
        "  * experimented with different CNN layers ( basically added one more layer to make shrink the image size to 6x6 but 14x14 was quicker and gave better result)\n",
        "  * average time per epoch is 20 min and 5 epochs gave 89% test accuracy with CNN approach\n",
        "  * deviation in the epoch accuracy indicates that few more epoch can reach upto 90 - 93% which way ahead of last finding during capstone submission (54%).[validation accuracy has also increased so far and hence predicting the accuracy to go beyond 90%]\n",
        "  * imagenet transfer learing takes 1 hour to complete one epoch and for one epoch it gives accuracy around 45%\n",
        "  * 3 epochs on imagenet with 512x512x3 image with 2 convolution at the end with dense network post that gave 90% training accuracy, 65% validation and testing result. Clearly, there is overfit.\n",
        "  * shrinking the size by 256x256 or 150x150 (as per imagenet assumption on input), drastically reduces the epoch time upto 5 min but the accuracy do not improve even for 5 epochs. Loss decreases but there is no change in the accuracy or accuracy decreases.\n",
        "  * till now, it leaves with no additional lesson learnt apart from few experimental observations. i.e. in the last capstone submission, i had CNN and imagenet experiment. but this time different CNN gave a better result and imagenet is yet to match the outcome of CNN. Only new thing learnt is that simplest conv2d (3x3) with max pooling can give better result.\n",
        "  * as part of new learning , I have :\n",
        "    * tf eager execution, instead of graph based as in last submission\n",
        "    * image generator to simplify code, augment the images and to reshape( not recommended).\n",
        "    * keras way of working with transfer learning\n",
        "    * numpy to image save\n",
        "    * experimenting with workers and parallel processing at \"fit\" and \"fit_generator\".\n",
        "    * couple of experiments were not possible if TPU and colab was not arround.\n",
        "    * bokeh experiments with images.\n",
        "    * automating data load operation through linux commands (wget, unzipping, creation and deletion folders, moving files, listing files)\n",
        "\n",
        "* Iteration 5:\n",
        "  * Saving models:\n",
        "    * I guess below approach is good:\n",
        "      * while(1):\n",
        "          * model.fit()\n",
        "          * model.save('./final_model.h5', include_optimizer=True)\n",
        "\n",
        "* Iteration 6:\n",
        "  * Simple LR has too many loss\n",
        "  * NN also has high loss and hence low accuracy \n",
        "  * NN has limitation number of neurons with which python crashes if the initial neuron shape crosses 8K.\n",
        "\n",
        "\n",
        "* iteration 7:\n",
        "  * PCA + Normalization ( scaling between 0 to 1 ) gave a better result 40 % with PCA features of 1024\n",
        "    * PCA training /fit took half an hour\n",
        "    * Dense layer training took mere 3 min for 100 epochs\n",
        "\n",
        "  * PCA + Normalization ( scaling between 0 to 1 ) gave even better result 45%\n",
        "    * PCA training/fit took 1hr 15 min\n",
        "    * dense layer took around 5-8 min for 100 epochs\n",
        "\n",
        "  * evidentally, it is all about minimizing the loss. More number of features in PCA gives higher accuracy is the clear indication of lossess due to drastic step down from 200k to 1k or 2k neurons. So what are the losses we have in our pipeline?\n",
        "    * images are saved as jpg; at that time we have loss of scaling it to 0-255 UNSIGNED INTEGER.\n",
        "    * later when we do a normalization the loss is compounded.\n",
        "    * there may be loss in scaling and doing pca due to sklearn capability to work on float64 (if such constraints exists)\n",
        "\n",
        "\n",
        "* Iteration 8:\n",
        "  * WTF !!!!, np.int for image array gave LR an accuracy of 45%\n",
        "  * np.float64 for image array gave 72% accuracy\n",
        "  * np.float128 for image array gave 76% for LR classification.\n",
        "  * However, as anticipated Neural Network dint in herit this accuracy change.(45%)\n",
        "      * I though additional layers would fine tune the 1 LR layer but it seems they have introduced loss ( value of 8 for NN against 3 in LR). Where is the loss\n",
        "          * RELU : there may have been loss due to negative activation values. Just like that of PCA can have -ve value an activation value might have been negative and might have been thrown away by RELU.\n",
        "            * one more reason to suspect is that changes in the network apart from additional layer is introduction of activation function relu.\n",
        "\n",
        "        * Sigmoid : tried randomly sigmoid function. Pictorically, I can remember that the graph looks like letter \"S\" (slant), so I know that it handles either side of axes and hence should help if my hypothesis on relu loss.Loss definitely got reduced. Loss value looks better (0.9) when compared to both LR (3)  and relu(8)\n",
        "          * training took more 80 secs, because i vaguley remember that sigmoid has exponential function in its equation\n",
        "          * However, it looks promising, to run more epochs in search of reduced loss and increased accuracy.\n",
        "            * Note that here training accuracy was max at 66% but testing at 42% but it evident that epochs are too low (5 epochs).\n",
        "\n",
        "        * tanh: I know that it does not have exponential functions. Indeed there was reduction in training time (20 secs less per epoch compared to sigmoid : near 60 secs)\n",
        "            * accuracy around 45% again not enough epochs to conclude\n",
        "\n",
        "        * linear: suicidal.(as per my hypothesis) Negative values has to be controlled. Linear does NOT do that. LOSS is huge !!!\n",
        "          * training time : 1 min around, accuracy = 0.3\n",
        "        \n",
        "        * softsign: miniature version of sigmoid. <1 min training time , promissing accuracy ( 60:50).\n",
        "          * unlike expected model did not learn on increased epochs\n",
        "\n",
        "        * selu: Surprisingly failed. \n",
        "          * 61s 1s/step - loss: 11.3066 - accuracy: 0.3044 - Recall: 0.3044 - Precision: 0.3044\n",
        "\n",
        "\n",
        "    * PCA computation mandates that we need to have the entire dataset in memory and hence it is failing in my case.\n",
        "        * float128 for image array consumes most of the RAM leaving no room for PCA or NN\n",
        "\n",
        "Iteration 9: \n",
        "  * Data loading is a simplest task but I have made so many revisions\n",
        "    * with view of minimizing loss, fresh read from the file is consuming 80 seconds per epoch from few seconds!!!\n",
        "  * tensorflow does not support float128, max of float64 is feasible.\n",
        "  * new design is to create batches and save the batches and load it when generator is called.\n",
        "  * random.choices does not work; need to use random.sample instead\n",
        "  * the decrease in the accuracy may have been due to float32 default type\n",
        "    *  tf.keras.backend.floatx()\n",
        "  * I had to cut a layer of 4098 neurons to do the float64 computation. I also had to let go precision and recall computation for the same.\n",
        "  * However, convolution did prove that it is best even now. 3 epochs , 17 min each, 83% validation accuracy. We may take 2 more epochs to compare it with that of imagegenerator result. This comparison would tell us the impact of loss hypothesis.\n",
        "      * with 6x6 as the last convolution we had 90% accuracy (2.5 hour training)\n",
        "      * with 14x14 as the last convolution isze we had same 90% accuracy (1 hr training)\n",
        "      * in both the cases the model overfits, the training accuracy is 93-94% but testing is 90% (even with dropout before softmax)\n",
        "      * Guess it is fair enough to say that LR = 75%, CNN=90%; let us see boosted tree and mobilenet\n",
        "\n",
        "\n",
        "* Iteration 10:\n",
        "  * mobilenet says eager execution is not supported\n",
        "      * https://towardsdatascience.com/easy-image-classification-with-tensorflow-2-0-f734fee52d13 gives the quick review of txfr learning\n",
        "          * However, mobilenet, densenet requires 3 channel data. i.e. mxnx3 shape.\n",
        "              * easy np stack technique actually helps us to build 3 channel \n",
        "  * imagenet is too heavy takes 1 hr for an epoch\n",
        "\n",
        "* Iteration 11:\n",
        "  * Mobilenet or any transfer learning neural networks are industry standards for the image classification problem. We have to effectively use it. Current loss and accuracy is not acceptable. Though it might give the result as that of simple CNN network that we have seen before but it should not fail like this. So continuing the experiments in this area.\n",
        "  * Mobilenet gives a quick turn around: < 10 min\n",
        "  * top layer removed version is 16x16x1024 for input of 512x512x3. This leaves the model have more parameters to train than the frozen one. \n",
        "    * This is the reason we have huge loss and no learning from accuracy.\n",
        "      * tried a averagepooling (16x16) , increased training accuracy but no changes to validation accuracy. Overfitting!!!\n",
        "\n",
        "  * Also my first attempt was to put empty channels to make it 3 channel input to mobilenet. Would repeating the same 2d layers 2 times, give better result ?\n",
        "\n",
        "  * will reducing the size through convolution at first and then feeding the same to mobilenet give better result ? The intuition here is that mobilenet uses 221x221x3 input images and what we have is 512x512x3.\n",
        "\n",
        "  * Mobilenet as is model is not learning instead it overfits. Training accuracy increases but the validation score is static even after many epochs (>5)\n",
        "    * Epoch 3/20\n",
        "135/135 [==============================] - 500s 4s/step - loss: 4.6052 - accuracy: 0.7040 - val_loss: 8.1150 - val_accuracy: 0.4891\n",
        "        * as you could see the loss is also high\n",
        "\n",
        "  * Max pooling gave some ray of hope:\n",
        "  68/68 [==============================] - 441s 6s/step - loss: 0.8351 - accuracy: 0.6154 - val_loss: 3.1877 - val_accuracy: 0.4565\n",
        "\n",
        "  * Apperantly, transfer learning is not a silver bullet.\n",
        "    * tried with 3 channel input by first converting the 2d array to png image(imwrite) then loading the same through tf ImageGenerator\n",
        "    * tried squeezing the image as required by the mobilenet model.\n",
        "      * loss is more and no learning in training or validation\n",
        "    * tried putting max pool alone instead of averagepool. Accuracy floats arround 50%\n",
        "    * 1x1 convolution to reduce the feature space and then max pooling.\n",
        "      * there is loss and and no learning (around 30% accuracy)\n",
        "    * 2x2 or 3x3 convolutions followed by max pool between them\n",
        "        * again loss is huge.\n",
        "\n",
        "    * In summary, due to the size of the image the popular model end up in leaving the last layer dimension as 16x16x1024. This is 1/3rd of the 512x512x3 image. However, this is equal to the original 2d image size.Hence, the learning from this through average or few convolution layers are less.\n",
        "\n",
        "* iteration 12:\n",
        "  * Now let us see if boosted trees can help us or not.\n",
        "  * dataset had to be changed to dataframe as the boosted tree requires column names of the features.\n",
        "  * a dataframe grows in a iteration of image fetch. Currently, takes 20 min for training and 4 mins for testing\n",
        "  * with 512*512 feature columns training is pretty bad slow.\n",
        "    * not even 2 trees with 2 epochs are able to complete in an hour. with around 2200 samples.\n",
        "      *tft.pca might help us in this regard as we are able fit training data in memory\n",
        "\n",
        "* iteration 13: (Reason for MobileNet low performance)\n",
        "  * This iteration was dedicated for image segmentation and then prediction post it.\n",
        "  * while analyzing the code from tensorflow, I was also investingating the data injesting into it. Then several questions arose like what was the original data, how it got preprocessed/transformed.\n",
        "\n",
        "  * During that analysis, it stuck me that ImageGenerator is already converting the single channel image to 3 channel through np.stack approach. I was saving the normalized 512x512x1 array to jpg image. Though there was warning that there is a lossy conversion from 0 to 1 range data to 0 to 255, it did not happen. When the jpg was read it was still in 0 to 1 range. Moreover, we use to scale the image by x 1/255. It used to further reduce the numbers which are already scaled between 0 to 1. To add to that we had the tensor of type np.float32 and not np.float64. This was the reason for the **loss**.\n",
        "\n",
        "  * There was a need of single consolidated processing, rather than hdf5 file processing, storing it in numpy file then through custom generator reading the numpy file. I always know that tensorflow supports it but never explored much in there. Again thanks to tensorflow manuals came to know about the tensor handling through py\n",
        "  _functions which made the data preprocessing code reduced to around 20 lines and 5 minutes operation !!!! Even the train time performance of the dataset preprocessing is amazing - one epoch used to take around 16-18 minutes got reduced to 2.5 minutes!!!! Now I am wondering if the combination could be even more fantastic!!! \n",
        "    * i.e to save the numpy batch through tf.data\n",
        "    * during the train time read the numpy batch through tf.data\n",
        "\n",
        "  * But is the above 2 operation equivalent to caching ????\n",
        "    * Caching can also creates a file and access it lock based post one epoch. It is nice observation to see if the 2 stage data preparation is equivalent to the one stage ingestion with caching.\n",
        "\n",
        "* iteration 14: (reason for not to use boosted trees from tf)\n",
        "  * tf manual says that we cannot use boosted trees for multiclass classication\n",
        "  ```n_classes: number of label classes. Default is binary classification. Multiclass support is not yet implemented.```\n",
        "\n",
        "* iteration 15: xgboost\n",
        "  * xgboost gave 92% testing accuracy with 64x64 image size.This took less than 3 minutes !!!!\n",
        "  * however, label was one hot encoded, should it be a worry ?\n",
        "  * it once gave 96% with 128x128 image size but took around 13 minutes. However, this result was not consistent. It gave 92% in the second run. Currently, checking with more number of trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhZvQSsD0r7U",
        "colab_type": "text"
      },
      "source": [
        "#### Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z262X7bM5kZ1",
        "colab_type": "text"
      },
      "source": [
        "* Iteration 16: xgboost + tumor neighbors\n",
        "  * though we have taken 64 pixel from neighboring tumor region. XGBOOST didnt give good result.\n",
        "  * I have my doubts regarding scaling. When we scale first and then do resize, resulting matrix is scaled ???\n",
        "    * No, it made no difference if normalization is done before or after resize to xgboost.\n",
        "\n",
        "\n",
        "* Iteration 17: CNN with TF DATA\n",
        "  * tf data + cache save 50% of the time with a tracjectory of accuracy gain over the epochs as that of the older one.\n",
        "  * filter depth is kept constant across different layers and in each experiment it is decreased to half. i.e. from 64 to 32,16,8. In all the readings, Initial few epochs showed same training and validation loss but the start point of the accuracy are different\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iThtN8-UQc4Y",
        "colab_type": "text"
      },
      "source": [
        "# To Do\n",
        "* Batch normalization for both CNN and LR\n",
        "* dropout between each dense layer\n",
        "* is LR with PCA ( n = say 10K) equal to NN with cone structure ?\n",
        "  * as in, cone structure (decreasing stacked NN layer neurons) reduces the feature vector at each layer so is this a parallel approach to PCA ?\n",
        "\n",
        "* NN Experiments:\n",
        "  * adding addtional CNN layer to make the last dimention as 14x14 or 6x6\n",
        "      * 6x6 took additional time for one epoch and also took addional epochs to reach the accuracy as that of 14x14.\n",
        "      \n",
        "  * PCA and LR:\n",
        "    * 1 million (half of flattened layer) PCA dimention\n",
        "    * 50K PCA dimention\n",
        "    * 10K PCA dimention\n",
        "    * 5k\n",
        "    * 1K\n",
        "    * 512\n",
        "    * 256\n",
        "  * NN layers\n",
        "    * 100K, 50K, 25K, 12K, 6K, 3K, 1K, 512, 256, 128, 64, 32, 16, 8\n",
        "      * batch normalization\n",
        "      * drop outs\n",
        "\n",
        "* Preprocessing:\n",
        "  * skull removal through clustering\n",
        "  * image generator to reduce the size to 256x256\n",
        "\n",
        "\n",
        "* transfer learning\n",
        "* image segmentation\n",
        "  * try with only window of image with tumor for accuracy\n",
        "  * how is image segmentation actually done\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYzFII_AvYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in train_generator:\n",
        "  print(i[1].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZKA21TJB1c7",
        "colab_type": "code",
        "outputId": "e35b2a92-bfb7-4db5-aa8b-5b947d0e695f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import numpy as np\n",
        "img = np.array([[1, 2], [3, 4]])\n",
        "print(img)\n",
        "stacked_img = np.stack((img,)*3, axis=-1)\n",
        "print(stacked_img.shape)\n",
        "\n",
        "np.zeros((512,512),)\n",
        "\n",
        "np.stack(([[1,2],[3,4]], [[0,0],[0,0]]),axis=-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "(2, 2, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 0],\n",
              "        [2, 0]],\n",
              "\n",
              "       [[3, 0],\n",
              "        [4, 0]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB0oEZYiGzJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imageflatdf():\n",
        "  imageflatlist=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  #for d in os.listdir( \"training_data/\"):\n",
        "  #  for f in os.listdir(\"training_data/\"+d+\"/\"):\n",
        "  for file_name in os.listdir(\"download/mat\"):\n",
        "      #plt.imshow(plt.imread(\"training_data/1/\"+f))\n",
        "      #print(plt.imread(\"training_data/1/\"+f).reshape(1,-1).shape)\n",
        "      with h5py.File(\"download/mat/\"+file_name,'r') as f:\n",
        "        image_array = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "        label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "        image_array = image_array/image_array.max()\n",
        "        if image_array.shape[0] == 512:\n",
        "          imageflatlist.append((list(image_array.reshape(-1)),label_transform[label]))\n",
        "        else:\n",
        "          image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          imageflatlist.append((list(image_array.reshape(-1)),label_transform[label])) \n",
        "      #break\n",
        "  df = pd.DataFrame(imageflatlist,columns=['image_array','label'])\n",
        "  return df\n",
        "\n",
        "df_flat = imageflatdf()\n",
        "df_flat.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukVkA3BZ3N5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(df):\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    return pca\n",
        "\n",
        "pca = test(df_flat.copy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxth9ywUYDXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv50XhoNSkiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.groupby(\"label\").agg(\"count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SXp2d-O0qmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.image_array[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDZHQnZy0Rea",
        "colab_type": "text"
      },
      "source": [
        "#PCA + LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOaYkAXtBY2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(len(os.listdir(\"/content/validation_data/npz/\"))//32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgl8ShdQPwZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doLR():\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(3, input_shape=[512*512], activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  batch_size=32\n",
        "  model.fit_generator(myTrainGenerator(batch_size)\n",
        "          ,epochs=100\n",
        "          ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "          ,validation_data=myValidateGenerator(batch_size)\n",
        "          ,validation_steps=len(os.listdir(\"validation_data/npz/\"))\n",
        "          )\n",
        "  \n",
        "  return model.evaluate(myTestGenerator(batch_size))\n",
        "\n",
        "doLR()\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHxm_SkUzAWv",
        "colab_type": "text"
      },
      "source": [
        "def doLR(df):\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  #df['label'] = pd.Categorical(df['label'])\n",
        "  #df['label'] = df.label.astype(np.int)\n",
        "\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(3, input_shape=[262144], activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label))\n",
        "  \n",
        "  for feat, targ in dataset.take(5):\n",
        "    print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "  train_dataset = dataset.batch(50)\n",
        "  #model.fit(dataset)\n",
        "  model.fit_generator(train_dataset,epochs=100)\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label))\n",
        "  test_dataset = dataset.batch(50)\n",
        "  return model.evaluate(test_dataset)\n",
        "\n",
        "doLR(df_flat.copy())\n",
        "print(\"End :\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "120GB0T8wq4H",
        "colab_type": "text"
      },
      "source": [
        "def tfData(df):\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label)).batch(50)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label)).batch(50)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "train_dataset, test_dataset = tfData(df_flat.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-F-tPj37a4T",
        "colab_type": "text"
      },
      "source": [
        "def myGenerator(df,batch_size):\n",
        "  for i in range(0, df.shape[0],batch_size):\n",
        "    yield df.image_array.values[i:i+batch_size], df.label.values[i:i+batchsize]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAjr-uFWOGm",
        "colab_type": "code",
        "outputId": "8d85edaa-99c0-457d-c08f-5992bf4c97d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "def doNN():\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  try:\n",
        "    model.reset_states()\n",
        "  except:\n",
        "    print(\"skipping model reset\")\n",
        "    pass\n",
        "  \n",
        "  activation_function = \"relu\"\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(2048, input_shape=[512*512], activation=activation_function) \n",
        "            #,tf.keras.layers.Dropout(0.5)\n",
        "            #,tf.keras.layers.Dense(2048, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(1024, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(512, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(128, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              #,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  batch_size=32\n",
        "  model.fit_generator(myTrainGenerator(batch_size)\n",
        "          ,epochs=100\n",
        "          ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "          ,validation_data=myValidateGenerator(batch_size)\n",
        "          ,validation_steps=len(os.listdir(\"validation_data/npz/\"))\n",
        "          )\n",
        "  result =  model.evaluate(myTestGenerator(batch_size))\n",
        "  model.reset_states()\n",
        "  return result\n",
        "\n",
        "doNN()\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-602e7546170a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdoNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-602e7546170a>\u001b[0m in \u001b[0;36mdoNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdoNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJw1KmYsH_Pj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "def doNN(df):\n",
        "  #tf.reset_default_graph()\n",
        "  #df['label'] = pd.Categorical(df['label'])\n",
        "  #df['label'] = df.label.astype(np.int)\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  \n",
        "  activation_function = \"softsign\"\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4098, input_shape=[262144], activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(2048, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(1024, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(512, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(128, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "\n",
        "  #dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label))\n",
        "  \n",
        "  #for feat, targ in train_dataset.take(5):\n",
        "  #  print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "  #train_dataset = dataset.batch(50)\n",
        "  #model.fit(dataset)\n",
        "  model.fit_generator(train_dataset, epochs=5)\n",
        "  \n",
        "  #dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label))\n",
        "  #test_dataset = dataset.batch(50)\n",
        "  return model.evaluate(test_dataset)\n",
        "\n",
        "doNN(df_flat.copy())\n",
        "print(\"End :\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmFSRH0IVPc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myPca:\n",
        "  def doPCANN(df):\n",
        "    print(\"Start :\", datetime.datetime.now())\n",
        "    #df['label'] = pd.Categorical(df['label'])\n",
        "    #df['label'] = df.label.astype(np.int)\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.Session().reset()\n",
        "\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"image_array_pca\"] = df.image_array.apply(lambda x: scaler.fit_transform(pca.transform(x.reshape(1, -1))[0].reshape(1, -1))[0])\n",
        "    print(\"Post PCA :\", datetime.datetime.now())\n",
        "    df_test = df.sample(frac=0.2)\n",
        "    df_train = df.drop(df_test.index)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "              #tf.keras.layers.Dense(4098, input_shape=[1024], activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(2048, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(1024, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(512, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(128, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              tf.keras.layers.Dense(3, input_shape=[1024], activation=\"softmax\") \n",
        "              ])\n",
        "    model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array_pca, df_train.label))\n",
        "    \n",
        "    for feat, targ in dataset.take(5):\n",
        "      print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "    train_dataset = dataset.batch(50)\n",
        "    #model.fit(dataset)\n",
        "    model.fit_generator(train_dataset,epochs=5)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array_pca, df_test.label))\n",
        "    test_dataset = dataset.batch(50)\n",
        "    return model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Result :\",myPca.doPCANN(df_flat.copy()))\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjzPG58tUsC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doPCA(df):\n",
        "  import numpy as np\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components=2,whiten=True)\n",
        "\n",
        "  print(np.array(df.image_array.to_list()).shape)\n",
        "  pca.fit(df.image_array.to_list())\n",
        "  #pca.transform(image[0].reshape((1,-1)))\n",
        "  #return [pca.transform(image) for image in df.image_array.to_list() if image.shape[0] == 262144]\n",
        "  df['x'] = df.image_array.apply(lambda x: pca.transform(x.reshape(1, -1))[0][0] if x.shape[0]== 262144 else 0)\n",
        "  df['y'] = df.image_array.apply(lambda x: pca.transform(x.reshape(1, -1))[0][1] if x.shape[0]== 262144 else 0)\n",
        "\n",
        "  return df\n",
        "\n",
        "x_y = doPCA(df_flat.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL06EyYxKWab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_y.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qy4t7upMSQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_y['label'] = x_y.label.astype(np.int)\n",
        "x_y.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce37jsZKvMFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotp(x_y):\n",
        "  plt.scatter(x_y['x'],x_y['y'],c=x_y['label'])\n",
        "  plt.show()\n",
        "\n",
        "plotp(x_y[x_y.label == 1])\n",
        "plotp(x_y[x_y.label == 2])\n",
        "plotp(x_y[x_y.label == 3])\n",
        "plotp(x_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1YBCCWqv3l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(plt.imread(\"training_data/1/1.jpg\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk3PzIHrwjJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_image = plt.imread(\"training_data/1/1.jpg\").copy()\n",
        "ma = temp_image.max()*.8\n",
        "print(ma)\n",
        "temp_image[temp_image > ma]= 0\n",
        "plt.imshow(temp_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mytrMAChxR6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doCluster(df):\n",
        "  from sklearn import cluster\n",
        "  k_means = cluster.KMeans(n_clusters=2, n_init=4)\n",
        "  k_means.fit(df.image_array.to_list())\n",
        "\n",
        "  return df\n",
        "\n",
        "tt = doCluster(df_flat.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35CZ3PUb0nGZ",
        "colab_type": "text"
      },
      "source": [
        "##### Incremental PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNa27eyz0pi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def returnImageLabel(file_list):\n",
        "  image_list=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  for file_name in file_list:\n",
        "    \n",
        "    with h5py.File(\"download/mat/\"+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] == 512:\n",
        "            image_list.append((list(image_array.reshape(-1)),label_transform[label]))\n",
        "          else:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "            image_list.append((list(image_array.reshape(-1)),label_transform[label])) \n",
        "  return np.array(image_list)\n",
        "\n",
        "def myGenerator(batch_size):\n",
        "  files = os.listdir(\"download/mat\")\n",
        "  for i in itertools.cycle(range(0,len(files),batch_size)):\n",
        "    yield returnImageLabel(files[i:i+batch_size])\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0q7WmR80yb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myPca:\n",
        "  def doPCANN():\n",
        "    print(\"Start :\", datetime.datetime.now())\n",
        "    #df['label'] = pd.Categorical(df['label'])\n",
        "    #df['label'] = df.label.astype(np.int)\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.Session().reset()\n",
        "\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"image_array_pca\"] = df.image_array.apply(lambda x: scaler.fit_transform(pca.transform(x.reshape(1, -1))[0].reshape(1, -1))[0])\n",
        "    print(\"Post PCA :\", datetime.datetime.now())\n",
        "    df_test = df.sample(frac=0.2)\n",
        "    df_train = df.drop(df_test.index)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "              #tf.keras.layers.Dense(4098, input_shape=[1024], activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(2048, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(1024, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(512, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(128, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              tf.keras.layers.Dense(3, input_shape=[1024], activation=\"softmax\") \n",
        "              ])\n",
        "    model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array_pca, df_train.label))\n",
        "    \n",
        "    for feat, targ in dataset.take(5):\n",
        "      print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "    train_dataset = dataset.batch(50)\n",
        "    #model.fit(dataset)\n",
        "    model.fit_generator(train_dataset,epochs=5)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array_pca, df_test.label))\n",
        "    test_dataset = dataset.batch(50)\n",
        "    return model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Result :\",myPca.doPCANN())\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}