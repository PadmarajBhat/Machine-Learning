{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BrainTumorClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Machine-Learning/blob/master/BrainTumorClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCyCln9DwLS",
        "colab_type": "text"
      },
      "source": [
        "# Detection of 3 Brain Tumors (Meningioma, Glioma and Pituitary) in T1-weighted contrast enhanced images\n",
        "\n",
        "### - Revisitng the Udacity Capstone Project in pursuit of better accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExaNYOAl3nSH",
        "colab_type": "text"
      },
      "source": [
        "# What is the problem statement?\n",
        "  * predict the tumor class given a MRI image\n",
        "  * OR predict the tumor class when both MRI and Tumor region is given !!!\n",
        "      * tumor region is identified and put in input dataset by experts\n",
        "          * can we have Image Segmentation problem ?\n",
        "\n",
        "\n",
        "  * I think this is the order of problem from easy level to difficult level\n",
        "    * Identify the tumor class from raw MRI image (here accuracy may be low)\n",
        "    * Identify the tumor class from raw MRI image with tumor region identified info (here accuracy may be better)\n",
        "    * Auto detect the tumor segment in a MRI image and classify the tumor (ideal application for a radiologist)\n",
        "\n",
        "    Let us try all the 3 !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47aIRBi1lPdJ",
        "colab_type": "text"
      },
      "source": [
        "# Import Packages\n",
        "* read the input MRI images (.mat) files through ***h5py***\n",
        "* **bokeh** plot for the zoomed in analysis of a tumor and neighbors\n",
        "* ***pandas*** for data analysis and preprocessing\n",
        "* ***tensorflow*** for modelling and predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzYp6q2F5Doj",
        "colab_type": "code",
        "outputId": "98d2c6f6-3289-4840-f90f-3afcfe8286a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XamypXiCEdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from bokeh.io import output_notebook, show\n",
        "from bokeh.layouts import row\n",
        "from bokeh.plotting import figure\n",
        "output_notebook()\n",
        "\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import random # for radom selection from a list\n",
        "import datetime\n",
        "import itertools\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8abLHDC4ut4",
        "colab_type": "code",
        "outputId": "7503ccc6-72a7-4185-bc50-d377a2c4e976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E7e82UO_cLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a Global variable\n",
        "tumor_names = [\"\",\"Meningioma\",\"Glioma\",\"Pituitary\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68mR7i5xla0f",
        "colab_type": "text"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "* **Iteration 1**:\n",
        "    * Mount Google Drive\n",
        "    * Unzip it in colab disk\n",
        "    * load mat attributes to list of tuples ( with mri and tumor 5 point summary details)\n",
        "    * create a panda dataframe for analysis\n",
        "\n",
        "    ##### Issues Faced:\n",
        "    1. loading to panda with image took half(6GB) of RAM\n",
        "    * loading tumor along with mri image (as in mat file) crashed the colab\n",
        "      * Solution: let us load image but save only 5 point summary for both mri image and tumor\n",
        "\n",
        "    2. How do we scale/normalize the data?\n",
        "      * would tumor region have 0 in it ?\n",
        "        * only way to know is through the value present in the binary indicator == 1\n",
        "            * implementation through 2 for loops takes forever !!!\n",
        "              * need to implement throuhg np.where...a[a == 1] !!!\n",
        "            \n",
        "\n",
        "    3. Some images are less than 512\n",
        "        * pad the difference with 0s.\n",
        "        \n",
        "    4. Should tumor image be scaled between 0 -1? For now, brightness values are relative to that of the whole image to which it belongs to.\n",
        "\n",
        "    5. Epoch run failed due to no data generated by the custom generator.\n",
        "      * Going to try the ImageGenerator from the TF.\n",
        "\n",
        "\n",
        "* **Iteration 2** :\n",
        "  * ImageGenerator worked fine but the np to image conversion had used dtype of np.uint8 to avoid warning during saving to image. However, that lead to corrupt image and hence loss was more and accuracy was less.\n",
        "  * Generator built for the iteration is correct ?\n",
        "      * are *flips* is not damaging data\n",
        "\n",
        "* **Iteration 3** :\n",
        "  * validate the image augmentation in the ImageGenerator\n",
        "  * initial load the data was fine. zip based train and test failed\n",
        "\n",
        "* **Iteration 4**:\n",
        "  * open all mat(hdf5) files and load 5 point summary of mri and tumor to a panda df\n",
        "  * save all the numpy mri image array to training_data directory\n",
        "  * split the df to training : testing = 80 :20\n",
        "  * move the 20% of the testing to testing_data\n",
        "  * out of the 80% training data, move the 10% for the validation during training\n",
        "      * download\n",
        "        * 5\n",
        "        * b*.zip\n",
        "        * *.txt\n",
        "        * mat\n",
        "          * *.mat\n",
        "\n",
        "      * training_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "      * validation_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "      * testing_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6trYXPZPJ54V",
        "colab_type": "text"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWr8OsJ1ar6P",
        "colab_type": "text"
      },
      "source": [
        "##### Google Drive File Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWQcmRvRFD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf training_data validation_data testing_data download"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCtee7XQbSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1998fe1b-4d35-47d8-b706-d8a10ea67be0"
      },
      "source": [
        "def loadMatFiles(dir=\"training_data/\"):\n",
        "  if not os.path.isfile(\"download/mat/1.mat\"):\n",
        "    cmds = [\n",
        "            \"rm -rf download data training_data validation_data testing_data\"\n",
        "            ,\"wget https://ndownloader.figshare.com/articles/1512427/versions/5 -P download\"\n",
        "            ,\"unzip download/5 -d download\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_1-766.zip -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_1533-2298.zip -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_767-1532.zip   -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_2299-3064.zip  -d download/mat\"]\n",
        "\n",
        "    for c in cmds:\n",
        "      os.system(c)\n",
        "    time.sleep(5)\n",
        "  else:\n",
        "    print(\"mat files are loaded into download/mat directory\")\n",
        "\n",
        "loadMatFiles()\n",
        "print(\"Total Data : \", len(os.listdir(\"download/mat\")))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Data :  3064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUa5H7fsQkxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "843a751b-636a-4800-af29-f623367cda94"
      },
      "source": [
        "if not os.path.isdir(\"training_data/\"):\n",
        "\n",
        "  if not os.path.isdir(\"download/mat\") or not len(os.listdir(\"download/mat\")) :\n",
        "    loadMatFiles()\n",
        "\n",
        "  os.system(\"mkdir training_data validation_data testing_data\")\n",
        "\n",
        "  files = os.listdir(\"download/mat\").copy()\n",
        "  training_files = random.sample(files, k=round(len(files) *.7))\n",
        "\n",
        "  for file in training_files:\n",
        "    try:\n",
        "      os.rename(\"download/mat/\"+file, \"training_data/\"+file)\n",
        "    except:\n",
        "      print(\"Skipping :\", file)\n",
        "  time.sleep(5)\n",
        "\n",
        "print(\"Training Data:\", len(os.listdir(\"training_data/\")))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data: 2145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijgDKtrSQsgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5388b6bc-7c33-4de6-8a7f-2de15c5c88f5"
      },
      "source": [
        "files = os.listdir(\"download/mat\").copy()\n",
        "validation_files = random.sample(files, k=round(len(files) *.1))\n",
        "\n",
        "for file in validation_files:\n",
        "  try:\n",
        "    os.rename(\"download/mat/\"+file, \"validation_data/\"+file)\n",
        "  except:\n",
        "    print(\"Skipping :\", file)\n",
        "time.sleep(5)\n",
        "print(\"Validation Data:\", len(os.listdir(\"validation_data/\")))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data: 175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_9jSTFhQyeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f07502c-b3e6-41b7-d80a-1aa935d96273"
      },
      "source": [
        "for file in os.listdir(\"download/mat/\"):\n",
        "      if not (file in os.listdir(\"training_data/\") or file in os.listdir(\"validation_data\")):\n",
        "        try:\n",
        "          os.rename(\"download/mat/\"+file, \"testing_data/\"+file)\n",
        "        except:\n",
        "          print(\"Skipping :\", file)\n",
        "time.sleep(5)\n",
        "print(\"Testing Data:\", len(os.listdir(\"testing_data/\")))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Data: 744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JiGOCo1vjPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def returnImageLabel(loc, file_list):\n",
        "  image_list=[]\n",
        "  label_list=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  for file_name in file_list:\n",
        "    \n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          \n",
        "          #image_list.append((image_array,  label_transform[label]))\n",
        "          image_list.append(image_array)\n",
        "          label_list.append(label_transform[label])\n",
        "  return np.array(image_list), np.array(label_list)\n",
        "  #df = pd.DataFrame(image_list,columns=[\"image_array\",\"label\"])\n",
        "  #return df\n",
        "\n",
        "def myTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myValidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrXoZw8BxUR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55f258cb-ea8d-4b8d-9a2f-fdcbe068c5b1"
      },
      "source": [
        "def npSaver(loc):\n",
        "  \n",
        "  files = os.listdir(loc).copy()\n",
        "  os.system(\"mkdir \"+loc+\"npz/\")\n",
        "  for i in range(0,len(files),32):\n",
        "    x,y = returnImageLabel(loc, files[i:i+32])\n",
        "    print(loc+\"npz/\"+str(i)+\".npz\")\n",
        "    np.savez(loc+\"/npz/\"+str(i), x=x, y=y)\n",
        "    #time.sleep(1)\n",
        "    #np.save(loc+str(i),np.array(returnImageLabel(loc, files[i:i+32])))\n",
        "\n",
        "if not os.path.isfile(\"training_data/npz/0.npz\"):\n",
        "  npSaver(\"training_data/\")\n",
        "print(\"Training Batch Files :\", len(os.listdir(\"training_data/npz\")))\n",
        "\n",
        "if not os.path.isfile(\"validation_data/npz/0.npz\"):\n",
        "  npSaver(\"validation_data/\")\n",
        "print(\"Validation Batch Files :\", len(os.listdir(\"validation_data/npz\")))\n",
        "\n",
        "if not os.path.isfile(\"testing_data/npz/0.npz\"):\n",
        "  npSaver(\"testing_data/\")\n",
        "print(\"Testing Batch Files :\", len(os.listdir(\"testing_data/npz\")))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_data/npz/0.npz\n",
            "training_data/npz/32.npz\n",
            "training_data/npz/64.npz\n",
            "training_data/npz/96.npz\n",
            "training_data/npz/128.npz\n",
            "training_data/npz/160.npz\n",
            "training_data/npz/192.npz\n",
            "training_data/npz/224.npz\n",
            "training_data/npz/256.npz\n",
            "training_data/npz/288.npz\n",
            "training_data/npz/320.npz\n",
            "training_data/npz/352.npz\n",
            "training_data/npz/384.npz\n",
            "training_data/npz/416.npz\n",
            "training_data/npz/448.npz\n",
            "training_data/npz/480.npz\n",
            "training_data/npz/512.npz\n",
            "training_data/npz/544.npz\n",
            "training_data/npz/576.npz\n",
            "training_data/npz/608.npz\n",
            "training_data/npz/640.npz\n",
            "training_data/npz/672.npz\n",
            "training_data/npz/704.npz\n",
            "training_data/npz/736.npz\n",
            "training_data/npz/768.npz\n",
            "training_data/npz/800.npz\n",
            "training_data/npz/832.npz\n",
            "training_data/npz/864.npz\n",
            "training_data/npz/896.npz\n",
            "training_data/npz/928.npz\n",
            "training_data/npz/960.npz\n",
            "training_data/npz/992.npz\n",
            "training_data/npz/1024.npz\n",
            "training_data/npz/1056.npz\n",
            "training_data/npz/1088.npz\n",
            "training_data/npz/1120.npz\n",
            "training_data/npz/1152.npz\n",
            "training_data/npz/1184.npz\n",
            "training_data/npz/1216.npz\n",
            "training_data/npz/1248.npz\n",
            "training_data/npz/1280.npz\n",
            "training_data/npz/1312.npz\n",
            "training_data/npz/1344.npz\n",
            "training_data/npz/1376.npz\n",
            "training_data/npz/1408.npz\n",
            "training_data/npz/1440.npz\n",
            "training_data/npz/1472.npz\n",
            "training_data/npz/1504.npz\n",
            "training_data/npz/1536.npz\n",
            "training_data/npz/1568.npz\n",
            "training_data/npz/1600.npz\n",
            "training_data/npz/1632.npz\n",
            "training_data/npz/1664.npz\n",
            "training_data/npz/1696.npz\n",
            "training_data/npz/1728.npz\n",
            "training_data/npz/1760.npz\n",
            "training_data/npz/1792.npz\n",
            "training_data/npz/1824.npz\n",
            "training_data/npz/1856.npz\n",
            "training_data/npz/1888.npz\n",
            "training_data/npz/1920.npz\n",
            "training_data/npz/1952.npz\n",
            "training_data/npz/1984.npz\n",
            "training_data/npz/2016.npz\n",
            "training_data/npz/2048.npz\n",
            "training_data/npz/2080.npz\n",
            "training_data/npz/2112.npz\n",
            "training_data/npz/2144.npz\n",
            "Training Batch Files : 68\n",
            "validation_data/npz/0.npz\n",
            "validation_data/npz/32.npz\n",
            "validation_data/npz/64.npz\n",
            "validation_data/npz/96.npz\n",
            "validation_data/npz/128.npz\n",
            "validation_data/npz/160.npz\n",
            "Validation Batch Files : 6\n",
            "testing_data/npz/0.npz\n",
            "testing_data/npz/32.npz\n",
            "testing_data/npz/64.npz\n",
            "testing_data/npz/96.npz\n",
            "testing_data/npz/128.npz\n",
            "testing_data/npz/160.npz\n",
            "testing_data/npz/192.npz\n",
            "testing_data/npz/224.npz\n",
            "testing_data/npz/256.npz\n",
            "testing_data/npz/288.npz\n",
            "testing_data/npz/320.npz\n",
            "testing_data/npz/352.npz\n",
            "testing_data/npz/384.npz\n",
            "testing_data/npz/416.npz\n",
            "testing_data/npz/448.npz\n",
            "testing_data/npz/480.npz\n",
            "testing_data/npz/512.npz\n",
            "testing_data/npz/544.npz\n",
            "testing_data/npz/576.npz\n",
            "testing_data/npz/608.npz\n",
            "testing_data/npz/640.npz\n",
            "testing_data/npz/672.npz\n",
            "testing_data/npz/704.npz\n",
            "testing_data/npz/736.npz\n",
            "Testing Batch Files : 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZfJOqJ16mbg",
        "colab_type": "text"
      },
      "source": [
        "[os.system(\"rm -rf \"+\"training_data/\"+f) for f in os.listdir(\"training_data/\") if \".np\" in f or \".csv\" in f]\n",
        "[os.system(\"rm -rf \"+\"validation_data/\"+f) for f in os.listdir(\"validation_data/\") if \".np\" in f or \".csv\" in f]\n",
        "[os.system(\"rm -rf \"+\"testing_data/\"+f) for f in os.listdir(\"testing_data/\") if \".np\" in f or \".csv\" in f]\n",
        "!rm -rf \"training_data/npz validation_data/npz testing_data/npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyh_sPtY7UMy",
        "colab_type": "text"
      },
      "source": [
        "### Load Image Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoGqScRpu7F5",
        "colab_type": "text"
      },
      "source": [
        "def retrieveImage(file_name):\n",
        "  f = h5py.File(file_name,'r')\n",
        "  mri_image = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return mri_image/mri_image.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_gT4oC7bYb",
        "colab_type": "text"
      },
      "source": [
        "### Load Tumor Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcPw_HqJ7pkX",
        "colab_type": "text"
      },
      "source": [
        "def retrieveTumorImage(file_name):\n",
        "  f = h5py.File(file_name,'r')\n",
        "  mri_image = np.array(f['cjdata']['tumorMask'],dtype=np.float128)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return mri_image/mri_image.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-c5y3LXGnCE",
        "colab_type": "text"
      },
      "source": [
        "### Create Directories for ImageGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry3jlMuXKdmD",
        "colab_type": "text"
      },
      "source": [
        "!mkdir \"training_data/images/\"\n",
        "!mkdir \"training_data/images/1\"\n",
        "!mkdir \"training_data/images/2\"\n",
        "!mkdir \"training_data/images/3\"\n",
        "\n",
        "!mkdir \"testing_data/images/\"\n",
        "!mkdir \"testing_data/images/1\"\n",
        "!mkdir \"testing_data/images/2\"\n",
        "!mkdir \"testing_data/images/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF_-ayUh7jlS",
        "colab_type": "text"
      },
      "source": [
        "### Load Image and Tumor Statistics to Panda\n",
        "\n",
        "*   Panda df would have 5 point summary of both mri and tumor\n",
        "*   data directory will have label wise subdirectories for ImageGenerator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUvYbhjEzet",
        "colab_type": "text"
      },
      "source": [
        "def return_imageInfo_from_mat_file(dir,file_name):\n",
        "    f = h5py.File(dir+file_name,'r')\n",
        "\n",
        "    mri_image = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "    #scaler = MinMaxScaler(feature_range=(1,2))\n",
        "    #mri_image = scaler.fit(mri_image)\n",
        "    mri_image = mri_image/mri_image.max()\n",
        "\n",
        "    if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "    \n",
        "    temp_mri_image = np.copy(mri_image)\n",
        "    temp_mri_image[temp_mri_image == 0 ] = 2\n",
        "\n",
        "    mri_quartiles = np.percentile(mri_image[mri_image > 0], [25, 50, 75,80,85,90,95,96,97,98,99])\n",
        "\n",
        "    tumor_image = np.array(f['cjdata']['tumorMask'], dtype=np.float128)\n",
        "    if tumor_image.shape[0] < 512:\n",
        "      print(\"Shape of the tumor image : \", tumor_image.shape)\n",
        "      tumor_image = np.pad(tumor_image,(512 - tumor_image.shape[0])//2,'constant',constant_values=0)\n",
        "    \n",
        "    tumor_image = temp_mri_image * tumor_image\n",
        "    tumor_image = tumor_image[tumor_image > 0]\n",
        "    tumor_image[tumor_image == 2] = 0\n",
        "\n",
        "    '''tumor_array =[]\n",
        "    for i in range(0,512):\n",
        "      for j in range(0,512):\n",
        "        if tumor_image[i][j]:\n",
        "          tumor_array.append(mri_image[i][j])\n",
        "\n",
        "    tumor_image = np.array(tumor_array, dtype=np.float)'''\n",
        "\n",
        "    tumor_quartiles = np.percentile(tumor_image, [25, 50, 75,80,85,90,95,96,97,98,99])\n",
        "\n",
        "    label=np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "    imageio.imwrite(dir+\"images/\"+str(label)+\"/\"+file_name.split(\".\")[0]+'.jpg', np.array(f['cjdata']['image'],dtype=np.int16))\n",
        "\n",
        "    return np.array(f['cjdata']['PID'],dtype=np.int)[0][0] \\\n",
        "            ,mri_image.min() \\\n",
        "            ,mri_image.max() \\\n",
        "            ,mri_quartiles[0] \\\n",
        "            ,mri_quartiles[1] \\\n",
        "            ,mri_quartiles[2] \\\n",
        "            ,mri_quartiles[3] \\\n",
        "            ,mri_quartiles[4] \\\n",
        "            ,mri_quartiles[5] \\\n",
        "            ,mri_quartiles[6] \\\n",
        "            ,mri_quartiles[7] \\\n",
        "            ,mri_quartiles[8] \\\n",
        "            ,mri_quartiles[9] \\\n",
        "            ,mri_quartiles[10] \\\n",
        "            ,tumor_image.min() \\\n",
        "            ,tumor_image.max() \\\n",
        "            ,tumor_quartiles[0] \\\n",
        "            ,tumor_quartiles[1] \\\n",
        "            ,tumor_quartiles[2] \\\n",
        "            ,tumor_quartiles[3] \\\n",
        "            ,tumor_quartiles[4] \\\n",
        "            ,tumor_quartiles[5] \\\n",
        "            ,tumor_quartiles[6] \\\n",
        "            ,tumor_quartiles[7] \\\n",
        "            ,tumor_quartiles[8] \\\n",
        "            ,tumor_quartiles[9] \\\n",
        "            ,tumor_quartiles[10] \\\n",
        "            ,tumor_image.shape \\\n",
        "            ,file_name\\\n",
        "            ,np.array(f['cjdata']['label'], dtype=np.int)[0][0] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h9X8MvPNy50",
        "colab_type": "text"
      },
      "source": [
        "def loadDf(dir=\"training_data/\"):\n",
        "  patients_details = []\n",
        "  '''for root, dirs, files in os.walk(\"/content/drive/My Drive/1512427/\", topdown = False):\n",
        "    for f in files:\n",
        "      if \".zip\" in f:\n",
        "          file = zipfile.ZipFile(root+f, \"r\")\n",
        "          for name in file.namelist():\n",
        "            file.extract(name,\".\")\n",
        "            patients_details.append(return_imageInfo_from_mat_file(name))\n",
        "          #break\n",
        "      #break  '''   \n",
        "  \n",
        "  for f in getFileNames(dir):\n",
        "    if \".mat\" in f:\n",
        "      patients_details.append(return_imageInfo_from_mat_file(dir,f))\n",
        "  mri_col_names = [\"mri_min\",\"mri_max\",\"mri_1q\",\"mri_median\", \"mri_3q\",\"mri_80\",\"mri_85\",\"mri_90\",\"mri_95\",\"mri_96\",\"mri_97\",\"mri_98\",\"mri_99\"]\n",
        "  tumor_col_names = [\"t_min\",\"t_max\",\"t_1q\",\"t_median\",\"t_3q\",\"t_80\",\"t_85\",\"t_90\",\"t_95\",\"t_96\",\"t_97\",\"t_98\",\"t_99\",\"tumor_size\"]\n",
        "  col_names = [\"pid\"] + mri_col_names + tumor_col_names+ [\"file_name\",\"label\"]\n",
        "  return pd.DataFrame(patients_details,columns=col_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooal44AcufXG",
        "colab_type": "text"
      },
      "source": [
        "### Load Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCSo34a4Vlta",
        "colab_type": "text"
      },
      "source": [
        "df = loadDf()\n",
        "df[\"square_shape\"] = df.tumor_size.apply(lambda x: np.sqrt(x[0]))\n",
        "df.sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-G7l5uvulcY",
        "colab_type": "text"
      },
      "source": [
        "### Load Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upUrUpLMunt1",
        "colab_type": "text"
      },
      "source": [
        "df_test = loadDf(\"testing_data/\")\n",
        "df_test[\"square_shape\"] = df_test.tumor_size.apply(lambda x: np.sqrt(x[0]))\n",
        "df_test.sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCGj95Q6u1eC",
        "colab_type": "text"
      },
      "source": [
        "### Test the loaded data (both Training and Testing Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBtoXDYUu7Zc",
        "colab_type": "text"
      },
      "source": [
        "def displayNpImages(dir=\"training_data/\"):\n",
        "  for i in range(1,4):\n",
        "    print(\"2 samples for \",tumor_names[i])\n",
        "    for j in random.choices( os.listdir(dir+str(i)),k=2):\n",
        "      plt.imshow(plt.imread(dir+str(i)+\"/\"+str(j)))\n",
        "      plt.show()\n",
        "\n",
        "#displayNpImages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udE3comcx3Oe",
        "colab_type": "text"
      },
      "source": [
        "#### Visual Testing Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHWSr83x8mO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displayNpImages(\"testing_data/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJc8C31m9Efe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displayNpImages(\"validation_data/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeU2lq6Lzb_-",
        "colab_type": "text"
      },
      "source": [
        "def analyzeZipDir(df,start,end):\n",
        "  df[\"file_num\"] = df.file_name.apply(lambda x: x.split(\".\")[0])\n",
        "  df[\"file_num\"] = df.file_num.astype(np.int)\n",
        "  \n",
        "  return df[df.file_num.isin(list(range(start,end)))]\n",
        "analyzeZipDir(df.copy(),1,766).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UnHytYu1RhE",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df.copy(),1533,2298).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0By0gpPh1W8R",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df_test.copy(),2299,3064).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHedgA0R2cy9",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df.copy(),767,1532).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-XfW7wbi2n",
        "colab_type": "text"
      },
      "source": [
        "### Create Test directory for validation through generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpZ1DnxSFdz",
        "colab_type": "text"
      },
      "source": [
        "!rm -rf \"test\"\n",
        "!mkdir \"test\"\n",
        "!mkdir \"test/1\"\n",
        "!mkdir \"test/2\"\n",
        "!mkdir \"test/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPpIuurTUKt",
        "colab_type": "text"
      },
      "source": [
        "!ls -l /content/data/2/3046.jpg\n",
        "!ls -l /content/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhVfTlhdXtG-",
        "colab_type": "text"
      },
      "source": [
        "!ls -l /content/data/2/2404.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GLeYdm-cWAU",
        "colab_type": "text"
      },
      "source": [
        "### Move the test set to test directory from data directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMyBzfxxP07c",
        "colab_type": "text"
      },
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/data\", topdown = False):\n",
        "  \n",
        "    \n",
        "    if len(files) > 0:\n",
        "      print(root, dirs, files)\n",
        "\n",
        "      #indices = np.random.randint(0,len(files),size=round(len(files)*.2))\n",
        "      rand_files = random.choices(files,k=round(len(files)*.2))\n",
        "      \n",
        "      for f in rand_files:\n",
        "        #print(f)\n",
        "        try:\n",
        "          shutil.move(root+\"/\"+f, \"/content/test/\"+root.split(\"/\")[-1]+\"/\"+f)\n",
        "        except :\n",
        "          print(\"Ignoring : \",f)\n",
        "\n",
        "#list(os.walk(\"/content/data\")) /content/test /content/training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-eDxHYafKUK",
        "colab_type": "text"
      },
      "source": [
        "### ImageGenerators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pyYghx5gdCe",
        "colab_type": "text"
      },
      "source": [
        "### Load tf.data\n",
        "* inspired from: https://stackoverflow.com/questions/48309631/tensorflow-tf-data-dataset-reading-large-hdf5-files\n",
        "* extract all files\n",
        "* save file names in a tensor\n",
        "* write a generator function to read a file and return numpy mri image array and its label\n",
        "* using tf.dataset.interleave function, read the file concurrently\n",
        "* apply batch and shuffle\n",
        "* feed it to model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77irdWdhtaxN",
        "colab_type": "text"
      },
      "source": [
        "##### Download the MRI image zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgd4_g2h7rYa",
        "colab_type": "text"
      },
      "source": [
        "def mygenerator(file_name, dir=\"training_data/\"):\n",
        "  #print(file_name)\n",
        "  f = h5py.File(dir+file_name,'r')\n",
        "  #print(f['cjdata']['image'].dtype)\n",
        "  mri_image = np.array(f['cjdata']['image'],dtype=np.int16)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      #print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return (mri_image, np.array(f['cjdata']['label'], dtype=np.int)[0][0])\n",
        "\n",
        "\n",
        "df_temp = pd.DataFrame([mygenerator(f) for f in getFileNames()],columns=[\"image\",\"label\"])\n",
        "\n",
        "'''ds = tf.data.Dataset.from_tensor_slices([  tf.data.Dataset.from_generator(\n",
        "        mygenerator(filename), \n",
        "        (tf.uint8,tf.int8), \n",
        "        (tf.TensorShape([]), tf.TensorShape([None]))) for filename in ds])'''\n",
        "df_temp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4-RohAP6hK",
        "colab_type": "text"
      },
      "source": [
        "imageio.imwrite(\"test.jpg\",df_temp.iloc[0][0])\n",
        "plt.imshow(imageio.imread(\"test.jpg\"),cmap='bone')\n",
        "plt.show()\n",
        "plt.imshow(plt.imread(\"test.jpg\"))\n",
        "plt.show()\n",
        "plt.imshow(df_temp.iloc[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgjK1-K4N8Ue",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(np.array(df_temp.iloc[0][0],dtype=np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLreCMdqiiAq",
        "colab_type": "text"
      },
      "source": [
        "class ToTensor:\n",
        "  def __init__(self):\n",
        "    print(\"Obj Created\")\n",
        "  \n",
        "  def unzipData(self,source_dirname, dest_dir=\"data/\"):\n",
        "    try:\n",
        "      shutil.rmtree(dest_dir)\n",
        "    except:\n",
        "      pass\n",
        "    os.mkdir(dest_dir)\n",
        "    print(\"Exploring \",source_dirname, \"directory\")\n",
        "    for root, dirs, files in os.walk(source_dirname, topdown = False):\n",
        "      for f in files:\n",
        "        print(\"Found file \",f)\n",
        "        if \".zip\" in f:\n",
        "          print(\"Unzipping \", f)\n",
        "          with zipfile.ZipFile(f) as zf:\n",
        "            zf.extractall(dest_dir)\n",
        "    print(\"Files Loaded !!!\")\n",
        "    return self\n",
        "\n",
        "  def getFileName(self,dir_name=\"data/\"):\n",
        "    return os.listdir(dir_name)\n",
        "\n",
        "toTensor = ToTensor()\n",
        "toTensor.unzipData(\"/content/drive/My Drive/1512427/\").getFileName()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0KYEepMovPx",
        "colab_type": "text"
      },
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tqRGoUK5APW",
        "colab_type": "text"
      },
      "source": [
        "## Statistical Analysis\n",
        "* Number of patients in the dataset\n",
        "* Patient wise distribution of tumor classes\n",
        "* Comparison of below attributes for the 3 tumor classes\n",
        "  * 1st quantile of MRI image\n",
        "  * Median of the MRI image\n",
        "  * 3rd quantile of the MRI image\n",
        "  * min value distribution of the Tumor\n",
        "  * max value distribution of the Tumor\n",
        "  * 1st quantile of the Tumor\n",
        "  * median of the Tumor\n",
        "  * 3rd quantile of the Tumor\n",
        "    * Analysis: \n",
        "      * All tumors have darkest area which may indicate the tumor itself\n",
        "      * All tumors have uniform distribution of brightness (apart from the dark area)\n",
        "      * MRI images have darker area outside the skull (non scan area)\n",
        "          * will this influence the model ?\n",
        "          * should the color of the tumor and the non important area of the MRI scan be different ?\n",
        "          \n",
        "* 256x256 image size distribution (any bias in there ?)\n",
        "\n",
        "##### Issues Faced:\n",
        "* Bokeh plots are interactive but they consume a lot of space(>100mb) in the notebook \n",
        "  * markdown for now, when interested can be seen by enabling it as code cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aumAqfm3VymM",
        "colab_type": "text"
      },
      "source": [
        "df.pid.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXwJqH_jnfor",
        "colab_type": "text"
      },
      "source": [
        "There are only 5 patients info present !!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvBti1CInkCG",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"pid\").agg(\"count\").reset_index()[['pid','mri_min']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-95548oIsq",
        "colab_type": "text"
      },
      "source": [
        "df.groupby([\"pid\",\"label\"]).agg(\"count\").reset_index()[['pid','label','mri_min']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z0gzrC-qPjs",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"label\").agg(\"count\").reset_index()[['label','pid']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCP1RnBrn2n",
        "colab_type": "text"
      },
      "source": [
        "def plotStatistics(df, tumor_name):\n",
        "  df = df[[\"mri_1q\",\"mri_median\",\"mri_3q\",\"t_min\",\"t_1q\",\"t_median\",\"t_3q\",\"t_max\"]]\n",
        "  df=(df-df.min())/(df.max()-df.min())\n",
        "  fig, ax = plt.subplots(1, 8,sharex=True,sharey=True,tight_layout=True)\n",
        "  fig.set_figheight(4)\n",
        "  fig.set_figwidth(13)\n",
        "  \n",
        "  fig.suptitle(tumor_name+\" Tumor\")\n",
        "  #plt.subplot(1,8,1)\n",
        "  ax[0].hist(df.mri_1q.tolist())\n",
        "  ax[0].set_title(\"mri_1q\")\n",
        "  #plt.subplot(1,8,2)\n",
        "  ax[1].hist(df.mri_median.tolist())\n",
        "  ax[1].set_title(\"mri_median\")\n",
        "  #plt.subplot(1,8,3)\n",
        "  ax[2].hist(df.mri_3q.tolist())\n",
        "  ax[2].set_title(\"mri_3q\")\n",
        "  #plt.subplot(1,8,4)\n",
        "  ax[3].hist(df.t_min.tolist())\n",
        "  ax[3].set_title(\"t_min\")\n",
        "  #plt.subplot(1,8,5)\n",
        "  ax[4].hist(df.t_1q.tolist())\n",
        "  ax[4].set_title(\"t_1q\")\n",
        "  #plt.subplot(1,8,6)\n",
        "  ax[5].hist(df.t_median.tolist())\n",
        "  ax[5].set_title(\"t_median\")\n",
        "  #plt.subplot(1,8,7)\n",
        "  ax[6].hist(df.t_3q.tolist())\n",
        "  ax[6].set_title(\"t_3q\")\n",
        "  #plt.subplot(1,8,8)\n",
        "  ax[7].hist(df.t_max.tolist())\n",
        "  ax[7].set_title(\"t_max\")\n",
        "  plt.show()\n",
        "\n",
        "plotStatistics(df[df.label ==1], tumor_names[1])\n",
        "plotStatistics(df[df.label ==2], tumor_names[2])\n",
        "plotStatistics(df[df.label ==3], tumor_names[3])\n",
        "plotStatistics(df, \"All\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW6obIAoho0m",
        "colab_type": "text"
      },
      "source": [
        "## Can we reduce the image size ?\n",
        "* Why?\n",
        "  * faster model building\n",
        "  * lower convolution experiment iterations\n",
        "  * lower ram usage and hence higher batch size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQYMkEwLlfPv",
        "colab_type": "text"
      },
      "source": [
        "### Approach 1 : Can we segregate the skull ?\n",
        "  * removing the unwanted area \n",
        "    * percentile approach: identify the percentile and see if any of the tumor percentile is always less that the MRI percentile. \n",
        "        i.e. to prove mri_99 > t_99. This failed as indicated below\n",
        "    * brightness based skull identification:\n",
        "      * nearest neighbor ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H8qcdkeIFyo",
        "colab_type": "text"
      },
      "source": [
        "df[df.mri_99 < df.t_99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAI0eeqfjUZi",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(\"120.mat\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCNwwdLSoLjt",
        "colab_type": "text"
      },
      "source": [
        "### Approach 2: PCA\n",
        "* Note that if we do the PCA transformation, we 2D image will be reduced to 1D. Therefore, we can not use it for the Convolution approach.\n",
        "  * we can note here that just by 2 features (components) and the trained PCA model, we are able to recreate the image with not much difference. [See the last 3 images] This showcases the PCA strength.\n",
        "\n",
        "* Withhelding the PCA approach as we are going to pursuit the Convolution \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOkU29MBoSzg",
        "colab_type": "text"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=5,whiten=True)\n",
        "image=[]\n",
        "image.append(retrieveImage(\"120.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"1.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"2.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"3.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"4.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"5.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"6.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"7.mat\").reshape(-1))\n",
        "\n",
        "#print(image.shape)\n",
        "\n",
        "pca.fit(image)\n",
        "plt.imshow(pca.mean_.reshape((512,512)),\n",
        "           cmap=plt.cm.bone)\n",
        "plt.show()\n",
        "\n",
        "print(pca.noise_variance_)\n",
        "print(image[0].reshape((1,-1)).shape)\n",
        "pca.transform(image[0].reshape((1,-1)))\n",
        "\n",
        "#plt.imshow(pca.transform(image[1].reshape(1,-1)).reshape((512,512)),cmap=plt.cm.bone)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ptnmf6M4Phe",
        "colab_type": "text"
      },
      "source": [
        "components = pca.transform(image[0].reshape(1,-1))\n",
        "projected = pca.inverse_transform(components)\n",
        "plt.imshow(projected.reshape((512,512)))\n",
        "plt.show()\n",
        "plt.imshow(image[0].reshape((512,512)))\n",
        "plt.show()\n",
        "plt.imshow(retrieveImage(\"120.mat\"))\n",
        "plt.show()\n",
        "print(\"doe it match :\", projected.reshape((512,512)) == retrieveImage(\"120.mat\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmKL2wU4eHoA",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(image[0].reshape((256,256)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NZjperm5HpZ",
        "colab_type": "text"
      },
      "source": [
        "## Visual Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYCnbIgH360L",
        "colab_type": "text"
      },
      "source": [
        "### Smallest Tumor Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH9v4orNwTFk",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(list(df[df.tumor_size == df.tumor_size.min()]['file_name'])[0]));\n",
        "plt.imshow(retrieveTumorImage(list(df[df.tumor_size == df.tumor_size.min()]['file_name'])[0]),alpha=0.5);\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj_utuj44fUp",
        "colab_type": "text"
      },
      "source": [
        "### Biggest Tumor in the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0btsuHfkxfty",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]));\n",
        "plt.imshow(retrieveTumorImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]),alpha=0.5);\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Jgv6oj30uI",
        "colab_type": "text"
      },
      "source": [
        "### Numpy Resize failed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aYJNq38zCRL",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(np.resize(retrieveImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]),(256,256)));\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJy0WXR3A_Td",
        "colab_type": "text"
      },
      "source": [
        "### Bokeh Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqrwFc48BCh9",
        "colab_type": "text"
      },
      "source": [
        "def bokehPlot(file_name, tumor_label):\n",
        "  tumor_names = [\"\",\"Meningioma\",\"Glioma\",\"Pituitary\"]\n",
        "  im = retrieveImage(file_name)\n",
        "  s1 = figure(width=512, plot_height=512, title=tumor_names[tumor_label]+\" MRI Image\")\n",
        "  s1.image([im],x=[0],y=[0],dw=[512],dh=[512])\n",
        "\n",
        "  im2 = retrieveTumorImage(file_name)\n",
        "\n",
        "  s2 = figure(width=500, plot_height=500, title=tumor_names[tumor_label]+\" MRI Image with Tumor Highlighted\")\n",
        "  s2.image([im2],x=[0],y=[0],dw=[512],dh=[512])\n",
        "  s2.image([im],x=[0],y=[0],dw=[512],dh=[512],global_alpha=0.5)\n",
        "\n",
        "  show(row(s1,s2))\n",
        "\n",
        "bokehPlot(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0], list(df[df.tumor_size == df.tumor_size.max()]['label'])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ0e1zAWWE4l",
        "colab_type": "text"
      },
      "source": [
        "#### Meningioma Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9dliKZMWINQ",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 1].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pP6euXXWqpU",
        "colab_type": "text"
      },
      "source": [
        "#### Glioma Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22qcn6KiWvMJ",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 2].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_3qNVQqV9A8",
        "colab_type": "text"
      },
      "source": [
        "#### Pituitary Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8P5A-cQO79W",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 3].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdvYCpyo2vC",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6QgmzMKv6Hw",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing ideas:\n",
        "\n",
        "1.  Dataset has tumor region indicator which would allow us to get the average brightness of the area.\n",
        "\n",
        "2. It is said that brightest region is skull and skull is not important for the tumor detection. It is only brain position determines the tumor class. If we remove skull remaining image is brain ?\n",
        "\n",
        "3. if we start with a window of image which would maximize the presence of tumor and expand to include some brain region around the tumor then i guess it is the best data for training(and predicting). Because tumor position in brain is THE factor that decides the tumor class.\n",
        "\n",
        "4. what is the optimum batch size for training?\n",
        "\n",
        "5. what is the overall Image augumented training dataset size ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDE9zmipLimp",
        "colab_type": "text"
      },
      "source": [
        "## Train & Test split\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dA-tbvOLt9b",
        "colab_type": "text"
      },
      "source": [
        "def getSplit(df):\n",
        "  df_test=df.sample(frac=.2)\n",
        "  df = df.drop(df_test.index)\n",
        "  return df, df_test\n",
        "\n",
        "df_orig = df.copy()\n",
        "df,df_test = getSplit(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGWNu-Zpk04v",
        "colab_type": "text"
      },
      "source": [
        "## Batch Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvedxzrclan5",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"label\").agg(\"count\").reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81fcRggmMy5",
        "colab_type": "text"
      },
      "source": [
        "def returnBatchIndices(df,batch_size):\n",
        "  label_1 = df[df.label == 1].index.tolist()\n",
        "  label_2 = df[df.label == 2].index.tolist()\n",
        "  label_3 = df[df.label == 3].index.tolist()\n",
        "\n",
        "  label_list = []\n",
        "  #print(len(label_1), len(label_2),len(label_3),list(range(0,max(len(label_1),len(label_2),len(label_3)),batch_size)))\n",
        "  for i in range(0,max(len(label_1),len(label_2),len(label_3)),batch_size):\n",
        "    label_list.append(label_1[i:i+batch_size] + label_2[i:i+batch_size] + label_3[i:i+batch_size])\n",
        "  return label_list\n",
        "\n",
        "#yieldbatch(df,5)\n",
        "for batch in returnBatchIndices(df,5):\n",
        "  print(batch)\n",
        "  break\n",
        "\n",
        "print(\"Total Number of Batches: \", len(returnBatchIndices(df,5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk60VHloMRo3",
        "colab_type": "text"
      },
      "source": [
        "### For Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexzUUlVk3pi",
        "colab_type": "text"
      },
      "source": [
        "def returnABatch(df,batch_size):\n",
        "  #returns a balanced label mri images\n",
        "  index_list = returnBatchIndices(df,batch_size)\n",
        "  #print(\"index list\",len(list(index_list)))\n",
        "  df2 = pd.get_dummies(df['label'], prefix = 'label')\n",
        "  df = pd.concat([df,df2],axis=1)\n",
        "  for j in index_list:\n",
        "    batch_images=[]\n",
        "    batch_labels=[]\n",
        "    #print(\"j\",j)\n",
        "    for i in j:\n",
        "      #print(\"i\",i)\n",
        "      label_list=[]\n",
        "      image = retrieveImage(list(df[df.index == i]['file_name'])[0])\n",
        "      transformed_image = image.reshape((512,512,1))\n",
        "      batch_images.append(transformed_image)\n",
        "      label_list.append(df[df.index == i]['label_1'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_2'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_3'].tolist()[0])\n",
        "      batch_labels.append(label_list)\n",
        "      #print(\"Batches :\",len(batch_images),len(batch_labels))\n",
        "\n",
        "    #from keras.utils import to_categorical\n",
        "    #batch_labels = to_categorical(batch_labels)\n",
        "    yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "for i in returnABatch(df.reset_index(),2)  :\n",
        "  if i[1].shape[0] < 6:\n",
        "    print(len(i),len(i[1]))\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql7EHi_cMYYP",
        "colab_type": "text"
      },
      "source": [
        "### For Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhi_DvKXMXYN",
        "colab_type": "text"
      },
      "source": [
        "def returnABatch1d(df,batch_size):\n",
        "  #returns a balanced label mri images\n",
        "  index_list = returnBatchIndices(df,batch_size)\n",
        "  #print(\"index list\",len(list(index_list)))\n",
        "  df2 = pd.get_dummies(df['label'], prefix = 'label')\n",
        "  df = pd.concat([df,df2],axis=1)\n",
        "  for j in index_list:\n",
        "    batch_images=[]\n",
        "    batch_labels=[]\n",
        "    #print(\"j\",j)\n",
        "    for i in j:\n",
        "      #print(\"i\",i)\n",
        "      label_list=[]\n",
        "      image = retrieveImage(list(df[df.index == i]['file_name'])[0])\n",
        "      transformed_image = image.reshape(512*512)\n",
        "      batch_images.append(transformed_image)\n",
        "      label_list.append(df[df.index == i]['label_1'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_2'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_3'].tolist()[0])\n",
        "      batch_labels.append(label_list)\n",
        "      #print(\"Batches :\",len(batch_images),len(batch_labels))\n",
        "\n",
        "    #from keras.utils import to_categorical\n",
        "    #batch_labels = to_categorical(batch_labels)\n",
        "    yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "for i in returnABatch1d(df.reset_index(),2)  :\n",
        "  if i[1].shape[0] < 6:\n",
        "    print(len(i),len(i[1]))\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFyXNbgckbCt",
        "colab_type": "text"
      },
      "source": [
        "#Model Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAFFed2pEX8W",
        "colab_type": "text"
      },
      "source": [
        "## CNN Approach using Tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcguDqrmyuR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "a059b1f4-8d34-4f87-e0a8-38a4a74991cf"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(512,512,3))\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            #,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            #,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Flatten()\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(512, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Precision\",\"Recall\"])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 510, 510, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 255, 255, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 253, 253, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 124, 124, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 60, 60, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               6423040   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 6,574,083\n",
            "Trainable params: 6,574,083\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZHApFUtxgH",
        "colab_type": "text"
      },
      "source": [
        "img = plt.imread(\"/content/data/1/1.jpg\")\n",
        "plt.imshow(img,cmap=\"bone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AGrENCXpN1",
        "colab_type": "text"
      },
      "source": [
        "target = df_temp.pop('label')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df_temp.values, target.values))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecS_esHM67zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "8f5010d1-b4ba-4a4e-b53c-e546ef3e6369"
      },
      "source": [
        "history=model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=8\n",
        "                  )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8f3ce8e22d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history=model.fit_generator(train_generator\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0;31m#,steps_per_epoch=286\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBueyG7UzkVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('./final_model.h5', include_optimizer=True)\n",
        "from tensorflow.keras.models import load_model\n",
        "model2 = load_model('./final_model.h5')\n",
        "history2=model2.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=8\n",
        "                  )\n",
        "\n",
        "result2 = model2.evaluate(test_generator)\n",
        "print(result2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ex2GT1x-6SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uljl3NAf8Jos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit_generator(validate_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=test_generator\n",
        "                  ,workers=8\n",
        "                  )\n",
        "result = model.evaluate(test_generator)\n",
        "print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJhyb__OadUy",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4srxIYwTagJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preTrain():\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "      https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "      -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "  local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "  pre_trained_model = InceptionV3(input_shape = (512,512,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('mixed7')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  #x = layers.Flatten()(last_output)\n",
        "  x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='sigmoid')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "  return model\n",
        "imageNet_model = preTrain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlUhmzy2380C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageNet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxHeuBcA4TVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageNet_model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUHrCKk4ZMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator, validate_generator, test_generator = getGenerators(32,(512,512))\n",
        "history=imageNet_model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=3\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  #,workers=2\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cIJzWiY4dkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = imageNet_model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusltMTwF9NZ",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression using Tensorflow Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb8bFUocuERf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "             tf.keras.layers.Flatten(input_shape=(512, 512,3))\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\")            \n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTbDTPvPNtRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=5\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=2\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDTHRX8Q7_2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V_RPsdvE4dP",
        "colab_type": "text"
      },
      "source": [
        "batch_size = 64\n",
        "steps_per_epoch = round(df.groupby(\"label\").agg(\"count\").reset_index()['pid'].max()/batch_size)\n",
        "\n",
        "print(\"Total Training Dataset : \", df.shape[0])\n",
        "print(\"Batch Size : \", batch_size)\n",
        "print(\"Steps per epoch : \", steps_per_epoch)\n",
        "print(\"Test Datasize shape : \", df_test.shape[0])\n",
        "\n",
        "history=model.fit_generator(returnABatch1d(df,batch_size)\n",
        "                  ,steps_per_epoch=steps_per_epoch\n",
        "                  , epochs=5\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_FIgewtHwKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qfFq4-MXHXs",
        "colab_type": "text"
      },
      "source": [
        "# Model Testing\n",
        "* is the model bias for 256 size images ?\n",
        "* is there any imbalance in 256 size images ?\n",
        "* converting 512x512 to 256x256 size would definitely speed up the process but would it impact the accuracy ?\n",
        "* is the model has better accuracy for any type of tumor class? (as we have imbalanced set ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym6uQGG8gydZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFTO4bLSGRBk",
        "colab_type": "text"
      },
      "source": [
        "# Observations / Lesson Learnt:\n",
        "\n",
        "* Iteration 1:\n",
        "  * CNN of 512x512 took half an hour even on TPU\n",
        "  * more and more convolution layer decreases the neurons required for training( duh!!!) and hence the batch size can be increased.\n",
        "  * testing result was 49%. Not Acceptable.\n",
        "\n",
        "* Iteration 2:\n",
        "  * tried PCA but realized transformation is not suited for convolution\n",
        "  * tried to use imagegenerator by converting the numpy to image (jpg) file. But found that numpy to image was not successfull.\n",
        "\n",
        "* Iteration 3:\n",
        "  * experimented with ImageGenerator for training, validation and testing\n",
        "  * time is still a worry for CNN approach; it is 20 mins\n",
        "  * though it is fast for the logistic regression the epochs are not helping (epoch = 5)\n",
        "\n",
        "\n",
        "* Iteration 4:\n",
        "  * experimented with different CNN layers ( basically added one more layer to make shrink the image size to 6x6 but 14x14 was quicker and gave better result)\n",
        "  * average time per epoch is 20 min and 5 epochs gave 89% test accuracy with CNN approach\n",
        "  * deviation in the epoch accuracy indicates that few more epoch can reach upto 90 - 93% which way ahead of last finding during capstone submission (54%).[validation accuracy has also increased so far and hence predicting the accuracy to go beyond 90%]\n",
        "  * imagenet transfer learing takes 1 hour to complete one epoch and for one epoch it gives accuracy around 45%\n",
        "  * 3 epochs on imagenet with 512x512x3 image with 2 convolution at the end with dense network post that gave 90% training accuracy, 65% validation and testing result. Clearly, there is overfit.\n",
        "  * shrinking the size by 256x256 or 150x150 (as per imagenet assumption on input), drastically reduces the epoch time upto 5 min but the accuracy do not improve even for 5 epochs. Loss decreases but there is no change in the accuracy or accuracy decreases.\n",
        "  * till now, it leaves with no additional lesson learnt apart from few experimental observations. i.e. in the last capstone submission, i had CNN and imagenet experiment. but this time different CNN gave a better result and imagenet is yet to match the outcome of CNN. Only new thing learnt is that simplest conv2d (3x3) with max pooling can give better result.\n",
        "  * as part of new learning , I have :\n",
        "    * tf eager execution, instead of graph based as in last submission\n",
        "    * image generator to simplify code, augment the images and to reshape( not recommended).\n",
        "    * keras way of working with transfer learning\n",
        "    * numpy to image save\n",
        "    * experimenting with workers and parallel processing at \"fit\" and \"fit_generator\".\n",
        "    * couple of experiments were not possible if TPU and colab was not arround.\n",
        "    * bokeh experiments with images.\n",
        "    * automating data load operation through linux commands (wget, unzipping, creation and deletion folders, moving files, listing files)\n",
        "\n",
        "* Iteration 5:\n",
        "  * Saving models:\n",
        "    * I guess below approach is good:\n",
        "      * while(1):\n",
        "          * model.fit()\n",
        "          * model.save('./final_model.h5', include_optimizer=True)\n",
        "\n",
        "* Iteration 6:\n",
        "  * Simple LR has too many loss\n",
        "  * NN also has high loss and hence low accuracy \n",
        "  * NN has limitation number of neurons with which python crashes if the initial neuron shape crosses 8K.\n",
        "\n",
        "\n",
        "* iteration 7:\n",
        "  * PCA + Normalization ( scaling between 0 to 1 ) gave a better result 40 % with PCA features of 1024\n",
        "    * PCA training /fit took half an hour\n",
        "    * Dense layer training took mere 3 min for 100 epochs\n",
        "\n",
        "  * PCA + Normalization ( scaling between 0 to 1 ) gave even better result 45%\n",
        "    * PCA training/fit took 1hr 15 min\n",
        "    * dense layer took around 5-8 min for 100 epochs\n",
        "\n",
        "  * evidentally, it is all about minimizing the loss. More number of features in PCA gives higher accuracy is the clear indication of lossess due to drastic step down from 200k to 1k or 2k neurons. So what are the losses we have in our pipeline?\n",
        "    * images are saved as jpg; at that time we have loss of scaling it to 0-255 UNSIGNED INTEGER.\n",
        "    * later when we do a normalization the loss is compounded.\n",
        "    * there may be loss in scaling and doing pca due to sklearn capability to work on float64 (if such constraints exists)\n",
        "\n",
        "\n",
        "* Iteration 8:\n",
        "  * WTF !!!!, np.int for image array gave LR an accuracy of 45%\n",
        "  * np.float64 for image array gave 72% accuracy\n",
        "  * np.float128 for image array gave 76% for LR classification.\n",
        "  * However, as anticipated Neural Network dint in herit this accuracy change.(45%)\n",
        "      * I though additional layers would fine tune the 1 LR layer but it seems they have introduced loss ( value of 8 for NN against 3 in LR). Where is the loss\n",
        "          * RELU : there may have been loss due to negative activation values. Just like that of PCA can have -ve value an activation value might have been negative and might have been thrown away by RELU.\n",
        "            * one more reason to suspect is that changes in the network apart from additional layer is introduction of activation function relu.\n",
        "\n",
        "        * Sigmoid : tried randomly sigmoid function. Pictorically, I can remember that the graph looks like letter \"S\" (slant), so I know that it handles either side of axes and hence should help if my hypothesis on relu loss.Loss definitely got reduced. Loss value looks better (0.9) when compared to both LR (3)  and relu(8)\n",
        "          * training took more 80 secs, because i vaguley remember that sigmoid has exponential function in its equation\n",
        "          * However, it looks promising, to run more epochs in search of reduced loss and increased accuracy.\n",
        "            * Note that here training accuracy was max at 66% but testing at 42% but it evident that epochs are too low (5 epochs).\n",
        "\n",
        "        * tanh: I know that it does not have exponential functions. Indeed there was reduction in training time (20 secs less per epoch compared to sigmoid : near 60 secs)\n",
        "            * accuracy around 45% again not enough epochs to conclude\n",
        "\n",
        "        * linear: suicidal.(as per my hypothesis) Negative values has to be controlled. Linear does NOT do that. LOSS is huge !!!\n",
        "          * training time : 1 min around, accuracy = 0.3\n",
        "        \n",
        "        * softsign: miniature version of sigmoid. <1 min training time , promissing accuracy ( 60:50).\n",
        "          * unlike expected model did not learn on increased epochs\n",
        "\n",
        "        * selu: Surprisingly failed. \n",
        "          * 61s 1s/step - loss: 11.3066 - accuracy: 0.3044 - Recall: 0.3044 - Precision: 0.3044\n",
        "\n",
        "\n",
        "    * PCA computation mandates that we need to have the entire dataset in memory and hence it is failing in my case.\n",
        "        * float128 for image array consumes most of the RAM leaving no room for PCA or NN\n",
        "\n",
        "Iteration 9: \n",
        "  * Data loading is a simplest task but I have made so many revisions\n",
        "    * with view of minimizing loss, fresh read from the file is consuming 80 seconds per epoch from few seconds!!!\n",
        "  * tensorflow does not support float128, max of float64 is feasible.\n",
        "  * new design is to create batches and save the batches and load it when generator is called.\n",
        "  * random.choices does not work; need to use random.sample instead\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iThtN8-UQc4Y",
        "colab_type": "text"
      },
      "source": [
        "# To Do\n",
        "* Batch normalization for both CNN and LR\n",
        "* dropout between each dense layer\n",
        "* is LR with PCA ( n = say 10K) equal to NN with cone structure ?\n",
        "  * as in, cone structure (decreasing stacked NN layer neurons) reduces the feature vector at each layer so is this a parallel approach to PCA ?\n",
        "\n",
        "* NN Experiments:\n",
        "  * adding addtional CNN layer to make the last dimention as 14x14 or 6x6\n",
        "      * 6x6 took additional time for one epoch and also took addional epochs to reach the accuracy as that of 14x14.\n",
        "      \n",
        "  * PCA and LR:\n",
        "    * 1 million (half of flattened layer) PCA dimention\n",
        "    * 50K PCA dimention\n",
        "    * 10K PCA dimention\n",
        "    * 5k\n",
        "    * 1K\n",
        "    * 512\n",
        "    * 256\n",
        "  * NN layers\n",
        "    * 100K, 50K, 25K, 12K, 6K, 3K, 1K, 512, 256, 128, 64, 32, 16, 8\n",
        "      * batch normalization\n",
        "      * drop outs\n",
        "\n",
        "* Preprocessing:\n",
        "  * skull removal through clustering\n",
        "  * image generator to reduce the size to 256x256\n",
        "\n",
        "\n",
        "* transfer learning\n",
        "* image segmentation\n",
        "  * try with only window of image with tumor for accuracy\n",
        "  * how is image segmentation actually done\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYzFII_AvYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in train_generator:\n",
        "  print(i[1].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZKA21TJB1c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(df_flat.label).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB0oEZYiGzJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imageflatdf():\n",
        "  imageflatlist=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  #for d in os.listdir( \"training_data/\"):\n",
        "  #  for f in os.listdir(\"training_data/\"+d+\"/\"):\n",
        "  for file_name in os.listdir(\"download/mat\"):\n",
        "      #plt.imshow(plt.imread(\"training_data/1/\"+f))\n",
        "      #print(plt.imread(\"training_data/1/\"+f).reshape(1,-1).shape)\n",
        "      with h5py.File(\"download/mat/\"+file_name,'r') as f:\n",
        "        image_array = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "        label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "        image_array = image_array/image_array.max()\n",
        "        if image_array.shape[0] == 512:\n",
        "          imageflatlist.append((list(image_array.reshape(-1)),label_transform[label]))\n",
        "        else:\n",
        "          image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          imageflatlist.append((list(image_array.reshape(-1)),label_transform[label])) \n",
        "      #break\n",
        "  df = pd.DataFrame(imageflatlist,columns=['image_array','label'])\n",
        "  return df\n",
        "\n",
        "df_flat = imageflatdf()\n",
        "df_flat.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukVkA3BZ3N5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(df):\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    return pca\n",
        "\n",
        "pca = test(df_flat.copy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxth9ywUYDXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv50XhoNSkiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.groupby(\"label\").agg(\"count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SXp2d-O0qmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.image_array[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDZHQnZy0Rea",
        "colab_type": "text"
      },
      "source": [
        "#PCA + LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOaYkAXtBY2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(len(os.listdir(\"/content/validation_data/npz/\"))//32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgl8ShdQPwZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9deff74-16d9-44a5-db42-8eb6d828522c"
      },
      "source": [
        "def doLR():\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(3, input_shape=[512*512], activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  batch_size=32\n",
        "  model.fit_generator(myTrainGenerator(batch_size)\n",
        "          ,epochs=100\n",
        "          ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "          ,validation_data=myValidateGenerator(batch_size)\n",
        "          ,validation_steps=len(os.listdir(\"validation_data/npz/\"))\n",
        "          )\n",
        "  \n",
        "  return model.evaluate(myTestGenerator(batch_size))\n",
        "\n",
        "doLR()\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start : 2019-10-12 23:37:20.879716\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 786435    \n",
            "=================================================================\n",
            "Total params: 786,435\n",
            "Trainable params: 786,435\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "68/68 [==============================] - 16s 239ms/step - loss: 4.6158 - accuracy: 0.6681 - Recall: 0.6615 - Precision: 0.6716 - val_loss: 4.0474 - val_accuracy: 0.7429 - val_Recall: 0.7429 - val_Precision: 0.7429\n",
            "Epoch 2/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.5219 - accuracy: 0.6984 - Recall: 0.6984 - Precision: 0.6984 - val_loss: 4.1883 - val_accuracy: 0.7257 - val_Recall: 0.7257 - val_Precision: 0.7257\n",
            "Epoch 3/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.1896 - accuracy: 0.7221 - Recall: 0.7221 - Precision: 0.7221 - val_loss: 5.0530 - val_accuracy: 0.6514 - val_Recall: 0.6514 - val_Precision: 0.6514\n",
            "Epoch 4/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.3033 - accuracy: 0.7193 - Recall: 0.7193 - Precision: 0.7193 - val_loss: 3.8561 - val_accuracy: 0.7429 - val_Recall: 0.7429 - val_Precision: 0.7429\n",
            "Epoch 5/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 4.1497 - accuracy: 0.7319 - Recall: 0.7319 - Precision: 0.7319 - val_loss: 3.8063 - val_accuracy: 0.7543 - val_Recall: 0.7543 - val_Precision: 0.7543\n",
            "Epoch 6/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.1200 - accuracy: 0.7343 - Recall: 0.7343 - Precision: 0.7343 - val_loss: 4.4923 - val_accuracy: 0.7029 - val_Recall: 0.7029 - val_Precision: 0.7029\n",
            "Epoch 7/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.0403 - accuracy: 0.7389 - Recall: 0.7389 - Precision: 0.7389 - val_loss: 4.5154 - val_accuracy: 0.6914 - val_Recall: 0.6914 - val_Precision: 0.6914\n",
            "Epoch 8/100\n",
            "68/68 [==============================] - 15s 222ms/step - loss: 4.0417 - accuracy: 0.7408 - Recall: 0.7408 - Precision: 0.7408 - val_loss: 3.8578 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 9/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8945 - accuracy: 0.7524 - Recall: 0.7524 - Precision: 0.7524 - val_loss: 3.8001 - val_accuracy: 0.7543 - val_Recall: 0.7543 - val_Precision: 0.7543\n",
            "Epoch 10/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.2077 - accuracy: 0.7277 - Recall: 0.7277 - Precision: 0.7277 - val_loss: 3.7233 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 11/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.9590 - accuracy: 0.7455 - Recall: 0.7455 - Precision: 0.7455 - val_loss: 3.8001 - val_accuracy: 0.7543 - val_Recall: 0.7543 - val_Precision: 0.7543\n",
            "Epoch 12/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.9541 - accuracy: 0.7469 - Recall: 0.7469 - Precision: 0.7469 - val_loss: 4.3452 - val_accuracy: 0.7143 - val_Recall: 0.7143 - val_Precision: 0.7143\n",
            "Epoch 13/100\n",
            "68/68 [==============================] - 15s 220ms/step - loss: 3.9159 - accuracy: 0.7455 - Recall: 0.7455 - Precision: 0.7455 - val_loss: 3.7205 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 14/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 4.0563 - accuracy: 0.7389 - Recall: 0.7389 - Precision: 0.7389 - val_loss: 4.7855 - val_accuracy: 0.6857 - val_Recall: 0.6857 - val_Precision: 0.6857\n",
            "Epoch 15/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 4.1384 - accuracy: 0.7310 - Recall: 0.7310 - Precision: 0.7310 - val_loss: 3.9916 - val_accuracy: 0.7257 - val_Recall: 0.7257 - val_Precision: 0.7257\n",
            "Epoch 16/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 4.1810 - accuracy: 0.7319 - Recall: 0.7319 - Precision: 0.7319 - val_loss: 3.7866 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 17/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.8655 - accuracy: 0.7534 - Recall: 0.7534 - Precision: 0.7534 - val_loss: 4.6377 - val_accuracy: 0.6971 - val_Recall: 0.6971 - val_Precision: 0.6971\n",
            "Epoch 18/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 4.0317 - accuracy: 0.7403 - Recall: 0.7403 - Precision: 0.7403 - val_loss: 3.7161 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 19/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8254 - accuracy: 0.7566 - Recall: 0.7566 - Precision: 0.7566 - val_loss: 3.8007 - val_accuracy: 0.7543 - val_Recall: 0.7543 - val_Precision: 0.7543\n",
            "Epoch 20/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8152 - accuracy: 0.7585 - Recall: 0.7585 - Precision: 0.7585 - val_loss: 3.7161 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 21/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.7805 - accuracy: 0.7618 - Recall: 0.7618 - Precision: 0.7618 - val_loss: 3.8453 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 22/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.8164 - accuracy: 0.7585 - Recall: 0.7585 - Precision: 0.7585 - val_loss: 3.6982 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 23/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8157 - accuracy: 0.7585 - Recall: 0.7585 - Precision: 0.7585 - val_loss: 3.7161 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 24/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.9144 - accuracy: 0.7515 - Recall: 0.7515 - Precision: 0.7515 - val_loss: 3.7165 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 25/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.8075 - accuracy: 0.7576 - Recall: 0.7576 - Precision: 0.7576 - val_loss: 3.8255 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 26/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.8481 - accuracy: 0.7552 - Recall: 0.7552 - Precision: 0.7552 - val_loss: 3.7161 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 27/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8218 - accuracy: 0.7576 - Recall: 0.7576 - Precision: 0.7576 - val_loss: 3.6517 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 28/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8077 - accuracy: 0.7604 - Recall: 0.7604 - Precision: 0.7604 - val_loss: 4.0411 - val_accuracy: 0.7257 - val_Recall: 0.7257 - val_Precision: 0.7257\n",
            "Epoch 29/100\n",
            "68/68 [==============================] - 15s 221ms/step - loss: 3.9164 - accuracy: 0.7492 - Recall: 0.7492 - Precision: 0.7492 - val_loss: 3.8852 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 30/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.7870 - accuracy: 0.7599 - Recall: 0.7599 - Precision: 0.7599 - val_loss: 3.8868 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 31/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.8254 - accuracy: 0.7585 - Recall: 0.7585 - Precision: 0.7585 - val_loss: 3.7161 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 32/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8051 - accuracy: 0.7594 - Recall: 0.7594 - Precision: 0.7594 - val_loss: 3.6765 - val_accuracy: 0.7543 - val_Recall: 0.7543 - val_Precision: 0.7543\n",
            "Epoch 33/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.7818 - accuracy: 0.7608 - Recall: 0.7608 - Precision: 0.7608 - val_loss: 3.8799 - val_accuracy: 0.7486 - val_Recall: 0.7486 - val_Precision: 0.7486\n",
            "Epoch 34/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.8183 - accuracy: 0.7566 - Recall: 0.7566 - Precision: 0.7566 - val_loss: 3.7161 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 35/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8012 - accuracy: 0.7590 - Recall: 0.7590 - Precision: 0.7590 - val_loss: 3.9708 - val_accuracy: 0.7429 - val_Recall: 0.7429 - val_Precision: 0.7429\n",
            "Epoch 36/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.8394 - accuracy: 0.7562 - Recall: 0.7562 - Precision: 0.7562 - val_loss: 3.6504 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 37/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6325 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 38/100\n",
            "68/68 [==============================] - 16s 230ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6330 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 39/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6330 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 40/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6331 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 41/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6332 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 42/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6332 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 43/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6333 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 44/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6333 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 45/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6334 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 46/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6335 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 47/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6335 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 48/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6336 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 49/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6336 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 50/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6337 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 51/100\n",
            "68/68 [==============================] - 15s 216ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6338 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 52/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6338 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 53/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6339 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 54/100\n",
            "68/68 [==============================] - 15s 217ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6339 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 55/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6340 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 56/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6341 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 57/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6341 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 58/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6342 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 59/100\n",
            "68/68 [==============================] - 14s 213ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6342 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 60/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6343 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 61/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6344 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 62/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6344 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 63/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6345 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 64/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6345 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 65/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6346 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 66/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6347 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 67/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6347 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 68/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6348 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 69/100\n",
            "68/68 [==============================] - 15s 213ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6349 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 70/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6349 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 71/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6350 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 72/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6351 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 73/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6351 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 74/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6352 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 75/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6353 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 76/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6353 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 77/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6354 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 78/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6355 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 79/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6355 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 80/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6356 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 81/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6357 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 82/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6357 - val_accuracy: 0.7657 - val_Recall: 0.7657 - val_Precision: 0.7657\n",
            "Epoch 83/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6358 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 84/100\n",
            "68/68 [==============================] - 14s 213ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6359 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 85/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6359 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 86/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6360 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 87/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6361 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 88/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6361 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 89/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6362 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 90/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6363 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 91/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6364 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 92/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6364 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 93/100\n",
            "68/68 [==============================] - 15s 218ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6365 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 94/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6366 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 95/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6367 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 96/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6367 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 97/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6368 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 98/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6369 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 99/100\n",
            "68/68 [==============================] - 15s 214ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6370 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "Epoch 100/100\n",
            "68/68 [==============================] - 15s 215ms/step - loss: 3.7637 - accuracy: 0.7632 - Recall: 0.7632 - Precision: 0.7632 - val_loss: 3.6370 - val_accuracy: 0.7600 - val_Recall: 0.7600 - val_Precision: 0.7600\n",
            "   2016/Unknown - 412s 205ms/step - loss: 94.3836 - accuracy: 0.7594 - Recall: 0.7594 - Precision: 0.7594"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-2a759f98ebd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyTestGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdoLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-2a759f98ebd4>\u001b[0m in \u001b[0;36mdoLR\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m           )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyTestGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdoLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m   def predict(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     return self._model_iteration(\n\u001b[1;32m    455\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHxm_SkUzAWv",
        "colab_type": "text"
      },
      "source": [
        "def doLR(df):\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  #df['label'] = pd.Categorical(df['label'])\n",
        "  #df['label'] = df.label.astype(np.int)\n",
        "\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(3, input_shape=[262144], activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label))\n",
        "  \n",
        "  for feat, targ in dataset.take(5):\n",
        "    print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "  train_dataset = dataset.batch(50)\n",
        "  #model.fit(dataset)\n",
        "  model.fit_generator(train_dataset,epochs=100)\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label))\n",
        "  test_dataset = dataset.batch(50)\n",
        "  return model.evaluate(test_dataset)\n",
        "\n",
        "doLR(df_flat.copy())\n",
        "print(\"End :\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "120GB0T8wq4H",
        "colab_type": "text"
      },
      "source": [
        "def tfData(df):\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label)).batch(50)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label)).batch(50)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "train_dataset, test_dataset = tfData(df_flat.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-F-tPj37a4T",
        "colab_type": "text"
      },
      "source": [
        "def myGenerator(df,batch_size):\n",
        "  for i in range(0, df.shape[0],batch_size):\n",
        "    yield df.image_array.values[i:i+batch_size], df.label.values[i:i+batchsize]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAjr-uFWOGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1a2a46d-65ea-40ba-a829-e973c5bc99c0"
      },
      "source": [
        "def doNN():\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  \n",
        "  activation_function = \"softsign\"\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4098, input_shape=[512*512], activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            ,tf.keras.layers.Dense(2048, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            ,tf.keras.layers.Dense(1024, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            ,tf.keras.layers.Dense(512, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            ,tf.keras.layers.Dense(128, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  batch_size=32\n",
        "  model.fit_generator(myTrainGenerator(batch_size)\n",
        "          ,epochs=100\n",
        "          ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "          ,validation_data=myValidateGenerator(batch_size)\n",
        "          ,validation_steps=len(os.listdir(\"validation_data/npz/\"))\n",
        "          )\n",
        "  \n",
        "  return model.evaluate(myTestGenerator(batch_size))\n",
        "\n",
        "doNN()\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start : 2019-10-13 00:10:07.334146\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 4098)              1074270210\n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4098)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2048)              8394752   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 1,085,353,989\n",
            "Trainable params: 1,085,353,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "68/68 [==============================] - 90s 1s/step - loss: 0.9980 - accuracy: 0.5562 - Recall: 0.4452 - Precision: 0.6029 - val_loss: 1.0549 - val_accuracy: 0.5429 - val_Recall: 0.5371 - val_Precision: 0.5663\n",
            "Epoch 2/100\n",
            "68/68 [==============================] - 84s 1s/step - loss: 0.8480 - accuracy: 0.6266 - Recall: 0.5604 - Precision: 0.6608 - val_loss: 0.8994 - val_accuracy: 0.5829 - val_Recall: 0.5371 - val_Precision: 0.5987\n",
            "Epoch 3/100\n",
            "68/68 [==============================] - 85s 1s/step - loss: 0.9069 - accuracy: 0.5828 - Recall: 0.5077 - Precision: 0.6230 - val_loss: 0.9373 - val_accuracy: 0.5029 - val_Recall: 0.4000 - val_Precision: 0.5691\n",
            "Epoch 4/100\n",
            "68/68 [==============================] - 86s 1s/step - loss: 0.9028 - accuracy: 0.5907 - Recall: 0.5096 - Precision: 0.6336 - val_loss: 1.1906 - val_accuracy: 0.4114 - val_Recall: 0.4114 - val_Precision: 0.4260\n",
            "Epoch 5/100\n",
            "68/68 [==============================] - 91s 1s/step - loss: 0.8953 - accuracy: 0.5986 - Recall: 0.5124 - Precision: 0.6431 - val_loss: 1.2396 - val_accuracy: 0.4114 - val_Recall: 0.4114 - val_Precision: 0.4417\n",
            "Epoch 6/100\n",
            "68/68 [==============================] - 85s 1s/step - loss: 0.9018 - accuracy: 0.5869 - Recall: 0.4886 - Precision: 0.6363 - val_loss: 0.9551 - val_accuracy: 0.4914 - val_Recall: 0.3943 - val_Precision: 0.5349\n",
            "Epoch 7/100\n",
            "68/68 [==============================] - 86s 1s/step - loss: 0.9453 - accuracy: 0.5599 - Recall: 0.4615 - Precision: 0.6018 - val_loss: 0.9014 - val_accuracy: 0.5486 - val_Recall: 0.4914 - val_Precision: 0.6187\n",
            "Epoch 8/100\n",
            "68/68 [==============================] - 85s 1s/step - loss: 0.8920 - accuracy: 0.6037 - Recall: 0.5016 - Precision: 0.6303 - val_loss: 0.8861 - val_accuracy: 0.6914 - val_Recall: 0.4743 - val_Precision: 0.7281\n",
            "Epoch 9/100\n",
            "68/68 [==============================] - 82s 1s/step - loss: 0.9673 - accuracy: 0.5524 - Recall: 0.4145 - Precision: 0.5899 - val_loss: 0.9502 - val_accuracy: 0.4971 - val_Recall: 0.3086 - val_Precision: 0.6506\n",
            "Epoch 10/100\n",
            "68/68 [==============================] - 82s 1s/step - loss: 0.9180 - accuracy: 0.5837 - Recall: 0.4774 - Precision: 0.6271 - val_loss: 0.8093 - val_accuracy: 0.6743 - val_Recall: 0.6343 - val_Precision: 0.6894\n",
            "Epoch 11/100\n",
            "68/68 [==============================] - 82s 1s/step - loss: 0.9374 - accuracy: 0.5781 - Recall: 0.4797 - Precision: 0.6263 - val_loss: 1.0264 - val_accuracy: 0.5029 - val_Recall: 0.4286 - val_Precision: 0.5245\n",
            "Epoch 12/100\n",
            "68/68 [==============================] - 84s 1s/step - loss: 0.9146 - accuracy: 0.5967 - Recall: 0.5175 - Precision: 0.6328 - val_loss: 0.9186 - val_accuracy: 0.5943 - val_Recall: 0.3886 - val_Precision: 0.6296\n",
            "Epoch 13/100\n",
            "68/68 [==============================] - 82s 1s/step - loss: 0.9416 - accuracy: 0.5907 - Recall: 0.4774 - Precision: 0.6244 - val_loss: 0.8138 - val_accuracy: 0.6571 - val_Recall: 0.6057 - val_Precision: 0.6928\n",
            "Epoch 14/100\n",
            "68/68 [==============================] - 83s 1s/step - loss: 0.8590 - accuracy: 0.6126 - Recall: 0.5310 - Precision: 0.6450 - val_loss: 0.8053 - val_accuracy: 0.6457 - val_Recall: 0.5200 - val_Precision: 0.7222\n",
            "Epoch 15/100\n",
            "68/68 [==============================] - 84s 1s/step - loss: 0.8787 - accuracy: 0.6154 - Recall: 0.5357 - Precision: 0.6373 - val_loss: 0.7506 - val_accuracy: 0.6857 - val_Recall: 0.6571 - val_Precision: 0.7012\n",
            "Epoch 16/100\n",
            "68/68 [==============================] - 83s 1s/step - loss: 0.8449 - accuracy: 0.6303 - Recall: 0.5692 - Precision: 0.6557 - val_loss: 0.7624 - val_accuracy: 0.6743 - val_Recall: 0.6514 - val_Precision: 0.6909\n",
            "Epoch 17/100\n",
            "68/68 [==============================] - 82s 1s/step - loss: 0.8280 - accuracy: 0.6448 - Recall: 0.5660 - Precision: 0.6681 - val_loss: 1.0109 - val_accuracy: 0.4971 - val_Recall: 0.4914 - val_Precision: 0.5212\n",
            "Epoch 18/100\n",
            "68/68 [==============================] - 80s 1s/step - loss: 0.8785 - accuracy: 0.6117 - Recall: 0.5310 - Precision: 0.6479 - val_loss: 0.7309 - val_accuracy: 0.7029 - val_Recall: 0.6971 - val_Precision: 0.7093\n",
            "Epoch 19/100\n",
            "68/68 [==============================] - 72s 1s/step - loss: 0.9334 - accuracy: 0.5800 - Recall: 0.4867 - Precision: 0.6167 - val_loss: 0.8084 - val_accuracy: 0.6857 - val_Recall: 0.6171 - val_Precision: 0.7200\n",
            "Epoch 20/100\n",
            "68/68 [==============================] - 71s 1s/step - loss: 0.8360 - accuracy: 0.6373 - Recall: 0.5497 - Precision: 0.6684 - val_loss: 0.7332 - val_accuracy: 0.7371 - val_Recall: 0.7086 - val_Precision: 0.7425\n",
            "Epoch 21/100\n",
            "68/68 [==============================] - 71s 1s/step - loss: 0.8524 - accuracy: 0.6340 - Recall: 0.5660 - Precision: 0.6605 - val_loss: 0.7619 - val_accuracy: 0.6857 - val_Recall: 0.6514 - val_Precision: 0.6909\n",
            "Epoch 22/100\n",
            "68/68 [==============================] - 71s 1s/step - loss: 0.9352 - accuracy: 0.5758 - Recall: 0.4690 - Precision: 0.6060 - val_loss: 0.8964 - val_accuracy: 0.5943 - val_Recall: 0.5314 - val_Precision: 0.6414\n",
            "Epoch 23/100\n",
            "68/68 [==============================] - 71s 1s/step - loss: 0.8961 - accuracy: 0.6089 - Recall: 0.5142 - Precision: 0.6383 - val_loss: 0.8577 - val_accuracy: 0.6057 - val_Recall: 0.5886 - val_Precision: 0.6688\n",
            "Epoch 24/100\n",
            "68/68 [==============================] - 72s 1s/step - loss: 0.8242 - accuracy: 0.6485 - Recall: 0.5907 - Precision: 0.6704 - val_loss: 0.8388 - val_accuracy: 0.6000 - val_Recall: 0.5829 - val_Precision: 0.6375\n",
            "Epoch 25/100\n",
            "68/68 [==============================] - 72s 1s/step - loss: 0.7895 - accuracy: 0.6639 - Recall: 0.6163 - Precision: 0.6769 - val_loss: 0.7576 - val_accuracy: 0.6629 - val_Recall: 0.6514 - val_Precision: 0.6746\n",
            "Epoch 26/100\n",
            "68/68 [==============================] - 71s 1s/step - loss: 0.8428 - accuracy: 0.6480 - Recall: 0.5851 - Precision: 0.6758 - val_loss: 0.9028 - val_accuracy: 0.6171 - val_Recall: 0.5600 - val_Precision: 0.6049\n",
            "Epoch 27/100\n",
            "68/68 [==============================] - 71s 1s/step - loss: 0.8187 - accuracy: 0.6527 - Recall: 0.6089 - Precision: 0.6725 - val_loss: 0.8508 - val_accuracy: 0.6114 - val_Recall: 0.5714 - val_Precision: 0.6623\n",
            "Epoch 28/100\n",
            "68/68 [==============================] - 72s 1s/step - loss: 0.9112 - accuracy: 0.5977 - Recall: 0.5119 - Precision: 0.6158 - val_loss: 1.0292 - val_accuracy: 0.5371 - val_Recall: 0.5143 - val_Precision: 0.5325\n",
            "Epoch 29/100\n",
            "63/68 [==========================>...] - ETA: 5s - loss: 0.8219 - accuracy: 0.6484 - Recall: 0.5970 - Precision: 0.6628"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d23dbd51d751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyTestGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdoNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-d23dbd51d751>\u001b[0m in \u001b[0;36mdoNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_data/npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyValidateGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_data/npz/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m           )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6111\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6112\u001b[0;31m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[1;32m   6113\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6114\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJw1KmYsH_Pj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "def doNN(df):\n",
        "  #tf.reset_default_graph()\n",
        "  #df['label'] = pd.Categorical(df['label'])\n",
        "  #df['label'] = df.label.astype(np.int)\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  \n",
        "  activation_function = \"softsign\"\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4098, input_shape=[262144], activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(2048, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(1024, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(512, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(128, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "\n",
        "  #dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label))\n",
        "  \n",
        "  #for feat, targ in train_dataset.take(5):\n",
        "  #  print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "  #train_dataset = dataset.batch(50)\n",
        "  #model.fit(dataset)\n",
        "  model.fit_generator(train_dataset, epochs=5)\n",
        "  \n",
        "  #dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label))\n",
        "  #test_dataset = dataset.batch(50)\n",
        "  return model.evaluate(test_dataset)\n",
        "\n",
        "doNN(df_flat.copy())\n",
        "print(\"End :\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmFSRH0IVPc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myPca:\n",
        "  def doPCANN(df):\n",
        "    print(\"Start :\", datetime.datetime.now())\n",
        "    #df['label'] = pd.Categorical(df['label'])\n",
        "    #df['label'] = df.label.astype(np.int)\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.Session().reset()\n",
        "\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"image_array_pca\"] = df.image_array.apply(lambda x: scaler.fit_transform(pca.transform(x.reshape(1, -1))[0].reshape(1, -1))[0])\n",
        "    print(\"Post PCA :\", datetime.datetime.now())\n",
        "    df_test = df.sample(frac=0.2)\n",
        "    df_train = df.drop(df_test.index)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "              #tf.keras.layers.Dense(4098, input_shape=[1024], activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(2048, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(1024, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(512, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(128, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              tf.keras.layers.Dense(3, input_shape=[1024], activation=\"softmax\") \n",
        "              ])\n",
        "    model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array_pca, df_train.label))\n",
        "    \n",
        "    for feat, targ in dataset.take(5):\n",
        "      print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "    train_dataset = dataset.batch(50)\n",
        "    #model.fit(dataset)\n",
        "    model.fit_generator(train_dataset,epochs=5)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array_pca, df_test.label))\n",
        "    test_dataset = dataset.batch(50)\n",
        "    return model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Result :\",myPca.doPCANN(df_flat.copy()))\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjzPG58tUsC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doPCA(df):\n",
        "  import numpy as np\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components=2,whiten=True)\n",
        "\n",
        "  print(np.array(df.image_array.to_list()).shape)\n",
        "  pca.fit(df.image_array.to_list())\n",
        "  #pca.transform(image[0].reshape((1,-1)))\n",
        "  #return [pca.transform(image) for image in df.image_array.to_list() if image.shape[0] == 262144]\n",
        "  df['x'] = df.image_array.apply(lambda x: pca.transform(x.reshape(1, -1))[0][0] if x.shape[0]== 262144 else 0)\n",
        "  df['y'] = df.image_array.apply(lambda x: pca.transform(x.reshape(1, -1))[0][1] if x.shape[0]== 262144 else 0)\n",
        "\n",
        "  return df\n",
        "\n",
        "x_y = doPCA(df_flat.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL06EyYxKWab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_y.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qy4t7upMSQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_y['label'] = x_y.label.astype(np.int)\n",
        "x_y.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce37jsZKvMFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotp(x_y):\n",
        "  plt.scatter(x_y['x'],x_y['y'],c=x_y['label'])\n",
        "  plt.show()\n",
        "\n",
        "plotp(x_y[x_y.label == 1])\n",
        "plotp(x_y[x_y.label == 2])\n",
        "plotp(x_y[x_y.label == 3])\n",
        "plotp(x_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1YBCCWqv3l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(plt.imread(\"training_data/1/1.jpg\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk3PzIHrwjJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_image = plt.imread(\"training_data/1/1.jpg\").copy()\n",
        "ma = temp_image.max()*.8\n",
        "print(ma)\n",
        "temp_image[temp_image > ma]= 0\n",
        "plt.imshow(temp_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mytrMAChxR6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doCluster(df):\n",
        "  from sklearn import cluster\n",
        "  k_means = cluster.KMeans(n_clusters=2, n_init=4)\n",
        "  k_means.fit(df.image_array.to_list())\n",
        "\n",
        "  return df\n",
        "\n",
        "tt = doCluster(df_flat.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35CZ3PUb0nGZ",
        "colab_type": "text"
      },
      "source": [
        "##### Incremental PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNa27eyz0pi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def returnImageLabel(file_list):\n",
        "  image_list=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  for file_name in file_list:\n",
        "    \n",
        "    with h5py.File(\"download/mat/\"+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] == 512:\n",
        "            image_list.append((list(image_array.reshape(-1)),label_transform[label]))\n",
        "          else:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "            image_list.append((list(image_array.reshape(-1)),label_transform[label])) \n",
        "  return np.array(image_list)\n",
        "\n",
        "def myGenerator(batch_size):\n",
        "  files = os.listdir(\"download/mat\")\n",
        "  for i in itertools.cycle(range(0,len(files),batch_size)):\n",
        "    yield returnImageLabel(files[i:i+batch_size])\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0q7WmR80yb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myPca:\n",
        "  def doPCANN():\n",
        "    print(\"Start :\", datetime.datetime.now())\n",
        "    #df['label'] = pd.Categorical(df['label'])\n",
        "    #df['label'] = df.label.astype(np.int)\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.Session().reset()\n",
        "\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"image_array_pca\"] = df.image_array.apply(lambda x: scaler.fit_transform(pca.transform(x.reshape(1, -1))[0].reshape(1, -1))[0])\n",
        "    print(\"Post PCA :\", datetime.datetime.now())\n",
        "    df_test = df.sample(frac=0.2)\n",
        "    df_train = df.drop(df_test.index)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "              #tf.keras.layers.Dense(4098, input_shape=[1024], activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(2048, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(1024, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(512, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(128, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              tf.keras.layers.Dense(3, input_shape=[1024], activation=\"softmax\") \n",
        "              ])\n",
        "    model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array_pca, df_train.label))\n",
        "    \n",
        "    for feat, targ in dataset.take(5):\n",
        "      print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "    train_dataset = dataset.batch(50)\n",
        "    #model.fit(dataset)\n",
        "    model.fit_generator(train_dataset,epochs=5)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array_pca, df_test.label))\n",
        "    test_dataset = dataset.batch(50)\n",
        "    return model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Result :\",myPca.doPCANN())\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}