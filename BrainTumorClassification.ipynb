{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BrainTumorClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmarajBhat/Machine-Learning/blob/master/BrainTumorClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTCyCln9DwLS",
        "colab_type": "text"
      },
      "source": [
        "# Detection of 3 Brain Tumors (Meningioma, Glioma and Pituitary) in T1-weighted contrast enhanced images\n",
        "\n",
        "### - Revisitng the Udacity Capstone Project in pursuit of better accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExaNYOAl3nSH",
        "colab_type": "text"
      },
      "source": [
        "# What is the problem statement?\n",
        "  * predict the tumor class given a MRI image\n",
        "  * OR predict the tumor class when both MRI and Tumor region is given !!!\n",
        "      * tumor region is identified and put in input dataset by experts\n",
        "          * can we have Image Segmentation problem ?\n",
        "\n",
        "\n",
        "  * I think this is the order of problem from easy level to difficult level\n",
        "    * Identify the tumor class from raw MRI image (here accuracy may be low)\n",
        "    * Identify the tumor class from raw MRI image with tumor region identified info (here accuracy may be better)\n",
        "    * Auto detect the tumor segment in a MRI image and classify the tumor (ideal application for a radiologist)\n",
        "\n",
        "    Let us try all the 3 !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47aIRBi1lPdJ",
        "colab_type": "text"
      },
      "source": [
        "# Import Packages\n",
        "* read the input MRI images (.mat) files through ***h5py***\n",
        "* **bokeh** plot for the zoomed in analysis of a tumor and neighbors\n",
        "* ***pandas*** for data analysis and preprocessing\n",
        "* ***tensorflow*** for modelling and predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzYp6q2F5Doj",
        "colab_type": "code",
        "outputId": "03b2966d-1521-42a4-c75a-c5d53cb0037c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XamypXiCEdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from bokeh.io import output_notebook, show\n",
        "from bokeh.layouts import row\n",
        "from bokeh.plotting import figure\n",
        "output_notebook()\n",
        "\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import random # for radom selection from a list\n",
        "import datetime\n",
        "import itertools\n",
        "import time\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quwfr-qMPrrs",
        "colab_type": "text"
      },
      "source": [
        "### Most important line in the program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqclQBxuMQ9m",
        "colab_type": "text"
      },
      "source": [
        " print(tf.keras.backend.floatx())\n",
        " tf.keras.backend.set_floatx(\"float64\")\n",
        " print(tf.keras.backend.floatx())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8abLHDC4ut4",
        "colab_type": "code",
        "outputId": "21d31e27-fb6b-4acb-e025-a2cbd887ff85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E7e82UO_cLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a Global variable\n",
        "tumor_names = [\"\",\"Meningioma\",\"Glioma\",\"Pituitary\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68mR7i5xla0f",
        "colab_type": "text"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "* **Iteration 1**:\n",
        "    * Mount Google Drive\n",
        "    * Unzip it in colab disk\n",
        "    * load mat attributes to list of tuples ( with mri and tumor 5 point summary details)\n",
        "    * create a panda dataframe for analysis\n",
        "\n",
        "    ##### Issues Faced:\n",
        "    1. loading to panda with image took half(6GB) of RAM\n",
        "    * loading tumor along with mri image (as in mat file) crashed the colab\n",
        "      * Solution: let us load image but save only 5 point summary for both mri image and tumor\n",
        "\n",
        "    2. How do we scale/normalize the data?\n",
        "      * would tumor region have 0 in it ?\n",
        "        * only way to know is through the value present in the binary indicator == 1\n",
        "            * implementation through 2 for loops takes forever !!!\n",
        "              * need to implement throuhg np.where...a[a == 1] !!!\n",
        "            \n",
        "\n",
        "    3. Some images are less than 512\n",
        "        * pad the difference with 0s.\n",
        "        \n",
        "    4. Should tumor image be scaled between 0 -1? For now, brightness values are relative to that of the whole image to which it belongs to.\n",
        "\n",
        "    5. Epoch run failed due to no data generated by the custom generator.\n",
        "      * Going to try the ImageGenerator from the TF.\n",
        "\n",
        "\n",
        "* **Iteration 2** :\n",
        "  * ImageGenerator worked fine but the np to image conversion had used dtype of np.uint8 to avoid warning during saving to image. However, that lead to corrupt image and hence loss was more and accuracy was less.\n",
        "  * Generator built for the iteration is correct ?\n",
        "      * are *flips* is not damaging data\n",
        "\n",
        "* **Iteration 3** :\n",
        "  * validate the image augmentation in the ImageGenerator\n",
        "  * initial load the data was fine. zip based train and test failed\n",
        "\n",
        "* **Iteration 4**:\n",
        "  * open all mat(hdf5) files and load 5 point summary of mri and tumor to a panda df\n",
        "  * save all the numpy mri image array to training_data directory\n",
        "  * split the df to training : testing = 80 :20\n",
        "  * move the 20% of the testing to testing_data\n",
        "  * out of the 80% training data, move the 10% for the validation during training\n",
        "      * download\n",
        "        * 5\n",
        "        * b*.zip\n",
        "        * *.txt\n",
        "        * mat\n",
        "          * *.mat\n",
        "\n",
        "      * training_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "      * validation_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "      * testing_data\n",
        "        * 1\n",
        "          * *.npy\n",
        "        * 2\n",
        "          *  *.npy\n",
        "        * 3\n",
        "          * *.npy\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6trYXPZPJ54V",
        "colab_type": "text"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWr8OsJ1ar6P",
        "colab_type": "text"
      },
      "source": [
        "##### Google Drive File Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWQcmRvRFD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf training_data validation_data testing_data download"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCtee7XQbSv",
        "colab_type": "code",
        "outputId": "0eaab3b6-81c8-42e9-f230-bac70a92c33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def loadMatFiles(dir=\"training_data/\"):\n",
        "  if not os.path.isfile(\"download/mat/1.mat\") and not os.path.isfile(\"training_data/npz/0.npz\"):\n",
        "    cmds = [\n",
        "            \"rm -rf download data training_data validation_data testing_data\"\n",
        "            ,\"wget https://ndownloader.figshare.com/articles/1512427/versions/5 -P download\"\n",
        "            ,\"unzip download/5 -d download\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_1-766.zip -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_1533-2298.zip -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_767-1532.zip   -d download/mat\"\n",
        "            ,\"unzip -q download/brainTumorDataPublic_2299-3064.zip  -d download/mat\"]\n",
        "\n",
        "    for c in cmds:\n",
        "      os.system(c)\n",
        "    time.sleep(5)\n",
        "  else:\n",
        "    print(\"mat files are loaded into download/mat directory\")\n",
        "\n",
        "loadMatFiles()\n",
        "print(\"Total Data : \", len(os.listdir(\"download/mat\")))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Data :  3064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUa5H7fsQkxk",
        "colab_type": "text"
      },
      "source": [
        "if not os.path.isdir(\"training_data/\"):\n",
        "\n",
        "  if not os.path.isdir(\"download/mat\") or not len(os.listdir(\"download/mat\")) :\n",
        "    loadMatFiles()\n",
        "\n",
        "  os.system(\"mkdir training_data validation_data testing_data\")\n",
        "\n",
        "  files = os.listdir(\"download/mat\").copy()\n",
        "  training_files = random.sample(files, k=round(len(files) *.7))\n",
        "\n",
        "  for file in training_files:\n",
        "    try:\n",
        "      os.rename(\"download/mat/\"+file, \"training_data/\"+file)\n",
        "    except:\n",
        "      print(\"Skipping :\", file)\n",
        "  time.sleep(5)\n",
        "\n",
        "print(\"Training Data:\", len(os.listdir(\"training_data/\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijgDKtrSQsgs",
        "colab_type": "text"
      },
      "source": [
        "files = os.listdir(\"download/mat\").copy()\n",
        "validation_files = random.sample(files, k=round(len(files) *.1))\n",
        "\n",
        "for file in validation_files:\n",
        "  try:\n",
        "    os.rename(\"download/mat/\"+file, \"validation_data/\"+file)\n",
        "  except:\n",
        "    print(\"Skipping :\", file)\n",
        "time.sleep(5)\n",
        "print(\"Validation Data:\", len(os.listdir(\"validation_data/\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_9jSTFhQyeR",
        "colab_type": "text"
      },
      "source": [
        "for file in os.listdir(\"download/mat/\"):\n",
        "      if not (file in os.listdir(\"training_data/\") or file in os.listdir(\"validation_data\")):\n",
        "        try:\n",
        "          os.rename(\"download/mat/\"+file, \"testing_data/\"+file)\n",
        "        except:\n",
        "          print(\"Skipping :\", file)\n",
        "time.sleep(5)\n",
        "print(\"Testing Data:\", len(os.listdir(\"testing_data/\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrXoZw8BxUR9",
        "colab_type": "text"
      },
      "source": [
        "def returnImageLabel(loc, file_list):\n",
        "  image_list=[]\n",
        "  label_list=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "\n",
        "  if not os.path.isdir(loc+\"images/\"):\n",
        "    os.system(\"mkdir \"+loc+\"images/\")\n",
        "    os.system(\"mkdir \"+loc+\"images/1/\")\n",
        "    os.system(\"mkdir \"+loc+\"images/2/\")\n",
        "    os.system(\"mkdir \"+loc+\"images/3/\")\n",
        "  for file_name in file_list:\n",
        "    \n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          \n",
        "          #image_list.append((image_array,  label_transform[label]))\n",
        "          image_list.append(image_array)\n",
        "          label_list.append(label_transform[label])\n",
        "          imageio.imwrite(loc+\"images/\"+str(label)+\"/\"+file_name.split(\".\")[0]+'.png', image_array)\n",
        "  return np.array(image_list), np.array(label_list)\n",
        "\n",
        "def npSaver(loc, batch_size=16):\n",
        "  \n",
        "  files = os.listdir(loc).copy()\n",
        "  os.system(\"mkdir \"+loc+\"npz/\")\n",
        "  for i in range(0,len(files),batch_size):\n",
        "    x,y = returnImageLabel(loc, files[i:i+batch_size])\n",
        "    print(loc+\"npz/\"+str(i)+\".npz\")\n",
        "    np.savez(loc+\"/npz/\"+str(i), x=x, y=y)\n",
        "    #time.sleep(1)\n",
        "    #np.save(loc+str(i),np.array(returnImageLabel(loc, files[i:i+32])))\n",
        "\n",
        "if not os.path.isfile(\"training_data/npz/0.npz\"):\n",
        "  npSaver(\"training_data/\")\n",
        "print(\"Training Batch Files :\", len(os.listdir(\"training_data/npz\")))\n",
        "\n",
        "if not os.path.isfile(\"validation_data/npz/0.npz\"):\n",
        "  npSaver(\"validation_data/\")\n",
        "print(\"Validation Batch Files :\", len(os.listdir(\"validation_data/npz\")))\n",
        "\n",
        "if not os.path.isfile(\"testing_data/npz/0.npz\"):\n",
        "  npSaver(\"testing_data/\")\n",
        "print(\"Testing Batch Files :\", len(os.listdir(\"testing_data/npz\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx62oEM3sneI",
        "colab_type": "text"
      },
      "source": [
        "#### MyGenerators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiGOCo1vjPS",
        "colab_type": "text"
      },
      "source": [
        "def myTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myValidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512*512)), data['y']\n",
        "\n",
        "def myCNNTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512,512,1)), data['y']\n",
        "\n",
        "def myCNNValidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512,512,1)), data['y']\n",
        "\n",
        "def myCNNTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    yield data['x'].reshape((data['x'].shape[0],512,512,1)), data['y']\n",
        "\n",
        "\n",
        "def myCNNtfrTrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    images=[]\n",
        "    for i in range(data['x'].shape[0]):\n",
        "      image = np.stack((data['x'][i],)*3, axis=-1)\n",
        "      images.append(image)\n",
        "    #np.fromiter((np.stack((data['x'][i],np.zeros((512,512)),np.zeros((512,512))), axis=-1) for xi in data['x']), x.dtype)\n",
        "    yield np.array(images), data['y']\n",
        "\n",
        "def myCNNVtfralidateGenerator(batch_size):\n",
        "  files = os.listdir(\"validation_data/npz/\") \n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"validation_data/npz/\"+i)\n",
        "    images=[]\n",
        "    for i in range(data['x'].shape[0]):\n",
        "      #image = np.stack((data['x'][i],np.zeros((512,512)),np.zeros((512,512))), axis=-1)\n",
        "      image = np.stack((data['x'][i],)*3, axis=-1)\n",
        "      images.append(image)\n",
        "\n",
        "    yield np.array(images), data['y']\n",
        "\n",
        "def myCNNtfrTestGenerator(batch_size):\n",
        "  files =  os.listdir(\"testing_data/npz/\")\n",
        "  for i in files:\n",
        "    data = np.load(\"testing_data/npz/\"+i)\n",
        "    images=[]\n",
        "    for i in range(data['x'].shape[0]):\n",
        "      #image = np.stack((data['x'][i],np.zeros((512,512)),np.zeros((512,512))), axis=-1)\n",
        "      image = np.stack((data['x'][i],)*3, axis=-1)\n",
        "      images.append(image)\n",
        "\n",
        "    yield np.array(images), data['y']\n",
        "\n",
        "\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "train_data_gen = image_generator.flow_from_directory(directory=\"training_data/images\",\n",
        "                                                     batch_size=32,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(512, 512),\n",
        "                                                     #classes = list(CLASS_NAMES)\n",
        "                                                     )\n",
        "\n",
        "valid_data_gen = image_generator.flow_from_directory(directory=\"validation_data/images\",\n",
        "                                                     batch_size=32,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(512, 512),\n",
        "                                                     #classes = list(CLASS_NAMES)\n",
        "                                                     )\n",
        "\n",
        "test_data_gen = image_generator.flow_from_directory(directory=\"testing_data/images\",\n",
        "                                                     batch_size=32,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(512, 512),\n",
        "                                                     #classes = list(CLASS_NAMES)\n",
        "                                                     )\n",
        "\n",
        "\n",
        "\n",
        "def myBstrainGenerator(batch_size):\n",
        "  files =os.listdir(\"training_data/npz/\")\n",
        "  for i in itertools.cycle(files):\n",
        "    data = np.load(\"training_data/npz/\"+i)\n",
        "    yield {\"image_array\":data['x'].reshape((data['x'].shape[0],512,512,1))}, data['y']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKq1Mw9UPaG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf training_data/npz validation_data/npz testing_Data/npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yFVzZWatfpR",
        "colab_type": "text"
      },
      "source": [
        "for i in myCNNtfrTestGenerator(16):\n",
        "  print(i[0].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv8YafkX9zoz",
        "colab_type": "text"
      },
      "source": [
        "print(plt.imread(\"training_data/images/1/1.png\").shape, type(plt.imread(\"training_data/images/1/1.png\")),plt.imread(\"training_data/images/1/1.png\").dtype)\n",
        "plt.imshow(plt.imread(\"training_data/images/1/1.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJ7KGrx9ZdE",
        "colab_type": "text"
      },
      "source": [
        "for i in train_data_gen:\n",
        "  print(i[0].shape)\n",
        "  print(i[0][0].max())\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNKyXmtUT5uW",
        "colab_type": "text"
      },
      "source": [
        "# TensorFunctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctgxgPDTaGGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "029dbe03-1665-4523-d7ad-70b46fd41e56"
      },
      "source": [
        "with h5py.File(\"download/mat/1.mat\",'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          #image_array = image_array/image_array.max()\n",
        "          plt.imshow(image_array,cmap=\"bone\")\n",
        "          plt.show()\n",
        "          #print(np.resize(image_array,(128,128)).shape)\n",
        "          #imageio.imwrite(\"aaa.png\",np.resize(image_array,(128,128)))\n",
        "          #print(list(plt.imread(\"aaa.png\")[64]))\n",
        "          #print(list(np.resize(image_array,(128,128))))\n",
        "          imageio.imwrite(\"aaa.png\",np.array(tf.image.resize(np.stack((image_array,)*3,axis=-1),(128,128),method=\"nearest\")))\n",
        "          plt.imshow(plt.imread(\"aaa.png\"))\n",
        "          print(\"max value = \",np.array(tf.image.resize(np.stack((image_array,)*3,axis=-1),(128,128),method=\"nearest\")).max())\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmFnBPztTzdax9E+WAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0.0, 3317.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max value =  3317.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngICAGMKkEBAQEEOYFAICAmL4/wFfwSDO9v8+BQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdJPCepdw1u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir cache cache/training_data/ cache/validation_data/ cache/testing_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOMgYzeAQmbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHAPE = 224\n",
        "batch_size = 64\n",
        "def loadImage(loc, file_name):\n",
        "    label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "    loc = loc.numpy().decode('utf-8')\n",
        "    file_name = file_name.numpy().decode('utf-8')\n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          return tf.image.resize(np.stack((image_array,)*3,axis=-1),(SHAPE, SHAPE),method=\"nearest\"), label_transform[label]\n",
        "\n",
        "dataset= tf.data.Dataset.from_tensor_slices(os.listdir(\"download/mat/\")).\\\n",
        "            map(lambda x: tf.py_function(func=loadImage,\n",
        "      inp=[\"download/mat/\",x], Tout=(tf.float64, tf.int16)))\n",
        "training_data = dataset.shuffle(3000).take(2500).batch(batch_size).prefetch(2)\n",
        "dataset = dataset.skip(2500)\n",
        "validation_data = dataset.take(200).batch(batch_size).prefetch(2)\n",
        "dataset = dataset.skip(200)\n",
        "#testing_data = dataset.take(-1).repeat(1).batch(batch_size).prefetch(2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMPPBNmBlglA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9afdd388-be7d-4132-bb10-8c7cd5bd2043"
      },
      "source": [
        "for i in testing_data.take(1):\n",
        "  print(i)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: id=24514, shape=(16, 224, 224, 3), dtype=float64, numpy=\n",
            "array([[[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.00169722, 0.00169722, 0.00169722],\n",
            "         [0.00373388, 0.00373388, 0.00373388],\n",
            "         [0.00373388, 0.00373388, 0.00373388],\n",
            "         ...,\n",
            "         [0.00373388, 0.00373388, 0.00373388],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.00135777, 0.00135777, 0.00135777],\n",
            "         [0.00678887, 0.00678887, 0.00678887],\n",
            "         [0.00577054, 0.00577054, 0.00577054],\n",
            "         ...,\n",
            "         [0.00509165, 0.00509165, 0.00509165],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.00203666, 0.00203666, 0.00203666],\n",
            "         [0.00577054, 0.00577054, 0.00577054],\n",
            "         [0.00746775, 0.00746775, 0.00746775],\n",
            "         ...,\n",
            "         [0.00610998, 0.00610998, 0.00610998],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00441276, 0.00441276, 0.00441276],\n",
            "         [0.00543109, 0.00543109, 0.00543109],\n",
            "         ...,\n",
            "         [0.00848608, 0.00848608, 0.00848608],\n",
            "         [0.00475221, 0.00475221, 0.00475221],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00441276, 0.00441276, 0.00441276],\n",
            "         [0.00678887, 0.00678887, 0.00678887],\n",
            "         ...,\n",
            "         [0.01086219, 0.01086219, 0.01086219],\n",
            "         [0.00339443, 0.00339443, 0.00339443],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00271555, 0.00271555, 0.00271555],\n",
            "         [0.00305499, 0.00305499, 0.00305499],\n",
            "         ...,\n",
            "         [0.00339443, 0.00339443, 0.00339443],\n",
            "         [0.00169722, 0.00169722, 0.00169722],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.00041841, 0.00041841, 0.00041841],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.00041841, 0.00041841, 0.00041841],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0.        , 0.        , 0.        ],\n",
            "         [0.00929152, 0.00929152, 0.00929152],\n",
            "         [0.01045296, 0.01045296, 0.01045296],\n",
            "         ...,\n",
            "         [0.01006581, 0.01006581, 0.01006581],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00658149, 0.00658149, 0.00658149],\n",
            "         [0.01664731, 0.01664731, 0.01664731],\n",
            "         ...,\n",
            "         [0.01316299, 0.01316299, 0.01316299],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.01122726, 0.01122726, 0.01122726],\n",
            "         [0.01277584, 0.01277584, 0.01277584],\n",
            "         ...,\n",
            "         [0.01355014, 0.01355014, 0.01355014],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.0116144 , 0.0116144 , 0.0116144 ],\n",
            "         [0.01509872, 0.01509872, 0.01509872],\n",
            "         ...,\n",
            "         [0.01316299, 0.01316299, 0.01316299],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00774293, 0.00774293, 0.00774293],\n",
            "         [0.01509872, 0.01509872, 0.01509872],\n",
            "         ...,\n",
            "         [0.01548587, 0.01548587, 0.01548587],\n",
            "         [0.00038715, 0.00038715, 0.00038715],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.00619435, 0.00619435, 0.00619435],\n",
            "         [0.01045296, 0.01045296, 0.01045296],\n",
            "         ...,\n",
            "         [0.00929152, 0.00929152, 0.00929152],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]],\n",
            "\n",
            "\n",
            "       [[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ]]]])>, <tf.Tensor: id=24515, shape=(16, 3), dtype=int16, numpy=\n",
            "array([[1, 0, 0],\n",
            "       [0, 0, 1],\n",
            "       [0, 0, 1],\n",
            "       [1, 0, 0],\n",
            "       [0, 1, 0],\n",
            "       [1, 0, 0],\n",
            "       [1, 0, 0],\n",
            "       [0, 1, 0],\n",
            "       [0, 1, 0],\n",
            "       [0, 1, 0],\n",
            "       [0, 0, 1],\n",
            "       [1, 0, 0],\n",
            "       [1, 0, 0],\n",
            "       [1, 0, 0],\n",
            "       [0, 1, 0],\n",
            "       [1, 0, 0]], dtype=int16)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21SFjMn3g7ue",
        "colab_type": "text"
      },
      "source": [
        "print(os.listdir(\"training_data\"))\n",
        "train_data= tf.data.Dataset.from_tensor_slices(os.listdir(\"training_data\"))\n",
        "for i in train_data:\n",
        "  print(i)\n",
        "  break\n",
        "train_data = train_data.skip(1000)\n",
        "for i in train_data:\n",
        "  print(i)\n",
        "  break\n",
        "train_data = train_data.skip(1000)\n",
        "for i in train_data:\n",
        "  print(i)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZfJOqJ16mbg",
        "colab_type": "text"
      },
      "source": [
        "[os.system(\"rm -rf \"+\"training_data/\"+f) for f in os.listdir(\"training_data/\") if \".np\" in f or \".csv\" in f]\n",
        "[os.system(\"rm -rf \"+\"validation_data/\"+f) for f in os.listdir(\"validation_data/\") if \".np\" in f or \".csv\" in f]\n",
        "[os.system(\"rm -rf \"+\"testing_data/\"+f) for f in os.listdir(\"testing_data/\") if \".np\" in f or \".csv\" in f]\n",
        "!rm -rf \"training_data/npz validation_data/npz testing_data/npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyh_sPtY7UMy",
        "colab_type": "text"
      },
      "source": [
        "### Load Image Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoGqScRpu7F5",
        "colab_type": "text"
      },
      "source": [
        "def retrieveImage(file_name):\n",
        "  f = h5py.File(file_name,'r')\n",
        "  mri_image = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return mri_image/mri_image.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_gT4oC7bYb",
        "colab_type": "text"
      },
      "source": [
        "### Load Tumor Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcPw_HqJ7pkX",
        "colab_type": "text"
      },
      "source": [
        "def retrieveTumorImage(file_name):\n",
        "  f = h5py.File(file_name,'r')\n",
        "  mri_image = np.array(f['cjdata']['tumorMask'],dtype=np.float128)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return mri_image/mri_image.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-c5y3LXGnCE",
        "colab_type": "text"
      },
      "source": [
        "### Create Directories for ImageGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry3jlMuXKdmD",
        "colab_type": "text"
      },
      "source": [
        "!mkdir \"training_data/images/\"\n",
        "!mkdir \"training_data/images/1\"\n",
        "!mkdir \"training_data/images/2\"\n",
        "!mkdir \"training_data/images/3\"\n",
        "\n",
        "!mkdir \"testing_data/images/\"\n",
        "!mkdir \"testing_data/images/1\"\n",
        "!mkdir \"testing_data/images/2\"\n",
        "!mkdir \"testing_data/images/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF_-ayUh7jlS",
        "colab_type": "text"
      },
      "source": [
        "### Load Image and Tumor Statistics to Panda\n",
        "\n",
        "*   Panda df would have 5 point summary of both mri and tumor\n",
        "*   data directory will have label wise subdirectories for ImageGenerator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUvYbhjEzet",
        "colab_type": "text"
      },
      "source": [
        "def return_imageInfo_from_mat_file(dir,file_name):\n",
        "    f = h5py.File(dir+file_name,'r')\n",
        "\n",
        "    mri_image = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "    #scaler = MinMaxScaler(feature_range=(1,2))\n",
        "    #mri_image = scaler.fit(mri_image)\n",
        "    mri_image = mri_image/mri_image.max()\n",
        "\n",
        "    if mri_image.shape[0] < 512:\n",
        "      print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "    \n",
        "    temp_mri_image = np.copy(mri_image)\n",
        "    temp_mri_image[temp_mri_image == 0 ] = 2\n",
        "\n",
        "    mri_quartiles = np.percentile(mri_image[mri_image > 0], [25, 50, 75,80,85,90,95,96,97,98,99])\n",
        "\n",
        "    tumor_image = np.array(f['cjdata']['tumorMask'], dtype=np.float128)\n",
        "    if tumor_image.shape[0] < 512:\n",
        "      print(\"Shape of the tumor image : \", tumor_image.shape)\n",
        "      tumor_image = np.pad(tumor_image,(512 - tumor_image.shape[0])//2,'constant',constant_values=0)\n",
        "    \n",
        "    tumor_image = temp_mri_image * tumor_image\n",
        "    tumor_image = tumor_image[tumor_image > 0]\n",
        "    tumor_image[tumor_image == 2] = 0\n",
        "\n",
        "    '''tumor_array =[]\n",
        "    for i in range(0,512):\n",
        "      for j in range(0,512):\n",
        "        if tumor_image[i][j]:\n",
        "          tumor_array.append(mri_image[i][j])\n",
        "\n",
        "    tumor_image = np.array(tumor_array, dtype=np.float)'''\n",
        "\n",
        "    tumor_quartiles = np.percentile(tumor_image, [25, 50, 75,80,85,90,95,96,97,98,99])\n",
        "\n",
        "    label=np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "    imageio.imwrite(dir+\"images/\"+str(label)+\"/\"+file_name.split(\".\")[0]+'.jpg', np.array(f['cjdata']['image'],dtype=np.int16))\n",
        "\n",
        "    return np.array(f['cjdata']['PID'],dtype=np.int)[0][0] \\\n",
        "            ,mri_image.min() \\\n",
        "            ,mri_image.max() \\\n",
        "            ,mri_quartiles[0] \\\n",
        "            ,mri_quartiles[1] \\\n",
        "            ,mri_quartiles[2] \\\n",
        "            ,mri_quartiles[3] \\\n",
        "            ,mri_quartiles[4] \\\n",
        "            ,mri_quartiles[5] \\\n",
        "            ,mri_quartiles[6] \\\n",
        "            ,mri_quartiles[7] \\\n",
        "            ,mri_quartiles[8] \\\n",
        "            ,mri_quartiles[9] \\\n",
        "            ,mri_quartiles[10] \\\n",
        "            ,tumor_image.min() \\\n",
        "            ,tumor_image.max() \\\n",
        "            ,tumor_quartiles[0] \\\n",
        "            ,tumor_quartiles[1] \\\n",
        "            ,tumor_quartiles[2] \\\n",
        "            ,tumor_quartiles[3] \\\n",
        "            ,tumor_quartiles[4] \\\n",
        "            ,tumor_quartiles[5] \\\n",
        "            ,tumor_quartiles[6] \\\n",
        "            ,tumor_quartiles[7] \\\n",
        "            ,tumor_quartiles[8] \\\n",
        "            ,tumor_quartiles[9] \\\n",
        "            ,tumor_quartiles[10] \\\n",
        "            ,tumor_image.shape \\\n",
        "            ,file_name\\\n",
        "            ,np.array(f['cjdata']['label'], dtype=np.int)[0][0] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h9X8MvPNy50",
        "colab_type": "text"
      },
      "source": [
        "def loadDf(dir=\"training_data/\"):\n",
        "  patients_details = []\n",
        "  '''for root, dirs, files in os.walk(\"/content/drive/My Drive/1512427/\", topdown = False):\n",
        "    for f in files:\n",
        "      if \".zip\" in f:\n",
        "          file = zipfile.ZipFile(root+f, \"r\")\n",
        "          for name in file.namelist():\n",
        "            file.extract(name,\".\")\n",
        "            patients_details.append(return_imageInfo_from_mat_file(name))\n",
        "          #break\n",
        "      #break  '''   \n",
        "  \n",
        "  for f in getFileNames(dir):\n",
        "    if \".mat\" in f:\n",
        "      patients_details.append(return_imageInfo_from_mat_file(dir,f))\n",
        "  mri_col_names = [\"mri_min\",\"mri_max\",\"mri_1q\",\"mri_median\", \"mri_3q\",\"mri_80\",\"mri_85\",\"mri_90\",\"mri_95\",\"mri_96\",\"mri_97\",\"mri_98\",\"mri_99\"]\n",
        "  tumor_col_names = [\"t_min\",\"t_max\",\"t_1q\",\"t_median\",\"t_3q\",\"t_80\",\"t_85\",\"t_90\",\"t_95\",\"t_96\",\"t_97\",\"t_98\",\"t_99\",\"tumor_size\"]\n",
        "  col_names = [\"pid\"] + mri_col_names + tumor_col_names+ [\"file_name\",\"label\"]\n",
        "  return pd.DataFrame(patients_details,columns=col_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooal44AcufXG",
        "colab_type": "text"
      },
      "source": [
        "### Load Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCSo34a4Vlta",
        "colab_type": "text"
      },
      "source": [
        "df = loadDf()\n",
        "df[\"square_shape\"] = df.tumor_size.apply(lambda x: np.sqrt(x[0]))\n",
        "df.sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-G7l5uvulcY",
        "colab_type": "text"
      },
      "source": [
        "### Load Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upUrUpLMunt1",
        "colab_type": "text"
      },
      "source": [
        "df_test = loadDf(\"testing_data/\")\n",
        "df_test[\"square_shape\"] = df_test.tumor_size.apply(lambda x: np.sqrt(x[0]))\n",
        "df_test.sample(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCGj95Q6u1eC",
        "colab_type": "text"
      },
      "source": [
        "### Test the loaded data (both Training and Testing Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBtoXDYUu7Zc",
        "colab_type": "text"
      },
      "source": [
        "def displayNpImages(dir=\"training_data/\"):\n",
        "  for i in range(1,4):\n",
        "    print(\"2 samples for \",tumor_names[i])\n",
        "    for j in random.choices( os.listdir(dir+str(i)),k=2):\n",
        "      plt.imshow(plt.imread(dir+str(i)+\"/\"+str(j)))\n",
        "      plt.show()\n",
        "\n",
        "#displayNpImages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udE3comcx3Oe",
        "colab_type": "text"
      },
      "source": [
        "#### Visual Testing Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHWSr83x8mO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displayNpImages(\"testing_data/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJc8C31m9Efe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displayNpImages(\"validation_data/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeU2lq6Lzb_-",
        "colab_type": "text"
      },
      "source": [
        "def analyzeZipDir(df,start,end):\n",
        "  df[\"file_num\"] = df.file_name.apply(lambda x: x.split(\".\")[0])\n",
        "  df[\"file_num\"] = df.file_num.astype(np.int)\n",
        "  \n",
        "  return df[df.file_num.isin(list(range(start,end)))]\n",
        "analyzeZipDir(df.copy(),1,766).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UnHytYu1RhE",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df.copy(),1533,2298).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0By0gpPh1W8R",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df_test.copy(),2299,3064).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHedgA0R2cy9",
        "colab_type": "text"
      },
      "source": [
        "analyzeZipDir(df.copy(),767,1532).groupby(\"label\").agg(\"count\").reset_index()[[\"label\",\"pid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-XfW7wbi2n",
        "colab_type": "text"
      },
      "source": [
        "### Create Test directory for validation through generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpZ1DnxSFdz",
        "colab_type": "text"
      },
      "source": [
        "!rm -rf \"test\"\n",
        "!mkdir \"test\"\n",
        "!mkdir \"test/1\"\n",
        "!mkdir \"test/2\"\n",
        "!mkdir \"test/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPpIuurTUKt",
        "colab_type": "text"
      },
      "source": [
        "!ls -l /content/data/2/3046.jpg\n",
        "!ls -l /content/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhVfTlhdXtG-",
        "colab_type": "text"
      },
      "source": [
        "!ls -l /content/data/2/2404.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GLeYdm-cWAU",
        "colab_type": "text"
      },
      "source": [
        "### Move the test set to test directory from data directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMyBzfxxP07c",
        "colab_type": "text"
      },
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/data\", topdown = False):\n",
        "  \n",
        "    \n",
        "    if len(files) > 0:\n",
        "      print(root, dirs, files)\n",
        "\n",
        "      #indices = np.random.randint(0,len(files),size=round(len(files)*.2))\n",
        "      rand_files = random.choices(files,k=round(len(files)*.2))\n",
        "      \n",
        "      for f in rand_files:\n",
        "        #print(f)\n",
        "        try:\n",
        "          shutil.move(root+\"/\"+f, \"/content/test/\"+root.split(\"/\")[-1]+\"/\"+f)\n",
        "        except :\n",
        "          print(\"Ignoring : \",f)\n",
        "\n",
        "#list(os.walk(\"/content/data\")) /content/test /content/training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-eDxHYafKUK",
        "colab_type": "text"
      },
      "source": [
        "### ImageGenerators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pyYghx5gdCe",
        "colab_type": "text"
      },
      "source": [
        "### Load tf.data\n",
        "* inspired from: https://stackoverflow.com/questions/48309631/tensorflow-tf-data-dataset-reading-large-hdf5-files\n",
        "* extract all files\n",
        "* save file names in a tensor\n",
        "* write a generator function to read a file and return numpy mri image array and its label\n",
        "* using tf.dataset.interleave function, read the file concurrently\n",
        "* apply batch and shuffle\n",
        "* feed it to model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77irdWdhtaxN",
        "colab_type": "text"
      },
      "source": [
        "##### Download the MRI image zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgd4_g2h7rYa",
        "colab_type": "text"
      },
      "source": [
        "def mygenerator(file_name, dir=\"training_data/\"):\n",
        "  #print(file_name)\n",
        "  f = h5py.File(dir+file_name,'r')\n",
        "  #print(f['cjdata']['image'].dtype)\n",
        "  mri_image = np.array(f['cjdata']['image'],dtype=np.int16)\n",
        "  if mri_image.shape[0] < 512:\n",
        "      #print(\"Shape of the image : \", mri_image.shape)\n",
        "      mri_image = np.pad(mri_image,(512 - mri_image.shape[0])//2,'constant',constant_values=0)\n",
        "  return (mri_image, np.array(f['cjdata']['label'], dtype=np.int)[0][0])\n",
        "\n",
        "\n",
        "df_temp = pd.DataFrame([mygenerator(f) for f in getFileNames()],columns=[\"image\",\"label\"])\n",
        "\n",
        "'''ds = tf.data.Dataset.from_tensor_slices([  tf.data.Dataset.from_generator(\n",
        "        mygenerator(filename), \n",
        "        (tf.uint8,tf.int8), \n",
        "        (tf.TensorShape([]), tf.TensorShape([None]))) for filename in ds])'''\n",
        "df_temp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4-RohAP6hK",
        "colab_type": "text"
      },
      "source": [
        "imageio.imwrite(\"test.jpg\",df_temp.iloc[0][0])\n",
        "plt.imshow(imageio.imread(\"test.jpg\"),cmap='bone')\n",
        "plt.show()\n",
        "plt.imshow(plt.imread(\"test.jpg\"))\n",
        "plt.show()\n",
        "plt.imshow(df_temp.iloc[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgjK1-K4N8Ue",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(np.array(df_temp.iloc[0][0],dtype=np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLreCMdqiiAq",
        "colab_type": "text"
      },
      "source": [
        "class ToTensor:\n",
        "  def __init__(self):\n",
        "    print(\"Obj Created\")\n",
        "  \n",
        "  def unzipData(self,source_dirname, dest_dir=\"data/\"):\n",
        "    try:\n",
        "      shutil.rmtree(dest_dir)\n",
        "    except:\n",
        "      pass\n",
        "    os.mkdir(dest_dir)\n",
        "    print(\"Exploring \",source_dirname, \"directory\")\n",
        "    for root, dirs, files in os.walk(source_dirname, topdown = False):\n",
        "      for f in files:\n",
        "        print(\"Found file \",f)\n",
        "        if \".zip\" in f:\n",
        "          print(\"Unzipping \", f)\n",
        "          with zipfile.ZipFile(f) as zf:\n",
        "            zf.extractall(dest_dir)\n",
        "    print(\"Files Loaded !!!\")\n",
        "    return self\n",
        "\n",
        "  def getFileName(self,dir_name=\"data/\"):\n",
        "    return os.listdir(dir_name)\n",
        "\n",
        "toTensor = ToTensor()\n",
        "toTensor.unzipData(\"/content/drive/My Drive/1512427/\").getFileName()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0KYEepMovPx",
        "colab_type": "text"
      },
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tqRGoUK5APW",
        "colab_type": "text"
      },
      "source": [
        "## Statistical Analysis\n",
        "* Number of patients in the dataset\n",
        "* Patient wise distribution of tumor classes\n",
        "* Comparison of below attributes for the 3 tumor classes\n",
        "  * 1st quantile of MRI image\n",
        "  * Median of the MRI image\n",
        "  * 3rd quantile of the MRI image\n",
        "  * min value distribution of the Tumor\n",
        "  * max value distribution of the Tumor\n",
        "  * 1st quantile of the Tumor\n",
        "  * median of the Tumor\n",
        "  * 3rd quantile of the Tumor\n",
        "    * Analysis: \n",
        "      * All tumors have darkest area which may indicate the tumor itself\n",
        "      * All tumors have uniform distribution of brightness (apart from the dark area)\n",
        "      * MRI images have darker area outside the skull (non scan area)\n",
        "          * will this influence the model ?\n",
        "          * should the color of the tumor and the non important area of the MRI scan be different ?\n",
        "          \n",
        "* 256x256 image size distribution (any bias in there ?)\n",
        "\n",
        "##### Issues Faced:\n",
        "* Bokeh plots are interactive but they consume a lot of space(>100mb) in the notebook \n",
        "  * markdown for now, when interested can be seen by enabling it as code cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aumAqfm3VymM",
        "colab_type": "text"
      },
      "source": [
        "df.pid.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXwJqH_jnfor",
        "colab_type": "text"
      },
      "source": [
        "There are only 5 patients info present !!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvBti1CInkCG",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"pid\").agg(\"count\").reset_index()[['pid','mri_min']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-95548oIsq",
        "colab_type": "text"
      },
      "source": [
        "df.groupby([\"pid\",\"label\"]).agg(\"count\").reset_index()[['pid','label','mri_min']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z0gzrC-qPjs",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"label\").agg(\"count\").reset_index()[['label','pid']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCP1RnBrn2n",
        "colab_type": "text"
      },
      "source": [
        "def plotStatistics(df, tumor_name):\n",
        "  df = df[[\"mri_1q\",\"mri_median\",\"mri_3q\",\"t_min\",\"t_1q\",\"t_median\",\"t_3q\",\"t_max\"]]\n",
        "  df=(df-df.min())/(df.max()-df.min())\n",
        "  fig, ax = plt.subplots(1, 8,sharex=True,sharey=True,tight_layout=True)\n",
        "  fig.set_figheight(4)\n",
        "  fig.set_figwidth(13)\n",
        "  \n",
        "  fig.suptitle(tumor_name+\" Tumor\")\n",
        "  #plt.subplot(1,8,1)\n",
        "  ax[0].hist(df.mri_1q.tolist())\n",
        "  ax[0].set_title(\"mri_1q\")\n",
        "  #plt.subplot(1,8,2)\n",
        "  ax[1].hist(df.mri_median.tolist())\n",
        "  ax[1].set_title(\"mri_median\")\n",
        "  #plt.subplot(1,8,3)\n",
        "  ax[2].hist(df.mri_3q.tolist())\n",
        "  ax[2].set_title(\"mri_3q\")\n",
        "  #plt.subplot(1,8,4)\n",
        "  ax[3].hist(df.t_min.tolist())\n",
        "  ax[3].set_title(\"t_min\")\n",
        "  #plt.subplot(1,8,5)\n",
        "  ax[4].hist(df.t_1q.tolist())\n",
        "  ax[4].set_title(\"t_1q\")\n",
        "  #plt.subplot(1,8,6)\n",
        "  ax[5].hist(df.t_median.tolist())\n",
        "  ax[5].set_title(\"t_median\")\n",
        "  #plt.subplot(1,8,7)\n",
        "  ax[6].hist(df.t_3q.tolist())\n",
        "  ax[6].set_title(\"t_3q\")\n",
        "  #plt.subplot(1,8,8)\n",
        "  ax[7].hist(df.t_max.tolist())\n",
        "  ax[7].set_title(\"t_max\")\n",
        "  plt.show()\n",
        "\n",
        "plotStatistics(df[df.label ==1], tumor_names[1])\n",
        "plotStatistics(df[df.label ==2], tumor_names[2])\n",
        "plotStatistics(df[df.label ==3], tumor_names[3])\n",
        "plotStatistics(df, \"All\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW6obIAoho0m",
        "colab_type": "text"
      },
      "source": [
        "## Can we reduce the image size ?\n",
        "* Why?\n",
        "  * faster model building\n",
        "  * lower convolution experiment iterations\n",
        "  * lower ram usage and hence higher batch size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQYMkEwLlfPv",
        "colab_type": "text"
      },
      "source": [
        "### Approach 1 : Can we segregate the skull ?\n",
        "  * removing the unwanted area \n",
        "    * percentile approach: identify the percentile and see if any of the tumor percentile is always less that the MRI percentile. \n",
        "        i.e. to prove mri_99 > t_99. This failed as indicated below\n",
        "    * brightness based skull identification:\n",
        "      * nearest neighbor ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H8qcdkeIFyo",
        "colab_type": "text"
      },
      "source": [
        "df[df.mri_99 < df.t_99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAI0eeqfjUZi",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(\"120.mat\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCNwwdLSoLjt",
        "colab_type": "text"
      },
      "source": [
        "### Approach 2: PCA\n",
        "* Note that if we do the PCA transformation, we 2D image will be reduced to 1D. Therefore, we can not use it for the Convolution approach.\n",
        "  * we can note here that just by 2 features (components) and the trained PCA model, we are able to recreate the image with not much difference. [See the last 3 images] This showcases the PCA strength.\n",
        "\n",
        "* Withhelding the PCA approach as we are going to pursuit the Convolution \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOkU29MBoSzg",
        "colab_type": "text"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=5,whiten=True)\n",
        "image=[]\n",
        "image.append(retrieveImage(\"120.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"1.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"2.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"3.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"4.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"5.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"6.mat\").reshape(-1))\n",
        "image.append(retrieveImage(\"7.mat\").reshape(-1))\n",
        "\n",
        "#print(image.shape)\n",
        "\n",
        "pca.fit(image)\n",
        "plt.imshow(pca.mean_.reshape((512,512)),\n",
        "           cmap=plt.cm.bone)\n",
        "plt.show()\n",
        "\n",
        "print(pca.noise_variance_)\n",
        "print(image[0].reshape((1,-1)).shape)\n",
        "pca.transform(image[0].reshape((1,-1)))\n",
        "\n",
        "#plt.imshow(pca.transform(image[1].reshape(1,-1)).reshape((512,512)),cmap=plt.cm.bone)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ptnmf6M4Phe",
        "colab_type": "text"
      },
      "source": [
        "components = pca.transform(image[0].reshape(1,-1))\n",
        "projected = pca.inverse_transform(components)\n",
        "plt.imshow(projected.reshape((512,512)))\n",
        "plt.show()\n",
        "plt.imshow(image[0].reshape((512,512)))\n",
        "plt.show()\n",
        "plt.imshow(retrieveImage(\"120.mat\"))\n",
        "plt.show()\n",
        "print(\"doe it match :\", projected.reshape((512,512)) == retrieveImage(\"120.mat\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmKL2wU4eHoA",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(image[0].reshape((256,256)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NZjperm5HpZ",
        "colab_type": "text"
      },
      "source": [
        "## Visual Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYCnbIgH360L",
        "colab_type": "text"
      },
      "source": [
        "### Smallest Tumor Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH9v4orNwTFk",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(list(df[df.tumor_size == df.tumor_size.min()]['file_name'])[0]));\n",
        "plt.imshow(retrieveTumorImage(list(df[df.tumor_size == df.tumor_size.min()]['file_name'])[0]),alpha=0.5);\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj_utuj44fUp",
        "colab_type": "text"
      },
      "source": [
        "### Biggest Tumor in the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0btsuHfkxfty",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(retrieveImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]));\n",
        "plt.imshow(retrieveTumorImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]),alpha=0.5);\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Jgv6oj30uI",
        "colab_type": "text"
      },
      "source": [
        "### Numpy Resize failed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aYJNq38zCRL",
        "colab_type": "text"
      },
      "source": [
        "plt.imshow(np.resize(retrieveImage(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0]),(256,256)));\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJy0WXR3A_Td",
        "colab_type": "text"
      },
      "source": [
        "### Bokeh Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqrwFc48BCh9",
        "colab_type": "text"
      },
      "source": [
        "def bokehPlot(file_name, tumor_label):\n",
        "  tumor_names = [\"\",\"Meningioma\",\"Glioma\",\"Pituitary\"]\n",
        "  im = retrieveImage(file_name)\n",
        "  s1 = figure(width=512, plot_height=512, title=tumor_names[tumor_label]+\" MRI Image\")\n",
        "  s1.image([im],x=[0],y=[0],dw=[512],dh=[512])\n",
        "\n",
        "  im2 = retrieveTumorImage(file_name)\n",
        "\n",
        "  s2 = figure(width=500, plot_height=500, title=tumor_names[tumor_label]+\" MRI Image with Tumor Highlighted\")\n",
        "  s2.image([im2],x=[0],y=[0],dw=[512],dh=[512])\n",
        "  s2.image([im],x=[0],y=[0],dw=[512],dh=[512],global_alpha=0.5)\n",
        "\n",
        "  show(row(s1,s2))\n",
        "\n",
        "bokehPlot(list(df[df.tumor_size == df.tumor_size.max()]['file_name'])[0], list(df[df.tumor_size == df.tumor_size.max()]['label'])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ0e1zAWWE4l",
        "colab_type": "text"
      },
      "source": [
        "#### Meningioma Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9dliKZMWINQ",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 1].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pP6euXXWqpU",
        "colab_type": "text"
      },
      "source": [
        "#### Glioma Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22qcn6KiWvMJ",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 2].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_3qNVQqV9A8",
        "colab_type": "text"
      },
      "source": [
        "#### Pituitary Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8P5A-cQO79W",
        "colab_type": "text"
      },
      "source": [
        "for fname in list(df[df.label == 3].sample(3)[\"file_name\"]):\n",
        "  bokehPlot(fname,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdvYCpyo2vC",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6QgmzMKv6Hw",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing ideas:\n",
        "\n",
        "1.  Dataset has tumor region indicator which would allow us to get the average brightness of the area.\n",
        "\n",
        "2. It is said that brightest region is skull and skull is not important for the tumor detection. It is only brain position determines the tumor class. If we remove skull remaining image is brain ?\n",
        "\n",
        "3. if we start with a window of image which would maximize the presence of tumor and expand to include some brain region around the tumor then i guess it is the best data for training(and predicting). Because tumor position in brain is THE factor that decides the tumor class.\n",
        "\n",
        "4. what is the optimum batch size for training?\n",
        "\n",
        "5. what is the overall Image augumented training dataset size ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDE9zmipLimp",
        "colab_type": "text"
      },
      "source": [
        "## Train & Test split\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dA-tbvOLt9b",
        "colab_type": "text"
      },
      "source": [
        "def getSplit(df):\n",
        "  df_test=df.sample(frac=.2)\n",
        "  df = df.drop(df_test.index)\n",
        "  return df, df_test\n",
        "\n",
        "df_orig = df.copy()\n",
        "df,df_test = getSplit(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGWNu-Zpk04v",
        "colab_type": "text"
      },
      "source": [
        "## Batch Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvedxzrclan5",
        "colab_type": "text"
      },
      "source": [
        "df.groupby(\"label\").agg(\"count\").reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81fcRggmMy5",
        "colab_type": "text"
      },
      "source": [
        "def returnBatchIndices(df,batch_size):\n",
        "  label_1 = df[df.label == 1].index.tolist()\n",
        "  label_2 = df[df.label == 2].index.tolist()\n",
        "  label_3 = df[df.label == 3].index.tolist()\n",
        "\n",
        "  label_list = []\n",
        "  #print(len(label_1), len(label_2),len(label_3),list(range(0,max(len(label_1),len(label_2),len(label_3)),batch_size)))\n",
        "  for i in range(0,max(len(label_1),len(label_2),len(label_3)),batch_size):\n",
        "    label_list.append(label_1[i:i+batch_size] + label_2[i:i+batch_size] + label_3[i:i+batch_size])\n",
        "  return label_list\n",
        "\n",
        "#yieldbatch(df,5)\n",
        "for batch in returnBatchIndices(df,5):\n",
        "  print(batch)\n",
        "  break\n",
        "\n",
        "print(\"Total Number of Batches: \", len(returnBatchIndices(df,5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk60VHloMRo3",
        "colab_type": "text"
      },
      "source": [
        "### For Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexzUUlVk3pi",
        "colab_type": "text"
      },
      "source": [
        "def returnABatch(df,batch_size):\n",
        "  #returns a balanced label mri images\n",
        "  index_list = returnBatchIndices(df,batch_size)\n",
        "  #print(\"index list\",len(list(index_list)))\n",
        "  df2 = pd.get_dummies(df['label'], prefix = 'label')\n",
        "  df = pd.concat([df,df2],axis=1)\n",
        "  for j in index_list:\n",
        "    batch_images=[]\n",
        "    batch_labels=[]\n",
        "    #print(\"j\",j)\n",
        "    for i in j:\n",
        "      #print(\"i\",i)\n",
        "      label_list=[]\n",
        "      image = retrieveImage(list(df[df.index == i]['file_name'])[0])\n",
        "      transformed_image = image.reshape((512,512,1))\n",
        "      batch_images.append(transformed_image)\n",
        "      label_list.append(df[df.index == i]['label_1'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_2'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_3'].tolist()[0])\n",
        "      batch_labels.append(label_list)\n",
        "      #print(\"Batches :\",len(batch_images),len(batch_labels))\n",
        "\n",
        "    #from keras.utils import to_categorical\n",
        "    #batch_labels = to_categorical(batch_labels)\n",
        "    yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "for i in returnABatch(df.reset_index(),2)  :\n",
        "  if i[1].shape[0] < 6:\n",
        "    print(len(i),len(i[1]))\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql7EHi_cMYYP",
        "colab_type": "text"
      },
      "source": [
        "### For Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhi_DvKXMXYN",
        "colab_type": "text"
      },
      "source": [
        "def returnABatch1d(df,batch_size):\n",
        "  #returns a balanced label mri images\n",
        "  index_list = returnBatchIndices(df,batch_size)\n",
        "  #print(\"index list\",len(list(index_list)))\n",
        "  df2 = pd.get_dummies(df['label'], prefix = 'label')\n",
        "  df = pd.concat([df,df2],axis=1)\n",
        "  for j in index_list:\n",
        "    batch_images=[]\n",
        "    batch_labels=[]\n",
        "    #print(\"j\",j)\n",
        "    for i in j:\n",
        "      #print(\"i\",i)\n",
        "      label_list=[]\n",
        "      image = retrieveImage(list(df[df.index == i]['file_name'])[0])\n",
        "      transformed_image = image.reshape(512*512)\n",
        "      batch_images.append(transformed_image)\n",
        "      label_list.append(df[df.index == i]['label_1'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_2'].tolist()[0])\n",
        "      label_list.append(df[df.index == i]['label_3'].tolist()[0])\n",
        "      batch_labels.append(label_list)\n",
        "      #print(\"Batches :\",len(batch_images),len(batch_labels))\n",
        "\n",
        "    #from keras.utils import to_categorical\n",
        "    #batch_labels = to_categorical(batch_labels)\n",
        "    yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "for i in returnABatch1d(df.reset_index(),2)  :\n",
        "  if i[1].shape[0] < 6:\n",
        "    print(len(i),len(i[1]))\n",
        "    print(i[0].shape)\n",
        "    print(i[1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFyXNbgckbCt",
        "colab_type": "text"
      },
      "source": [
        "#Model Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAFFed2pEX8W",
        "colab_type": "text"
      },
      "source": [
        "## CNN Approach using Tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcguDqrmyuR-",
        "colab_type": "code",
        "outputId": "362ef0fd-1eb9-483a-bc94-1a072272716b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(512,512,1))\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,tf.keras.layers.Flatten()\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(512, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])#,\"Precision\",\"Recall\"])\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 510, 510, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 255, 255, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 253, 253, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 124, 124, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 60, 60, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 1,366,979\n",
            "Trainable params: 1,366,979\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZHApFUtxgH",
        "colab_type": "text"
      },
      "source": [
        "img = plt.imread(\"/content/data/1/1.jpg\")\n",
        "plt.imshow(img,cmap=\"bone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-AGrENCXpN1",
        "colab_type": "text"
      },
      "source": [
        "target = df_temp.pop('label')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df_temp.values, target.values))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecS_esHM67zr",
        "colab_type": "code",
        "outputId": "30994414-1417-4c82-8034-be68c0792488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "history=model.fit_generator(myCNNTrainGenerator(16)\n",
        "                  ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "                  , epochs=10\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=myCNNValidateGenerator(16)\n",
        "                  ,validation_steps = len(os.listdir(\"validation_data/npz\"))\n",
        "                  #,workers=8\n",
        "                  )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "135/135 [==============================] - 1015s 8s/step - loss: 0.8150 - accuracy: 0.6429 - val_loss: 0.5990 - val_accuracy: 0.7609\n",
            "Epoch 2/10\n",
            "135/135 [==============================] - 1012s 7s/step - loss: 0.5990 - accuracy: 0.7291 - val_loss: 0.5177 - val_accuracy: 0.8261\n",
            "Epoch 3/10\n",
            "135/135 [==============================] - 1026s 8s/step - loss: 0.4964 - accuracy: 0.7874 - val_loss: 0.3297 - val_accuracy: 0.8696\n",
            "Epoch 4/10\n",
            "135/135 [==============================] - 1021s 8s/step - loss: 0.4482 - accuracy: 0.8093 - val_loss: 0.3362 - val_accuracy: 0.8261\n",
            "Epoch 5/10\n",
            "135/135 [==============================] - 1026s 8s/step - loss: 0.3632 - accuracy: 0.8434 - val_loss: 0.2611 - val_accuracy: 0.9130\n",
            "Epoch 6/10\n",
            "135/135 [==============================] - 998s 7s/step - loss: 0.3338 - accuracy: 0.8564 - val_loss: 0.2685 - val_accuracy: 0.9022\n",
            "Epoch 7/10\n",
            "135/135 [==============================] - 1033s 8s/step - loss: 0.2637 - accuracy: 0.8914 - val_loss: 0.2408 - val_accuracy: 0.9130\n",
            "Epoch 8/10\n",
            "135/135 [==============================] - 1024s 8s/step - loss: 0.2236 - accuracy: 0.9058 - val_loss: 0.1923 - val_accuracy: 0.9348\n",
            "Epoch 9/10\n",
            "135/135 [==============================] - 1037s 8s/step - loss: 0.1984 - accuracy: 0.9156 - val_loss: 0.1848 - val_accuracy: 0.9348\n",
            "Epoch 10/10\n",
            "135/135 [==============================] - 1021s 8s/step - loss: 0.1748 - accuracy: 0.9310 - val_loss: 0.1895 - val_accuracy: 0.9348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBueyG7UzkVi",
        "colab_type": "text"
      },
      "source": [
        "model.save('./final_model.h5', include_optimizer=True)\n",
        "from tensorflow.keras.models import load_model\n",
        "model2 = load_model('./final_model.h5')\n",
        "history2=model2.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=8\n",
        "                  )\n",
        "\n",
        "result2 = model2.evaluate(test_generator)\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ex2GT1x-6SG",
        "colab_type": "code",
        "outputId": "489ff7b1-fb72-41f9-9ff8-147e51ee75d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "result = model.evaluate(myCNNTestGenerator(16))\n",
        "print(result)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 63s 1s/step - loss: 0.2277 - accuracy: 0.9069\n",
            "[0.22771831495890563, 0.9068923821039904]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uljl3NAf8Jos",
        "colab_type": "text"
      },
      "source": [
        "history=model.fit_generator(validate_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=2\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=test_generator\n",
        "                  ,workers=8\n",
        "                  )\n",
        "result = model.evaluate(test_generator)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJhyb__OadUy",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7OMaFAT5_hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4srxIYwTagJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "11a04b3e-fbab-4e4e-e155-d961677a28b3"
      },
      "source": [
        "def preTrain():\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "      https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "      -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "  local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "  pre_trained_model = InceptionV3(input_shape = (512,512,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  \n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('mixed7')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  #x = layers.Flatten()(last_output)\n",
        "  x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='sigmoid')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "  return model\n",
        "imageNet_model = preTrain()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 13:16:08--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.126.128, 2607:f8b0:4001:c14::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M   123MB/s    in 0.7s    \n",
            "\n",
            "2019-10-15 13:16:09 (123 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n",
            "last layer output shape:  (None, 30, 30, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSF1E-NkAZr",
        "colab_type": "text"
      },
      "source": [
        "imageNet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQPFkf4k7jhJ",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPEHY67206Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9849bc17-2986-4b60-b18b-715bfe9c9c5d"
      },
      "source": [
        "def pretrain2():\n",
        "  #https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "     https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "     -O /tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "\n",
        "  # https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5 \\\n",
        "  # -O /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
        "  from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "  from tensorflow.keras.applications.densenet import DenseNet121\n",
        "\n",
        "  local_weights_file = '/tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "  #'/tmp/mobilenet_1_0_128_tf_no_top.h5'\n",
        "\n",
        "  pre_trained_model = DenseNet121(input_shape = (512,512,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('relu')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  '''from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  '''\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  x = layers.Flatten()(last_output)\n",
        "  #x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='softmax')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  history=model.fit_generator(myCNNtfrTrainGenerator(16)\n",
        "                  ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "                  , epochs=10\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=myCNNVtfralidateGenerator(16)\n",
        "                  ,validation_steps = len(os.listdir(\"validation_data/npz\"))\n",
        "                  #,workers=8\n",
        "                  )\n",
        "pretrain2()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 14:43:31--  https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f5b4b85e-fa1e-11e7-9a46-5fbe25b60245?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191015%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191015T144331Z&X-Amz-Expires=300&X-Amz-Signature=f0f73b26c79ace8c8affab8e66397e615f83e8a97c1028fa05be4e485e9f4e67&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddensenet121_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-10-15 14:43:31--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f5b4b85e-fa1e-11e7-9a46-5fbe25b60245?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191015%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191015T144331Z&X-Amz-Expires=300&X-Amz-Signature=f0f73b26c79ace8c8affab8e66397e615f83e8a97c1028fa05be4e485e9f4e67&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddensenet121_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.147.67\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.147.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30011760 (29M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/densenet121_we 100%[===================>]  28.62M  49.0MB/s    in 0.6s    \n",
            "\n",
            "2019-10-15 14:43:32 (49.0 MB/s) - ‘/tmp/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [30011760/30011760]\n",
            "\n",
            "last layer output shape:  (None, 16, 16, 1024)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 518, 518, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 256, 256, 64) 9408        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 256, 256, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 256, 256, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 258, 258, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 128, 128, 64) 0           zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 128, 128, 64) 256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 128, 128, 64) 0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 128, 128, 128 8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 128, 128, 128 0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 128, 128, 96) 0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 128, 128, 96) 384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 128, 128, 96) 0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 128, 128, 128 12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 128, 128, 128 0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 128, 128, 128 0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 128, 128, 128 512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 128, 128, 128 0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 128, 128, 128 16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 128, 128, 128 0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 128, 128, 160 0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 128, 128, 160 640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 128, 128, 160 0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 128, 128, 128 20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 128, 128, 128 0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 128, 128, 192 0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 128, 128, 192 768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 128, 128, 192 0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 128, 128, 128 24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 128, 128, 128 0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 128, 128, 224 0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 128, 128, 224 896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 128, 128, 224 0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 128, 128, 128 28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 128, 128, 128 512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 128, 128, 128 0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 128, 128, 32) 36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 128, 128, 256 0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 128, 128, 256 1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 128, 128, 256 0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 128, 128, 128 32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 64, 64, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 64, 64, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 64, 64, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 64, 64, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 64, 64, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 64, 64, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 64, 64, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 64, 64, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 64, 64, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 64, 64, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 64, 64, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 64, 64, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 64, 64, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 64, 64, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 64, 64, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 64, 64, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 64, 64, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 64, 64, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 64, 64, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 64, 64, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 64, 64, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 64, 64, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 64, 64, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 64, 64, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 64, 64, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 64, 64, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 64, 64, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 64, 64, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 64, 64, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 64, 64, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 64, 64, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 64, 64, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 64, 64, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 64, 64, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 64, 64, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 64, 64, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 64, 64, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 64, 64, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 64, 64, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 64, 64, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 64, 64, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 64, 64, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 64, 64, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 64, 64, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 64, 64, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 64, 64, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 64, 64, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 64, 64, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 64, 64, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 64, 64, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 64, 64, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 64, 64, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 64, 64, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 64, 64, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 64, 64, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 64, 64, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 64, 64, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 64, 64, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 64, 64, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 64, 64, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 64, 64, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 64, 64, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 64, 64, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 64, 64, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 64, 64, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 64, 64, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 32, 32, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 32, 32, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 32, 32, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 32, 32, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 32, 32, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 32, 32, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 32, 32, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 32, 32, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 32, 32, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 32, 32, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 32, 32, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 32, 32, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 32, 32, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 32, 32, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 32, 32, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 32, 32, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 32, 32, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 32, 32, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 32, 32, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 32, 32, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 32, 32, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 32, 32, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 32, 32, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 32, 32, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 32, 32, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 32, 32, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 32, 32, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 32, 32, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 32, 32, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 32, 32, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 32, 32, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 32, 32, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 32, 32, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 32, 32, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 32, 32, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 32, 32, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 32, 32, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 32, 32, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 32, 32, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 32, 32, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 32, 32, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 32, 32, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 32, 32, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 32, 32, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 32, 32, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 32, 32, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 32, 32, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 32, 32, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 32, 32, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 32, 32, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 32, 32, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 32, 32, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 32, 32, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 32, 32, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 32, 32, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 32, 32, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 32, 32, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 32, 32, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 32, 32, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 32, 32, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 32, 32, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 32, 32, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 32, 32, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 32, 32, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 32, 32, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 32, 32, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 32, 32, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 32, 32, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 32, 32, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 32, 32, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 32, 32, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 32, 32, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 32, 32, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 32, 32, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 32, 32, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 32, 32, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 32, 32, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 32, 32, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 32, 32, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 32, 32, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 32, 32, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 32, 32, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 32, 32, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 32, 32, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 32, 32, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 32, 32, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 32, 32, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 32, 32, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 32, 32, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 32, 32, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 32, 32, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 32, 32, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 32, 32, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 32, 32, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 32, 32, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 32, 32, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 32, 32, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 32, 32, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 32, 32, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 32, 32, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 32, 32, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 32, 32, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 32, 32, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 32, 32, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 32, 32, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 32, 32, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 32, 32, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 32, 32, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 32, 32, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 32, 32, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 16, 16, 512)  0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 16, 16, 512)  0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 16, 16, 128)  65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 16, 16, 128)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 16, 16, 544)  0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 16, 16, 544)  2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 16, 16, 544)  0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 16, 16, 128)  69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 16, 16, 128)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 16, 16, 576)  0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 16, 16, 576)  2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 16, 16, 576)  0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 16, 16, 128)  73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 16, 16, 128)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 16, 16, 608)  0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 16, 16, 608)  2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 16, 16, 608)  0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 16, 16, 128)  77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 16, 16, 128)  0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 16, 16, 640)  0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 16, 16, 640)  2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 16, 16, 640)  0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 16, 16, 128)  81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 16, 16, 128)  0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 16, 16, 672)  0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 16, 16, 672)  2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 16, 16, 672)  0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 16, 16, 128)  86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 16, 16, 128)  0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 16, 16, 704)  0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 16, 16, 704)  2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 16, 16, 704)  0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 16, 16, 128)  90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 16, 16, 128)  0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 16, 16, 736)  0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 16, 16, 736)  2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 16, 16, 736)  0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 16, 16, 128)  94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 16, 16, 128)  0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 16, 16, 768)  0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 16, 16, 768)  3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 16, 16, 768)  0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 16, 16, 128)  98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 16, 16, 128)  512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 16, 16, 128)  0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 16, 16, 32)   36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 16, 16, 800)  0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 16, 16, 800)  3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 16, 16, 800)  0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 16, 16, 128)  102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 16, 16, 832)  0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 16, 16, 832)  3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 16, 16, 832)  0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 16, 16, 128)  106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 16, 16, 864)  0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 16, 16, 864)  3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 16, 16, 864)  0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 16, 16, 128)  110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 16, 16, 896)  0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 16, 16, 896)  3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 16, 16, 896)  0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 16, 16, 128)  114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 16, 16, 928)  0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 16, 16, 928)  3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 16, 16, 928)  0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 16, 16, 128)  118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 16, 16, 960)  0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 16, 16, 960)  3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 16, 16, 960)  0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 16, 16, 128)  122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 16, 16, 992)  0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 16, 16, 992)  3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 16, 16, 992)  0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 16, 16, 128)  126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 16, 16, 128)  512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 16, 16, 128)  0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 16, 16, 32)   36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 16, 16, 1024) 0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 16, 16, 1024) 4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 16, 16, 1024) 0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 262144)       0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          134218240   flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            1539        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 141,257,283\n",
            "Trainable params: 134,219,779\n",
            "Non-trainable params: 7,037,504\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "135/135 [==============================] - 2363s 18s/step - loss: 11.2456 - accuracy: 0.3016 - val_loss: 11.3610 - val_accuracy: 0.2935\n",
            "Epoch 2/10\n",
            "  5/135 [>.............................] - ETA: 40:08 - loss: 9.6709 - accuracy: 0.4000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Keras_worker_ForkPoolWorker-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d33498b76a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                   \u001b[0;31m#,workers=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                   )\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mpretrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d33498b76a66>\u001b[0m in \u001b[0;36mpretrain2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyCNNVtfralidateGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_data/npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                   \u001b[0;31m#,workers=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                   )\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7SCLpTz7uxK",
        "colab_type": "text"
      },
      "source": [
        "### MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVKk8Hn37pb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e2bf6dd-982d-409d-e25c-e094af7bb36b"
      },
      "source": [
        "def mobilenet():\n",
        "  #https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "     https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5 \\\n",
        "     -O /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
        "  from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "  \n",
        "  local_weights_file = '/tmp/mobilenet_1_0_128_tf_no_top.h5'\n",
        "  \n",
        "\n",
        "  pre_trained_model = MobileNet(input_shape = (255,255,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  print(pre_trained_model.summary())\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('conv_pw_13_relu')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "\n",
        "  '''from tensorflow.keras.optimizers import RMSprop\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(2,2)(x)\n",
        "  \n",
        "  # Flatten the output layer to 1 dimension\n",
        "  #x = layers.\n",
        "  x = layers.Flatten()(last_output)\n",
        "  x = layers.Dropout(0.5)(x)  \n",
        "  #x = layers.Flatten()(x)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.5)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='softmax')(x)  '''         \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Conv2D(3, (3,3), activation=\"relu\", input_shape=(512,512,1))\n",
        "            ,tf.keras.layers.MaxPooling2D(2,2)\n",
        "            ,pre_trained_model\n",
        "            ,tf.keras.layers.AveragePooling2D(7,7)\n",
        "            ,tf.keras.layers.Flatten()\n",
        "            ,tf.keras.layers.Dense(1000, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(512, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "          ])\n",
        "  #model = Model( pre_trained_model.input, x) \n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  history=model.fit_generator(myCNNTrainGenerator(16)\n",
        "                  ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "                  , epochs=20\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=myCNNValidateGenerator(16)\n",
        "                  ,validation_steps = len(os.listdir(\"validation_data/npz\"))\n",
        "                  #,workers=8\n",
        "                  )\n",
        "  print(model.evaluate(myCNNtfrTestGenerator(16)))\n",
        "mobilenet()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-16 13:54:35--  https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191016T135435Z&X-Amz-Expires=300&X-Amz-Signature=f3875f203cf242225c9e0964f055064b84bd9262a9e517f44849c6a01bbb3b09&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-10-16 13:54:35--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191016%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191016T135435Z&X-Amz-Expires=300&X-Amz-Signature=f3875f203cf242225c9e0964f055064b84bd9262a9e517f44849c6a01bbb3b09&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.225.144\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.225.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17225924 (16M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/mobilenet_1_0_128_tf_no_top.h5’\n",
            "\n",
            "/tmp/mobilenet_1_0_ 100%[===================>]  16.43M  46.8MB/s    in 0.4s    \n",
            "\n",
            "2019-10-16 13:54:36 (46.8 MB/s) - ‘/tmp/mobilenet_1_0_128_tf_no_top.h5’ saved [17225924/17225924]\n",
            "\n",
            "Model: \"mobilenet_1.00_255\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 255, 255, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 127, 127, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 127, 127, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 127, 127, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 127, 127, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 127, 127, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 127, 127, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 127, 127, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 63, 63, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 63, 63, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 63, 63, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 63, 63, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 63, 63, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 63, 63, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 63, 63, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 31, 31, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 31, 31, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 31, 31, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 31, 31, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 31, 31, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 15, 15, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 15, 15, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 15, 15, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 15, 15, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "=================================================================\n",
            "Total params: 3,228,864\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "None\n",
            "last layer output shape:  (None, 7, 7, 1024)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 510, 510, 3)       30        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 255, 255, 3)       0         \n",
            "_________________________________________________________________\n",
            "mobilenet_1.00_255 (Model)   (None, 7, 7, 1024)        3228864   \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1000)              1025000   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               512512    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 4,767,945\n",
            "Trainable params: 1,539,081\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "135/135 [==============================] - 175s 1s/step - loss: 1.1465 - accuracy: 0.5492 - val_loss: 1.3695 - val_accuracy: 0.2609\n",
            "Epoch 2/20\n",
            "135/135 [==============================] - 171s 1s/step - loss: 0.6840 - accuracy: 0.6965 - val_loss: 2.3565 - val_accuracy: 0.2609\n",
            "Epoch 3/20\n",
            "135/135 [==============================] - 170s 1s/step - loss: 0.6144 - accuracy: 0.7436 - val_loss: 3.3268 - val_accuracy: 0.2609\n",
            "Epoch 4/20\n",
            "135/135 [==============================] - 172s 1s/step - loss: 0.5226 - accuracy: 0.7702 - val_loss: 4.0483 - val_accuracy: 0.2609\n",
            "Epoch 5/20\n",
            "135/135 [==============================] - 171s 1s/step - loss: 0.5185 - accuracy: 0.7758 - val_loss: 4.1357 - val_accuracy: 0.2609\n",
            "Epoch 6/20\n",
            "  2/135 [..............................] - ETA: 2:43 - loss: 0.3427 - accuracy: 0.9062"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Keras_worker_ForkPoolWorker-9:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0c0a098c3492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                   )\n\u001b[1;32m     75\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyCNNtfrTestGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mmobilenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-0c0a098c3492>\u001b[0m in \u001b[0;36mmobilenet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyCNNValidateGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                   \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_data/npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                   \u001b[0;31m#,workers=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                   )\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by4izwHZAL4z",
        "colab_type": "text"
      },
      "source": [
        "#### MobileNet v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjKu5QJAueF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e22afb9-cebd-4093-8d05-3d032eb3a417"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SkipDataset shapes: (<unknown>, <unknown>), types: (tf.float64, tf.int16)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w6VtzpgAPah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d9cf191-16e6-456b-e336-269ee1684938"
      },
      "source": [
        "def mobilenet2():\n",
        "  #https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
        "  import os\n",
        "\n",
        "  from tensorflow.keras import layers\n",
        "  from tensorflow.keras import Model\n",
        "  !wget --no-check-certificate \\\n",
        "     https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5 \\\n",
        "     -O /tmp/mobilenet_1_0_128_tf_no_top.h5\n",
        "  from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "  \n",
        "  local_weights_file = '/tmp/mobilenet_1_0_128_tf_no_top.h5'\n",
        "  \n",
        "\n",
        "  pre_trained_model = MobileNet(input_shape = (224,224,3), \n",
        "                                  include_top = False, \n",
        "                                  weights = None)\n",
        "\n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #print(pre_trained_model.summary())\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  print(pre_trained_model.summary())\n",
        "\n",
        "  #last_layer = pre_trained_model.get_layer('conv_pw_13_relu')\n",
        "  last_layer = pre_trained_model.get_layer('conv_pad_12')\n",
        "  last_output = last_layer.output\n",
        "  #print('last layer output shape: ', last_layer.output_shape)\n",
        "  #last_output = last_layer.output\n",
        "  x = tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\")(last_output)\n",
        "  x = tf.keras.layers.MaxPooling2D()(x)\n",
        "  #x = tf.keras.layers.AveragePooling2D((6,6))(x)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  x = tf.keras.layers.Dense(3,activation=\"softmax\")(x)\n",
        "\n",
        "  model = Model( pre_trained_model.input, x)\n",
        "  '''\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.2)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (1, activation='sigmoid')(x) \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            #pre_trained_model\n",
        "            last_layer\n",
        "            ,tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\")\n",
        "            ,tf.keras.layers.MaxPooling2D()\n",
        "            ,tf.keras.layers.AveragePooling2D((7,7))\n",
        "            ,tf.keras.layers.Flatten()\n",
        "            ,tf.keras.layers.Dense(1024, activation=\"relu\")\n",
        "            #,tf.keras.layers.Conv2D(512, (3,3), activation=\"relu\")\n",
        "            #,tf.keras.layers.MaxPooling2D()\n",
        "            #,tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\")\n",
        "            #,tf.keras.layers.MaxPooling2D()\n",
        "            #,tf.keras.layers.Flatten()\n",
        "            #,tf.keras.layers.Dense(1000, activation=\"relu\")\n",
        "            ,tf.keras.layers.Dropout(0.2)\n",
        "            #,tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "            #,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(3,activation=\"softmax\")            \n",
        "          ])\n",
        "  #model = Model( pre_trained_model.input, x) '''\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.summary()\n",
        "  print(model.predict(validation_data,steps=1))\n",
        "  history=model.fit_generator(\n",
        "                  training_data\n",
        "                  ,epochs=1\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validation_data\n",
        "                  )\n",
        "  print(model.evaluate(validation_data,steps=1))\n",
        "mobilenet2()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-21 00:28:15--  https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf_no_top.h5\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191021%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191021T002815Z&X-Amz-Expires=300&X-Amz-Signature=d1b8bd938853ae8c3a06427efc4f1663a0c4b375002e3138788b335a081d87ad&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-10-21 00:28:15--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/f0241862-5d89-11e7-9e1c-85c9b5d36aaf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191021%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191021T002815Z&X-Amz-Expires=300&X-Amz-Signature=d1b8bd938853ae8c3a06427efc4f1663a0c4b375002e3138788b335a081d87ad&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmobilenet_1_0_128_tf_no_top.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.232.123\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.232.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17225924 (16M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/mobilenet_1_0_128_tf_no_top.h5’\n",
            "\n",
            "\r          /tmp/mobi   0%[                    ]       0  --.-KB/s               \r         /tmp/mobil  23%[===>                ]   3.78M  18.9MB/s               \r/tmp/mobilenet_1_0_ 100%[===================>]  16.43M  45.5MB/s    in 0.4s    \n",
            "\n",
            "2019-10-21 00:28:16 (45.5 MB/s) - ‘/tmp/mobilenet_1_0_128_tf_no_top.h5’ saved [17225924/17225924]\n",
            "\n",
            "Model: \"mobilenet_1.00_224\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "=================================================================\n",
            "Total params: 3,228,864\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        294976    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                147520    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 2,070,531\n",
            "Trainable params: 442,691\n",
            "Non-trainable params: 1,627,840\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-40ab4f1b7351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m                   )\n\u001b[1;32m     80\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mmobilenet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-40ab4f1b7351>\u001b[0m in \u001b[0;36mmobilenet2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   history=model.fit_generator(\n\u001b[1;32m     75\u001b[0m                   \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 408\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;34m\"\"\"A single step of the distributed execution across replicas.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     x, y, sample_weights = _prepare_feed_values(\n\u001b[0;32m---> 66\u001b[0;31m         model, input_iterator, mode)\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Call `Model.{train,test,predict}_on_batch` on every replica passing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# PerReplicas as arguments.  On every replica inside this call, each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_prepare_feed_values\u001b[0;34m(model, inputs, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mwill\u001b[0m \u001b[0malways\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mwrapped\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \"\"\"\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_input_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;31m# When the inputs are dict, then we want to flatten it in the same order as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_get_input_from_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;31m# Validate that all the elements in x and y are of the same type and shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   dist_utils.validate_distributed_dataset_inputs(\n\u001b[0;32m--> 149\u001b[0;31m       distribution_strategy_context.get_strategy(), x, y, sample_weights)\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mvalidate_distributed_dataset_inputs\u001b[0;34m(distribution_strategy, x, y, sample_weights)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;31m# If each element of x and y are not tensors, we cannot standardize and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;31m# validate the input and targets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mx_values_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_per_replica_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mvalidate_per_replica_inputs\u001b[0;34m(distribution_strategy, x)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;31m# Validate that the shape and dtype of all the elements in x are the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mvalidate_all_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0mvalidate_all_tensor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mvalidate_all_tensor_shapes\u001b[0;34m(x, x_values)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_all_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0;31m# Validate that the shape of all the elements in x have the same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m   \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_shape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"as_list() is not defined on an unknown TensorShape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S3IXXqNnUOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "9564969c-13d7-4802-c8c5-23f78ecd54d9"
      },
      "source": [
        "def pretrain1():\n",
        "  import tensorflow_hub as hub\n",
        "\n",
        "  module_selection = (\"mobilenet_v2\", 224) #@param [\"(\\\"mobilenet_v2\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "  handle_base, pixels = module_selection\n",
        "  #MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "  MODULE_HANDLE = \"https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3\"\n",
        "  IMAGE_SIZE = (pixels, pixels)\n",
        "  print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
        "\n",
        "  BATCH_SIZE = 16 #@param {type:\"integer\"}\n",
        "\n",
        "  print(\"Building model with\", MODULE_HANDLE)\n",
        "  model = tf.keras.Sequential([\n",
        "      hub.Module(MODULE_HANDLE, tags={\"train\"}, trainable=False),\n",
        "      tf.keras.layers.Dropout(rate=0.2),\n",
        "      tf.keras.layers.Dense(3, activation='softmax',\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "  ])\n",
        "  model.build((None,)+IMAGE_SIZE+(3,))\n",
        "  model.summary()\n",
        "\n",
        "pretrain1()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3 with input size (224, 224)\n",
            "Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_050_96/feature_vector/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4af54639eb34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpretrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-4af54639eb34>\u001b[0m in \u001b[0;36mpretrain1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HANDLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   model = tf.keras.Sequential([\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HANDLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       tf.keras.layers.Dense(3, activation='softmax',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m           tags=self._tags)\n\u001b[0m\u001b[1;32m    171\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[1;32m    404\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         import_scope=relative_scope_name)\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Build a list from the variable name in the module definition to the actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[1;32m   1452\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                                  **kwargs)[0]\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m   \u001b[0;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[1;32m   1464\u001b[0m                        \u001b[0;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m                        \"execution is enabled.\")\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlUhmzy2380C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageNet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxHeuBcA4TVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageNet_model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUHrCKk4ZMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator, validate_generator, test_generator = getGenerators(32,(512,512))\n",
        "history=imageNet_model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=3\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  #,workers=2\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cIJzWiY4dkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = imageNet_model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgY15ze8jZet",
        "colab_type": "text"
      },
      "source": [
        "## Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rieJMvjwtIy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bstData(loc = \"training_data/\"):\n",
        "  print(datetime.datetime.now())\n",
        "  image_list = []\n",
        "  label_list = []\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  #ddd = pd.DataFrame(columns=[str(x) for x in range(512*512)]+[\"label\"])\n",
        "  i =0\n",
        "  image_stacks = np.ndarray((2500,(512*512)+1),dtype=np.float64)\n",
        "  print(image_stacks.shape)\n",
        "  for file_name in [f  for f in os.listdir(loc) if \".mat\" in f]:\n",
        "    if i % 200 == 0:\n",
        "      print(i, end=\" \")\n",
        "      #break\n",
        "    with h5py.File(loc+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float64)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] != 512:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          \n",
        "          #image_list.append((image_array,  label_transform[label]))\n",
        "          #image_list.append(image_array.reshape(-1))\n",
        "          #label_list.append(label_transform[label])\n",
        "          image_array_label = np.concatenate((image_array.reshape(-1), [label]))\n",
        "          #print(image_array.reshape(-1).shape, image_array_label.shape)\n",
        "          #image_stacks = np.stack((image_stacks,image_array_label))\n",
        "          image_stacks[i] = image_array_label\n",
        "          i=i+1\n",
        "\n",
        "  #df = pd.DataFrame({\"image_array\":image_list,\"label\":label_list})\n",
        "  df = pd.DataFrame(image_stacks[:i],columns=[str(x) for x in range(512*512)]+[\"label\"])\n",
        "  print(\"Load Data End Time: \", datetime.datetime.now())\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kfFLA7jH1jC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "852a4aac-2e97-4064-d58b-f618b86d2ba8"
      },
      "source": [
        "df_bst = bstData()\n",
        "df_bst.head()\n",
        "df_bst.info()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-18 13:32:14.698719\n",
            "(2500, 262145)\n",
            "0 200 400 600 800 1000 1200 1400 1600 1800 2000 Load Data End Time:  2019-10-18 13:32:34.086877\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2145 entries, 0 to 2144\n",
            "Columns: 262145 entries, 0 to label\n",
            "dtypes: float64(262145)\n",
            "memory usage: 4.2 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wwl_dvAf1VI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "a587a1a9-3ef0-41dd-d56a-3a8db4de77fb"
      },
      "source": [
        "df_bst_eval = bstData(\"testing_data/\")\n",
        "df_bst_eval.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-18 13:32:56.337980\n",
            "(2500, 262145)\n",
            "0 200 400 600 800 Load Data End Time:  2019-10-18 13:33:03.867332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>262105</th>\n",
              "      <th>262106</th>\n",
              "      <th>262107</th>\n",
              "      <th>262108</th>\n",
              "      <th>262109</th>\n",
              "      <th>262110</th>\n",
              "      <th>262111</th>\n",
              "      <th>262112</th>\n",
              "      <th>262113</th>\n",
              "      <th>262114</th>\n",
              "      <th>262115</th>\n",
              "      <th>262116</th>\n",
              "      <th>262117</th>\n",
              "      <th>262118</th>\n",
              "      <th>262119</th>\n",
              "      <th>262120</th>\n",
              "      <th>262121</th>\n",
              "      <th>262122</th>\n",
              "      <th>262123</th>\n",
              "      <th>262124</th>\n",
              "      <th>262125</th>\n",
              "      <th>262126</th>\n",
              "      <th>262127</th>\n",
              "      <th>262128</th>\n",
              "      <th>262129</th>\n",
              "      <th>262130</th>\n",
              "      <th>262131</th>\n",
              "      <th>262132</th>\n",
              "      <th>262133</th>\n",
              "      <th>262134</th>\n",
              "      <th>262135</th>\n",
              "      <th>262136</th>\n",
              "      <th>262137</th>\n",
              "      <th>262138</th>\n",
              "      <th>262139</th>\n",
              "      <th>262140</th>\n",
              "      <th>262141</th>\n",
              "      <th>262142</th>\n",
              "      <th>262143</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>0.005565</td>\n",
              "      <td>0.007705</td>\n",
              "      <td>0.005565</td>\n",
              "      <td>0.005137</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>0.010702</td>\n",
              "      <td>0.012628</td>\n",
              "      <td>0.011772</td>\n",
              "      <td>0.010488</td>\n",
              "      <td>0.009846</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>0.013271</td>\n",
              "      <td>0.013271</td>\n",
              "      <td>0.011558</td>\n",
              "      <td>0.009204</td>\n",
              "      <td>0.010060</td>\n",
              "      <td>0.010274</td>\n",
              "      <td>0.011344</td>\n",
              "      <td>0.011130</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>0.009632</td>\n",
              "      <td>0.011130</td>\n",
              "      <td>0.009418</td>\n",
              "      <td>0.010274</td>\n",
              "      <td>0.009204</td>\n",
              "      <td>0.007491</td>\n",
              "      <td>0.008134</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008990</td>\n",
              "      <td>0.009632</td>\n",
              "      <td>0.007491</td>\n",
              "      <td>0.009204</td>\n",
              "      <td>0.011772</td>\n",
              "      <td>0.010702</td>\n",
              "      <td>0.010702</td>\n",
              "      <td>0.011130</td>\n",
              "      <td>0.012414</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>0.004281</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>0.011344</td>\n",
              "      <td>0.011772</td>\n",
              "      <td>0.008990</td>\n",
              "      <td>0.007920</td>\n",
              "      <td>0.007277</td>\n",
              "      <td>0.009418</td>\n",
              "      <td>0.012842</td>\n",
              "      <td>0.011344</td>\n",
              "      <td>0.008348</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.007920</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>0.006635</td>\n",
              "      <td>0.004495</td>\n",
              "      <td>0.006207</td>\n",
              "      <td>0.007920</td>\n",
              "      <td>0.009846</td>\n",
              "      <td>0.010702</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>0.004281</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>0.001712</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.004437</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.009317</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>0.013310</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.011535</td>\n",
              "      <td>0.009760</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.015528</td>\n",
              "      <td>0.014641</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.011535</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.011979</td>\n",
              "      <td>0.013753</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>0.013310</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0.009760</td>\n",
              "      <td>0.013310</td>\n",
              "      <td>0.015084</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>0.015528</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011979</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.017303</td>\n",
              "      <td>0.014197</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.010204</td>\n",
              "      <td>0.011979</td>\n",
              "      <td>0.010204</td>\n",
              "      <td>0.012422</td>\n",
              "      <td>0.014197</td>\n",
              "      <td>0.013753</td>\n",
              "      <td>0.017746</td>\n",
              "      <td>0.020852</td>\n",
              "      <td>0.017746</td>\n",
              "      <td>0.014641</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.010648</td>\n",
              "      <td>0.015084</td>\n",
              "      <td>0.009317</td>\n",
              "      <td>0.008873</td>\n",
              "      <td>0.013753</td>\n",
              "      <td>0.015528</td>\n",
              "      <td>0.011979</td>\n",
              "      <td>0.008873</td>\n",
              "      <td>0.009317</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>0.009760</td>\n",
              "      <td>0.008429</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.003106</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 262145 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4  ...    262140    262141    262142    262143  label\n",
              "0  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.000000  0.000000  0.000000    2.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  ...  0.004281  0.003211  0.001712  0.001284    2.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  ...  0.002218  0.000000  0.000000  0.000000    1.0\n",
              "3  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.000000  0.000000  0.000000    1.0\n",
              "4  0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.000000  0.000000  0.000000    1.0\n",
              "\n",
              "[5 rows x 262145 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54KzALIUIrmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORICAL_COLUMNS = ['label']\n",
        "NUMERIC_COLUMNS = list(df_bst)[:-1]\n",
        "\n",
        "dftrain = df_bst.copy()\n",
        "dftrain['label']= dftrain.label.astype(np.int)\n",
        "#y_train = dftrain.pop('label')\n",
        "  \n",
        "def one_hot_cat_column(feature_name, vocab):\n",
        "  return tf.feature_column.indicator_column(\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n",
        "                                                                vocab))\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  # Need to one-hot encode categorical features.\n",
        "  vocabulary = dftrain[feature_name].unique()\n",
        "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
        "  \n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
        "                                                          dtype=tf.float64))\n",
        "y_train = dftrain.pop('label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXBdnoIg6VC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10a368ae-899c-42e9-c992-1d24797fba3b"
      },
      "source": [
        "# Use entire batch since this is such a small dataset.\n",
        "\n",
        "NUM_EXAMPLES = len(y_train)\n",
        "print(NUM_EXAMPLES)\n",
        "\n",
        "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
        "  def input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
        "    if shuffle:\n",
        "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
        "    # For training, cycle thru dataset as many times as need (n_epochs=None).    \n",
        "    dataset = dataset.repeat(n_epochs)\n",
        "    # In memory training doesn't use batching.\n",
        "    dataset = dataset.batch(NUM_EXAMPLES)\n",
        "    return dataset\n",
        "  return input_fn\n",
        "\n",
        "# Training and evaluation input functions.\n",
        "train_input_fn = make_input_fn(dftrain, y_train, n_epochs=1)\n",
        "dfeval = df_bst_eval.copy()\n",
        "y_eval = dfeval.pop(\"label\")\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMnBdBLEjds7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "6013697f-0ec2-4125-a031-5b1e54c4e170"
      },
      "source": [
        "print(datetime.datetime.now())\n",
        "n_batches = 1\n",
        "est = tf.estimator.BoostedTreesClassifier(n_trees=2,train_in_memory=True,   feature_columns=feature_columns, n_batches_per_layer=n_batches)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-18 13:33:39.991369\n",
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjvv4fy0d\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjvv4fy0d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpjvv4fy0d', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1bdd429c88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpjvv4fy0d', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1bdd429c88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/boosted_trees.py:369: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/canned/boosted_trees.py:369: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4i8XN6CWIYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "e90458b2-d984-4d7f-f4ea-0edf7a1397ac"
      },
      "source": [
        "\n",
        "est.train(train_input_fn,max_steps=2)\n",
        "print(datetime.datetime.now())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cef99f6fe0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1185\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m   1186\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m-> 1187\u001b[0;31m               input_fn, ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m       estimator_spec = self._call_model_fn(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1024\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a7cdb375a5c3>\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EXAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m     self._structure = nest.map_structure(\n\u001b[1;32m   2358\u001b[0m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mto_batched_tensor_list\u001b[0;34m(element_spec, element)\u001b[0m\n\u001b[1;32m    346\u001b[0m   return _to_tensor_list_helper(\n\u001b[1;32m    347\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0;32m--> 348\u001b[0;31m           component), element_spec, element)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[0;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   return functools.reduce(\n\u001b[0;32m--> 322\u001b[0;31m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[0;34m(state, value)\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   return functools.reduce(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state, spec, component)\u001b[0m\n\u001b[1;32m    346\u001b[0m   return _to_tensor_list_helper(\n\u001b[1;32m    347\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0;32m--> 348\u001b[0;31m           component), element_spec, element)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU5y5lt9rorA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  print(datetime.datetime.now())\n",
        "  result = est.evaluate(eval_input_fn)\n",
        "  print(result)\n",
        "  print(datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTN_rMHhGlgR",
        "colab_type": "text"
      },
      "source": [
        "#### TFT PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TkirrD6FFv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "560f94a1-c8aa-4dfd-8814-e75971fb9513"
      },
      "source": [
        "!pip install tfx\n",
        "import tensorflow_transform as tft\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/2b/2063d0520cfedf4f2008917f95cd46018f980be49d838d0359e552ca839e/tfx-0.14.0-py3-none-any.whl (384kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 3.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-data-validation<0.15,>=0.14.1 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/86/77ec6a7c5c91ac69798fc3a9c911ff225a4c2833a42fb59d63c8162679e7/tensorflow_data_validation-0.14.1-cp36-cp36m-manylinux2010_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 47.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.15,>=0.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/5e/e56ccce03d780d49755148978693f74c3da12f08e0b903417f5c711e0c12/tensorflow_model_analysis-0.14.0-py3-none-any.whl (777kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.7 in /tensorflow-2.0.0/python3.6 (from tfx) (3.10.0)\n",
            "Requirement already satisfied: six<2,>=1.10 in /tensorflow-2.0.0/python3.6 (from tfx) (1.12.0)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.6/dist-packages (from tfx) (1.7.11)\n",
            "Collecting ml-metadata<0.15,>=0.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/15/902b399494dcb3d96f09795d55f72a1191ba33f3ae737600fd1d5db7f471/ml_metadata-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7.0 in /usr/local/lib/python3.6/dist-packages (from tfx) (7.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b3/b6dcbd94bf8a5ae6a0be5fc988bdfb0b0dfb87ea37e788dc4dcc039a3aee/apache_beam-2.16.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<1,>=0.1.6 in /tensorflow-2.0.0/python3.6 (from tfx) (0.8.1)\n",
            "Collecting tensorflow-transform<0.15,>=0.14 (from tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/84/c8770b330a3fbe4e6a727e3e922a04d3a755a79870e4ee090b959cb01983/tensorflow-transform-0.14.0.tar.gz (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython>=5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (5.5.0)\n",
            "Requirement already satisfied: pandas<1,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.24.2)\n",
            "Requirement already satisfied: tensorflow-metadata<0.15,>=0.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.14.0)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /tensorflow-2.0.0/python3.6 (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (1.17.2)\n",
            "Collecting scikit-learn<0.21,>=0.18 (from tensorflow-data-validation<0.15,>=0.14.1->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/5b/5da31a6572dc6b7b2846a7cfcbe2e060a0e6af0e1059a6516965e40371b7/scikit_learn-0.20.4-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<0.15.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.14.1)\n",
            "Requirement already satisfied: joblib<1,>=0.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.14.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.15,>=0.14->tfx) (7.5.1)\n",
            "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.15,>=0.14->tfx) (1.0.0)\n",
            "Collecting scipy==1.1.0 (from tensorflow-model-analysis<0.15,>=0.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /tensorflow-2.0.0/python3.6 (from protobuf<4,>=3.7->tfx) (41.4.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (3.0.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (1.4.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx) (0.11.3)\n",
            "Requirement already satisfied: grpcio<2,>=1.12.1 in /tensorflow-2.0.0/python3.6 (from apache-beam[gcp]<3,>=2.14->tfx) (1.24.1)\n",
            "Collecting dill<0.3.1,>=0.3.0 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (0.16.0)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (3.9.0)\n",
            "Requirement already satisfied: pyyaml<4.0.0,>=3.12 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (3.13)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.7)\n",
            "Collecting avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b2/98a736a31213d3e281a62bcae5572cf297d2546bc429accf36f9ee1604bf/avro-python3-1.9.1.tar.gz\n",
            "Collecting mock<3.0.0,>=1.0.1 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 24.0MB/s \n",
            "\u001b[?25hCollecting oauth2client<4,>=2.0.1 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (2018.9)\n",
            "Collecting fastavro<0.22,>=0.21.4 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/28/0206330c0002b1e28e21473117d0dc813defbd5891562d27af5c68c93899/fastavro-0.21.24-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.3.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 24.3MB/s \n",
            "\u001b[?25hCollecting python-dateutil<3,>=2.8.0 (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 44.2MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/af/0ef7d097a1d5ad0c843867600e86de915e8ab8864740f49a4636cfb51af6/google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 56.8MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/32/df3e36fd705a00092f1ffa9f41ce1df8dcb594ae313d239b87861a41fc2e/google-apitools-0.5.28.tar.gz (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.0.3)\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (3.1.1)\n",
            "Requirement already satisfied: google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<3,>=2.14->tfx) (1.14.1)\n",
            "Collecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/07a82945a7396ea34debafd476724bb5fc267c292790fdf2138c693f95c5/google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 52.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" (from apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/aa/29cbcf8cf7d08ce2d55b9dce858f7c632b434cb6451bed17cb4275804217/google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (4.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (4.7.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (1.0.18)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.15,>=0.14->tensorflow-data-validation<0.15,>=0.14.1->tfx) (1.6.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.2.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.5.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.6.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client<2,>=1.7.8->tfx) (0.2.6)\n",
            "Collecting pbr>=0.11 (from mock<3.0.0,>=1.0.1->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/a4/d5c83831a3452713e4b4f126149bc4fbda170f7cb16a86a00ce57ce0e9ad/pbr-5.4.3-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<3,>=2.14->tfx) (0.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]<3,>=2.14->tfx) (2.4.2)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (0.6.2)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (2.21.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx) (1.14.2)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3 (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Collecting fasteners>=0.14 (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx) (0.4.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=5.0->tensorflow-data-validation<0.15,>=0.14.1->tfx) (0.1.7)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.5.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (2.6.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (5.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (2.10.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.8.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.4.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.14->tfx) (2.8)\n",
            "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]<3,>=2.14->tfx)\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.15,>=0.14->tfx) (17.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.15,>=0.14->tfx) (0.5.1)\n",
            "Building wheels for collected packages: tensorflow-transform, dill, avro-python3, oauth2client, hdfs, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for tensorflow-transform (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-transform: filename=tensorflow_transform-0.14.0-cp36-none-any.whl size=282799 sha256=5b9cba85c45a647c79318373af4fe5f4806c65994f0ce1958d27bcba2947cb43\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/8f/19/808f4a2d4d23a13b6ec44682fc2662646e8d9193b49f4a5f93\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.0-cp36-none-any.whl size=77513 sha256=b2bd9893b2674fd60c6f5a45d2ccab504b36d7f9d7d2e1c0349f2387a21a1ce8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.1-cp36-none-any.whl size=43199 sha256=1710edc59d19900898e43b5a0f5407108a5948f3e1830f8ffc468eef3ce3e9af\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/54/6f/a5df680fd3224aa45145686f3b1b02a878a90ea769fcf9daaf\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp36-none-any.whl size=106382 sha256=1965988aa8a6d928fb081e16e556a4f446b8a0773200d30d34d1b361ec03e3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33214 sha256=a00111aba82e515a7c2e6f61570c6e7b16c4342f5c6c2c87f4358a185babcd7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.28-cp36-none-any.whl size=130111 sha256=fa14ddf357d8df5a959bfc84770b42cf410337a1f0c70568befed78c0a5a226b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/c2/92/837e8a4d649a209dff85b38d7fbb576b4b480738be70865f29\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp36-none-any.whl size=18499 sha256=f7f803ce84f2c3d4e5fa7b361aaf1cd2eae892109f728ea6fcd61b7cc446a66f\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built tensorflow-transform dill avro-python3 oauth2client hdfs google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.9 has requirement dill>=0.3.1, but you'll have dill 0.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, scikit-learn, dill, avro-python3, pbr, mock, oauth2client, fastavro, hdfs, python-dateutil, grpc-google-iam-v1, google-cloud-bigtable, monotonic, fasteners, google-apitools, google-cloud-pubsub, google-cloud-datastore, apache-beam, tensorflow-transform, tensorflow-data-validation, tensorflow-model-analysis, ml-metadata, tfx\n",
            "  Found existing installation: scipy 1.3.1\n",
            "    Uninstalling scipy-1.3.1:\n",
            "      Successfully uninstalled scipy-1.3.1\n",
            "  Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "  Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "  Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Found existing installation: python-dateutil 2.5.3\n",
            "    Uninstalling python-dateutil-2.5.3:\n",
            "      Successfully uninstalled python-dateutil-2.5.3\n",
            "  Found existing installation: google-cloud-datastore 1.8.0\n",
            "    Uninstalling google-cloud-datastore-1.8.0:\n",
            "      Successfully uninstalled google-cloud-datastore-1.8.0\n",
            "Successfully installed apache-beam-2.16.0 avro-python3-1.9.1 dill-0.3.0 fastavro-0.21.24 fasteners-0.15 google-apitools-0.5.28 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-pubsub-1.0.2 grpc-google-iam-v1-0.12.3 hdfs-2.5.8 ml-metadata-0.14.0 mock-2.0.0 monotonic-1.5 oauth2client-3.0.0 pbr-5.4.3 python-dateutil-2.8.0 scikit-learn-0.20.4 scipy-1.1.0 tensorflow-data-validation-0.14.1 tensorflow-model-analysis-0.14.0 tensorflow-transform-0.14.0 tfx-0.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "google",
                  "oauth2client",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8b0329e58a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tfx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_transform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_transform/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspect_preprocessing_fn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosted_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_quantile_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosted_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantile_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusltMTwF9NZ",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression using Tensorflow Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb8bFUocuERf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "             tf.keras.layers.Flatten(input_shape=(512, 512,3))\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\")            \n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTbDTPvPNtRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit_generator(train_generator\n",
        "                  #,steps_per_epoch=286\n",
        "                  , epochs=5\n",
        "                  ,use_multiprocessing=True\n",
        "                  ,validation_data=validate_generator\n",
        "                  ,workers=2\n",
        "                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDTHRX8Q7_2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.evaluate(test_generator)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V_RPsdvE4dP",
        "colab_type": "text"
      },
      "source": [
        "batch_size = 64\n",
        "steps_per_epoch = round(df.groupby(\"label\").agg(\"count\").reset_index()['pid'].max()/batch_size)\n",
        "\n",
        "print(\"Total Training Dataset : \", df.shape[0])\n",
        "print(\"Batch Size : \", batch_size)\n",
        "print(\"Steps per epoch : \", steps_per_epoch)\n",
        "print(\"Test Datasize shape : \", df_test.shape[0])\n",
        "\n",
        "history=model.fit_generator(returnABatch1d(df,batch_size)\n",
        "                  ,steps_per_epoch=steps_per_epoch\n",
        "                  , epochs=5\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_FIgewtHwKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qfFq4-MXHXs",
        "colab_type": "text"
      },
      "source": [
        "# Model Testing\n",
        "* is the model bias for 256 size images ?\n",
        "* is there any imbalance in 256 size images ?\n",
        "* converting 512x512 to 256x256 size would definitely speed up the process but would it impact the accuracy ?\n",
        "* is the model has better accuracy for any type of tumor class? (as we have imbalanced set ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym6uQGG8gydZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFTO4bLSGRBk",
        "colab_type": "text"
      },
      "source": [
        "# Observations / Lesson Learnt:\n",
        "\n",
        "* Iteration 1:\n",
        "  * CNN of 512x512 took half an hour even on TPU\n",
        "  * more and more convolution layer decreases the neurons required for training( duh!!!) and hence the batch size can be increased.\n",
        "  * testing result was 49%. Not Acceptable.\n",
        "\n",
        "* Iteration 2:\n",
        "  * tried PCA but realized transformation is not suited for convolution\n",
        "  * tried to use imagegenerator by converting the numpy to image (jpg) file. But found that numpy to image was not successfull.\n",
        "\n",
        "* Iteration 3:\n",
        "  * experimented with ImageGenerator for training, validation and testing\n",
        "  * time is still a worry for CNN approach; it is 20 mins\n",
        "  * though it is fast for the logistic regression the epochs are not helping (epoch = 5)\n",
        "\n",
        "\n",
        "* Iteration 4:\n",
        "  * experimented with different CNN layers ( basically added one more layer to make shrink the image size to 6x6 but 14x14 was quicker and gave better result)\n",
        "  * average time per epoch is 20 min and 5 epochs gave 89% test accuracy with CNN approach\n",
        "  * deviation in the epoch accuracy indicates that few more epoch can reach upto 90 - 93% which way ahead of last finding during capstone submission (54%).[validation accuracy has also increased so far and hence predicting the accuracy to go beyond 90%]\n",
        "  * imagenet transfer learing takes 1 hour to complete one epoch and for one epoch it gives accuracy around 45%\n",
        "  * 3 epochs on imagenet with 512x512x3 image with 2 convolution at the end with dense network post that gave 90% training accuracy, 65% validation and testing result. Clearly, there is overfit.\n",
        "  * shrinking the size by 256x256 or 150x150 (as per imagenet assumption on input), drastically reduces the epoch time upto 5 min but the accuracy do not improve even for 5 epochs. Loss decreases but there is no change in the accuracy or accuracy decreases.\n",
        "  * till now, it leaves with no additional lesson learnt apart from few experimental observations. i.e. in the last capstone submission, i had CNN and imagenet experiment. but this time different CNN gave a better result and imagenet is yet to match the outcome of CNN. Only new thing learnt is that simplest conv2d (3x3) with max pooling can give better result.\n",
        "  * as part of new learning , I have :\n",
        "    * tf eager execution, instead of graph based as in last submission\n",
        "    * image generator to simplify code, augment the images and to reshape( not recommended).\n",
        "    * keras way of working with transfer learning\n",
        "    * numpy to image save\n",
        "    * experimenting with workers and parallel processing at \"fit\" and \"fit_generator\".\n",
        "    * couple of experiments were not possible if TPU and colab was not arround.\n",
        "    * bokeh experiments with images.\n",
        "    * automating data load operation through linux commands (wget, unzipping, creation and deletion folders, moving files, listing files)\n",
        "\n",
        "* Iteration 5:\n",
        "  * Saving models:\n",
        "    * I guess below approach is good:\n",
        "      * while(1):\n",
        "          * model.fit()\n",
        "          * model.save('./final_model.h5', include_optimizer=True)\n",
        "\n",
        "* Iteration 6:\n",
        "  * Simple LR has too many loss\n",
        "  * NN also has high loss and hence low accuracy \n",
        "  * NN has limitation number of neurons with which python crashes if the initial neuron shape crosses 8K.\n",
        "\n",
        "\n",
        "* iteration 7:\n",
        "  * PCA + Normalization ( scaling between 0 to 1 ) gave a better result 40 % with PCA features of 1024\n",
        "    * PCA training /fit took half an hour\n",
        "    * Dense layer training took mere 3 min for 100 epochs\n",
        "\n",
        "  * PCA + Normalization ( scaling between 0 to 1 ) gave even better result 45%\n",
        "    * PCA training/fit took 1hr 15 min\n",
        "    * dense layer took around 5-8 min for 100 epochs\n",
        "\n",
        "  * evidentally, it is all about minimizing the loss. More number of features in PCA gives higher accuracy is the clear indication of lossess due to drastic step down from 200k to 1k or 2k neurons. So what are the losses we have in our pipeline?\n",
        "    * images are saved as jpg; at that time we have loss of scaling it to 0-255 UNSIGNED INTEGER.\n",
        "    * later when we do a normalization the loss is compounded.\n",
        "    * there may be loss in scaling and doing pca due to sklearn capability to work on float64 (if such constraints exists)\n",
        "\n",
        "\n",
        "* Iteration 8:\n",
        "  * WTF !!!!, np.int for image array gave LR an accuracy of 45%\n",
        "  * np.float64 for image array gave 72% accuracy\n",
        "  * np.float128 for image array gave 76% for LR classification.\n",
        "  * However, as anticipated Neural Network dint in herit this accuracy change.(45%)\n",
        "      * I though additional layers would fine tune the 1 LR layer but it seems they have introduced loss ( value of 8 for NN against 3 in LR). Where is the loss\n",
        "          * RELU : there may have been loss due to negative activation values. Just like that of PCA can have -ve value an activation value might have been negative and might have been thrown away by RELU.\n",
        "            * one more reason to suspect is that changes in the network apart from additional layer is introduction of activation function relu.\n",
        "\n",
        "        * Sigmoid : tried randomly sigmoid function. Pictorically, I can remember that the graph looks like letter \"S\" (slant), so I know that it handles either side of axes and hence should help if my hypothesis on relu loss.Loss definitely got reduced. Loss value looks better (0.9) when compared to both LR (3)  and relu(8)\n",
        "          * training took more 80 secs, because i vaguley remember that sigmoid has exponential function in its equation\n",
        "          * However, it looks promising, to run more epochs in search of reduced loss and increased accuracy.\n",
        "            * Note that here training accuracy was max at 66% but testing at 42% but it evident that epochs are too low (5 epochs).\n",
        "\n",
        "        * tanh: I know that it does not have exponential functions. Indeed there was reduction in training time (20 secs less per epoch compared to sigmoid : near 60 secs)\n",
        "            * accuracy around 45% again not enough epochs to conclude\n",
        "\n",
        "        * linear: suicidal.(as per my hypothesis) Negative values has to be controlled. Linear does NOT do that. LOSS is huge !!!\n",
        "          * training time : 1 min around, accuracy = 0.3\n",
        "        \n",
        "        * softsign: miniature version of sigmoid. <1 min training time , promissing accuracy ( 60:50).\n",
        "          * unlike expected model did not learn on increased epochs\n",
        "\n",
        "        * selu: Surprisingly failed. \n",
        "          * 61s 1s/step - loss: 11.3066 - accuracy: 0.3044 - Recall: 0.3044 - Precision: 0.3044\n",
        "\n",
        "\n",
        "    * PCA computation mandates that we need to have the entire dataset in memory and hence it is failing in my case.\n",
        "        * float128 for image array consumes most of the RAM leaving no room for PCA or NN\n",
        "\n",
        "Iteration 9: \n",
        "  * Data loading is a simplest task but I have made so many revisions\n",
        "    * with view of minimizing loss, fresh read from the file is consuming 80 seconds per epoch from few seconds!!!\n",
        "  * tensorflow does not support float128, max of float64 is feasible.\n",
        "  * new design is to create batches and save the batches and load it when generator is called.\n",
        "  * random.choices does not work; need to use random.sample instead\n",
        "  * the decrease in the accuracy may have been due to float32 default type\n",
        "    *  tf.keras.backend.floatx()\n",
        "  * I had to cut a layer of 4098 neurons to do the float64 computation. I also had to let go precision and recall computation for the same.\n",
        "  * However, convolution did prove that it is best even now. 3 epochs , 17 min each, 83% validation accuracy. We may take 2 more epochs to compare it with that of imagegenerator result. This comparison would tell us the impact of loss hypothesis.\n",
        "      * with 6x6 as the last convolution we had 90% accuracy (2.5 hour training)\n",
        "      * with 14x14 as the last convolution isze we had same 90% accuracy (1 hr training)\n",
        "      * in both the cases the model overfits, the training accuracy is 93-94% but testing is 90% (even with dropout before softmax)\n",
        "      * Guess it is fair enough to say that LR = 75%, CNN=90%; let us see boosted tree and mobilenet\n",
        "\n",
        "\n",
        "* Iteration 10:\n",
        "  * mobilenet says eager execution is not supported\n",
        "      * https://towardsdatascience.com/easy-image-classification-with-tensorflow-2-0-f734fee52d13 gives the quick review of txfr learning\n",
        "          * However, mobilenet, densenet requires 3 channel data. i.e. mxnx3 shape.\n",
        "              * easy np stack technique actually helps us to build 3 channel \n",
        "  * imagenet is too heavy takes 1 hr for an epoch\n",
        "\n",
        "* Iteration 11:\n",
        "  * Mobilenet or any transfer learning neural networks are industry standards for the image classification problem. We have to effectively use it. Current loss and accuracy is not acceptable. Though it might give the result as that of simple CNN network that we have seen before but it should not fail like this. So continuing the experiments in this area.\n",
        "  * Mobilenet gives a quick turn around: < 10 min\n",
        "  * top layer removed version is 16x16x1024 for input of 512x512x3. This leaves the model have more parameters to train than the frozen one. \n",
        "    * This is the reason we have huge loss and no learning from accuracy.\n",
        "      * tried a averagepooling (16x16) , increased training accuracy but no changes to validation accuracy. Overfitting!!!\n",
        "\n",
        "  * Also my first attempt was to put empty channels to make it 3 channel input to mobilenet. Would repeating the same 2d layers 2 times, give better result ?\n",
        "\n",
        "  * will reducing the size through convolution at first and then feeding the same to mobilenet give better result ? The intuition here is that mobilenet uses 221x221x3 input images and what we have is 512x512x3.\n",
        "\n",
        "  * Mobilenet as is model is not learning instead it overfits. Training accuracy increases but the validation score is static even after many epochs (>5)\n",
        "    * Epoch 3/20\n",
        "135/135 [==============================] - 500s 4s/step - loss: 4.6052 - accuracy: 0.7040 - val_loss: 8.1150 - val_accuracy: 0.4891\n",
        "        * as you could see the loss is also high\n",
        "\n",
        "  * Max pooling gave some ray of hope:\n",
        "  68/68 [==============================] - 441s 6s/step - loss: 0.8351 - accuracy: 0.6154 - val_loss: 3.1877 - val_accuracy: 0.4565\n",
        "\n",
        "  * Apperantly, transfer learning is not a silver bullet.\n",
        "    * tried with 3 channel input by first converting the 2d array to png image(imwrite) then loading the same through tf ImageGenerator\n",
        "    * tried squeezing the image as required by the mobilenet model.\n",
        "      * loss is more and no learning in training or validation\n",
        "    * tried putting max pool alone instead of averagepool. Accuracy floats arround 50%\n",
        "    * 1x1 convolution to reduce the feature space and then max pooling.\n",
        "      * there is loss and and no learning (around 30% accuracy)\n",
        "    * 2x2 or 3x3 convolutions followed by max pool between them\n",
        "        * again loss is huge.\n",
        "\n",
        "    * In summary, due to the size of the image the popular model end up in leaving the last layer dimension as 16x16x1024. This is 1/3rd of the 512x512x3 image. However, this is equal to the original 2d image size.Hence, the learning from this through average or few convolution layers are less.\n",
        "\n",
        "* iteration 12:\n",
        "  * Now let us see if boosted trees can help us or not.\n",
        "  * dataset had to be changed to dataframe as the boosted tree requires column names of the features.\n",
        "  * a dataframe grows in a iteration of image fetch. Currently, takes 20 min for training and 4 mins for testing\n",
        "  * with 512*512 feature columns training is pretty bad slow.\n",
        "    * not even 2 trees with 2 epochs are able to complete in an hour. with around 2200 samples.\n",
        "      *tft.pca might help us in this regard as we are able fit training data in memory\n",
        "\n",
        "* iteration 13: (Reason for MobileNet low performance)\n",
        "  * This iteration was dedicated for image segmentation and then prediction post it.\n",
        "  * while analyzing the code from tensorflow, I was also investingating the data injesting into it. Then several questions arose like what was the original data, how it got preprocessed/transformed.\n",
        "\n",
        "  * During that analysis, it stuck me that ImageGenerator is already converting the single channel image to 3 channel through np.stack approach. I was saving the normalized 512x512x1 array to jpg image. Though there was warning that there is a lossy conversion from 0 to 1 range data to 0 to 255, it did not happen. When the jpg was read it was still in 0 to 1 range. Moreover, we use to scale the image by x 1/255. It used to further reduce the numbers which are already scaled between 0 to 1. To add to that we had the tensor of type np.float32 and not np.float64. This was the reason for the **loss**.\n",
        "\n",
        "  * There was a need of single consolidated processing, rather than hdf5 file processing, storing it in numpy file then through custom generator reading the numpy file. I always know that tensorflow supports it but never explored much in there. Again thanks to tensorflow manuals came to know about the tensor handling through py\n",
        "  _functions which made the data preprocessing code reduced to around 20 lines and 5 minutes operation !!!! Even the train time performance of the dataset preprocessing is amazing - one epoch used to take around 16-18 minutes got reduced to 2.5 minutes!!!! Now I am wondering if the combination could be even more fantastic!!! \n",
        "    * i.e to save the numpy batch through tf.data\n",
        "    * during the train time read the numpy batch through tf.data\n",
        "\n",
        "  * But is the above 2 operation equivalent to caching ????\n",
        "    * Caching can also creates a file and access it lock based post one epoch. It is nice observation to see if the 2 stage data preparation is equivalent to the one stage ingestion with caching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iThtN8-UQc4Y",
        "colab_type": "text"
      },
      "source": [
        "# To Do\n",
        "* Batch normalization for both CNN and LR\n",
        "* dropout between each dense layer\n",
        "* is LR with PCA ( n = say 10K) equal to NN with cone structure ?\n",
        "  * as in, cone structure (decreasing stacked NN layer neurons) reduces the feature vector at each layer so is this a parallel approach to PCA ?\n",
        "\n",
        "* NN Experiments:\n",
        "  * adding addtional CNN layer to make the last dimention as 14x14 or 6x6\n",
        "      * 6x6 took additional time for one epoch and also took addional epochs to reach the accuracy as that of 14x14.\n",
        "      \n",
        "  * PCA and LR:\n",
        "    * 1 million (half of flattened layer) PCA dimention\n",
        "    * 50K PCA dimention\n",
        "    * 10K PCA dimention\n",
        "    * 5k\n",
        "    * 1K\n",
        "    * 512\n",
        "    * 256\n",
        "  * NN layers\n",
        "    * 100K, 50K, 25K, 12K, 6K, 3K, 1K, 512, 256, 128, 64, 32, 16, 8\n",
        "      * batch normalization\n",
        "      * drop outs\n",
        "\n",
        "* Preprocessing:\n",
        "  * skull removal through clustering\n",
        "  * image generator to reduce the size to 256x256\n",
        "\n",
        "\n",
        "* transfer learning\n",
        "* image segmentation\n",
        "  * try with only window of image with tumor for accuracy\n",
        "  * how is image segmentation actually done\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYzFII_AvYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in train_generator:\n",
        "  print(i[1].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZKA21TJB1c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e35b2a92-bfb7-4db5-aa8b-5b947d0e695f"
      },
      "source": [
        "import numpy as np\n",
        "img = np.array([[1, 2], [3, 4]])\n",
        "print(img)\n",
        "stacked_img = np.stack((img,)*3, axis=-1)\n",
        "print(stacked_img.shape)\n",
        "\n",
        "np.zeros((512,512),)\n",
        "\n",
        "np.stack(([[1,2],[3,4]], [[0,0],[0,0]]),axis=-1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "(2, 2, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 0],\n",
              "        [2, 0]],\n",
              "\n",
              "       [[3, 0],\n",
              "        [4, 0]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB0oEZYiGzJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imageflatdf():\n",
        "  imageflatlist=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  #for d in os.listdir( \"training_data/\"):\n",
        "  #  for f in os.listdir(\"training_data/\"+d+\"/\"):\n",
        "  for file_name in os.listdir(\"download/mat\"):\n",
        "      #plt.imshow(plt.imread(\"training_data/1/\"+f))\n",
        "      #print(plt.imread(\"training_data/1/\"+f).reshape(1,-1).shape)\n",
        "      with h5py.File(\"download/mat/\"+file_name,'r') as f:\n",
        "        image_array = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "        label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "        image_array = image_array/image_array.max()\n",
        "        if image_array.shape[0] == 512:\n",
        "          imageflatlist.append((list(image_array.reshape(-1)),label_transform[label]))\n",
        "        else:\n",
        "          image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "          imageflatlist.append((list(image_array.reshape(-1)),label_transform[label])) \n",
        "      #break\n",
        "  df = pd.DataFrame(imageflatlist,columns=['image_array','label'])\n",
        "  return df\n",
        "\n",
        "df_flat = imageflatdf()\n",
        "df_flat.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukVkA3BZ3N5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(df):\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    return pca\n",
        "\n",
        "pca = test(df_flat.copy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxth9ywUYDXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv50XhoNSkiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.groupby(\"label\").agg(\"count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SXp2d-O0qmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_flat.image_array[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDZHQnZy0Rea",
        "colab_type": "text"
      },
      "source": [
        "#PCA + LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOaYkAXtBY2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(len(os.listdir(\"/content/validation_data/npz/\"))//32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgl8ShdQPwZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doLR():\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(3, input_shape=[512*512], activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  batch_size=32\n",
        "  model.fit_generator(myTrainGenerator(batch_size)\n",
        "          ,epochs=100\n",
        "          ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "          ,validation_data=myValidateGenerator(batch_size)\n",
        "          ,validation_steps=len(os.listdir(\"validation_data/npz/\"))\n",
        "          )\n",
        "  \n",
        "  return model.evaluate(myTestGenerator(batch_size))\n",
        "\n",
        "doLR()\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHxm_SkUzAWv",
        "colab_type": "text"
      },
      "source": [
        "def doLR(df):\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  #df['label'] = pd.Categorical(df['label'])\n",
        "  #df['label'] = df.label.astype(np.int)\n",
        "\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  \n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(3, input_shape=[262144], activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label))\n",
        "  \n",
        "  for feat, targ in dataset.take(5):\n",
        "    print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "  train_dataset = dataset.batch(50)\n",
        "  #model.fit(dataset)\n",
        "  model.fit_generator(train_dataset,epochs=100)\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label))\n",
        "  test_dataset = dataset.batch(50)\n",
        "  return model.evaluate(test_dataset)\n",
        "\n",
        "doLR(df_flat.copy())\n",
        "print(\"End :\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "120GB0T8wq4H",
        "colab_type": "text"
      },
      "source": [
        "def tfData(df):\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label)).batch(50)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label)).batch(50)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "train_dataset, test_dataset = tfData(df_flat.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-F-tPj37a4T",
        "colab_type": "text"
      },
      "source": [
        "def myGenerator(df,batch_size):\n",
        "  for i in range(0, df.shape[0],batch_size):\n",
        "    yield df.image_array.values[i:i+batch_size], df.label.values[i:i+batchsize]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAjr-uFWOGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "8d85edaa-99c0-457d-c08f-5992bf4c97d7"
      },
      "source": [
        "def doNN():\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  try:\n",
        "    model.reset_states()\n",
        "  except:\n",
        "    print(\"skipping model reset\")\n",
        "    pass\n",
        "  \n",
        "  activation_function = \"relu\"\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(2048, input_shape=[512*512], activation=activation_function) \n",
        "            #,tf.keras.layers.Dropout(0.5)\n",
        "            #,tf.keras.layers.Dense(2048, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(1024, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(512, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(128, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.5)\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              #,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "              ,metrics=[\"accuracy\"])\n",
        "  print(model.summary())\n",
        "\n",
        "  batch_size=32\n",
        "  model.fit_generator(myTrainGenerator(batch_size)\n",
        "          ,epochs=100\n",
        "          ,steps_per_epoch=len(os.listdir(\"training_data/npz\"))\n",
        "          ,validation_data=myValidateGenerator(batch_size)\n",
        "          ,validation_steps=len(os.listdir(\"validation_data/npz/\"))\n",
        "          )\n",
        "  result =  model.evaluate(myTestGenerator(batch_size))\n",
        "  model.reset_states()\n",
        "  return result\n",
        "\n",
        "doNN()\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-602e7546170a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdoNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-602e7546170a>\u001b[0m in \u001b[0;36mdoNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdoNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJw1KmYsH_Pj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "def doNN(df):\n",
        "  #tf.reset_default_graph()\n",
        "  #df['label'] = pd.Categorical(df['label'])\n",
        "  #df['label'] = df.label.astype(np.int)\n",
        "  print(\"Start :\", datetime.datetime.now())\n",
        "  df_test = df.sample(frac=0.2)\n",
        "  df_train = df.drop(df_test.index)\n",
        "  \n",
        "  activation_function = \"softsign\"\n",
        "  model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4098, input_shape=[262144], activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(2048, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(1024, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(512, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(128, activation=activation_function) \n",
        "            ,tf.keras.layers.Dropout(0.1)\n",
        "            ,tf.keras.layers.Dense(3, activation=\"softmax\") \n",
        "            ])\n",
        "  model.compile(loss=\"categorical_crossentropy\"\n",
        "              ,optimizer= \"adam\"\n",
        "              ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "  print(model.summary())\n",
        "\n",
        "\n",
        "  #dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array, df_train.label))\n",
        "  \n",
        "  #for feat, targ in train_dataset.take(5):\n",
        "  #  print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "  #train_dataset = dataset.batch(50)\n",
        "  #model.fit(dataset)\n",
        "  model.fit_generator(train_dataset, epochs=5)\n",
        "  \n",
        "  #dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array, df_test.label))\n",
        "  #test_dataset = dataset.batch(50)\n",
        "  return model.evaluate(test_dataset)\n",
        "\n",
        "doNN(df_flat.copy())\n",
        "print(\"End :\", datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmFSRH0IVPc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myPca:\n",
        "  def doPCANN(df):\n",
        "    print(\"Start :\", datetime.datetime.now())\n",
        "    #df['label'] = pd.Categorical(df['label'])\n",
        "    #df['label'] = df.label.astype(np.int)\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.Session().reset()\n",
        "\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"image_array_pca\"] = df.image_array.apply(lambda x: scaler.fit_transform(pca.transform(x.reshape(1, -1))[0].reshape(1, -1))[0])\n",
        "    print(\"Post PCA :\", datetime.datetime.now())\n",
        "    df_test = df.sample(frac=0.2)\n",
        "    df_train = df.drop(df_test.index)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "              #tf.keras.layers.Dense(4098, input_shape=[1024], activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(2048, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(1024, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(512, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(128, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              tf.keras.layers.Dense(3, input_shape=[1024], activation=\"softmax\") \n",
        "              ])\n",
        "    model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array_pca, df_train.label))\n",
        "    \n",
        "    for feat, targ in dataset.take(5):\n",
        "      print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "    train_dataset = dataset.batch(50)\n",
        "    #model.fit(dataset)\n",
        "    model.fit_generator(train_dataset,epochs=5)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array_pca, df_test.label))\n",
        "    test_dataset = dataset.batch(50)\n",
        "    return model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Result :\",myPca.doPCANN(df_flat.copy()))\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjzPG58tUsC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doPCA(df):\n",
        "  import numpy as np\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components=2,whiten=True)\n",
        "\n",
        "  print(np.array(df.image_array.to_list()).shape)\n",
        "  pca.fit(df.image_array.to_list())\n",
        "  #pca.transform(image[0].reshape((1,-1)))\n",
        "  #return [pca.transform(image) for image in df.image_array.to_list() if image.shape[0] == 262144]\n",
        "  df['x'] = df.image_array.apply(lambda x: pca.transform(x.reshape(1, -1))[0][0] if x.shape[0]== 262144 else 0)\n",
        "  df['y'] = df.image_array.apply(lambda x: pca.transform(x.reshape(1, -1))[0][1] if x.shape[0]== 262144 else 0)\n",
        "\n",
        "  return df\n",
        "\n",
        "x_y = doPCA(df_flat.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL06EyYxKWab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_y.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qy4t7upMSQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_y['label'] = x_y.label.astype(np.int)\n",
        "x_y.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce37jsZKvMFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotp(x_y):\n",
        "  plt.scatter(x_y['x'],x_y['y'],c=x_y['label'])\n",
        "  plt.show()\n",
        "\n",
        "plotp(x_y[x_y.label == 1])\n",
        "plotp(x_y[x_y.label == 2])\n",
        "plotp(x_y[x_y.label == 3])\n",
        "plotp(x_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1YBCCWqv3l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(plt.imread(\"training_data/1/1.jpg\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk3PzIHrwjJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_image = plt.imread(\"training_data/1/1.jpg\").copy()\n",
        "ma = temp_image.max()*.8\n",
        "print(ma)\n",
        "temp_image[temp_image > ma]= 0\n",
        "plt.imshow(temp_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mytrMAChxR6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doCluster(df):\n",
        "  from sklearn import cluster\n",
        "  k_means = cluster.KMeans(n_clusters=2, n_init=4)\n",
        "  k_means.fit(df.image_array.to_list())\n",
        "\n",
        "  return df\n",
        "\n",
        "tt = doCluster(df_flat.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35CZ3PUb0nGZ",
        "colab_type": "text"
      },
      "source": [
        "##### Incremental PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNa27eyz0pi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def returnImageLabel(file_list):\n",
        "  image_list=[]\n",
        "  label_transform = [[],np.array([0,0,1]),np.array([0,1,0]),np.array([1,0,0])]\n",
        "  for file_name in file_list:\n",
        "    \n",
        "    with h5py.File(\"download/mat/\"+file_name,'r') as f:\n",
        "          image_array = np.array(f['cjdata']['image'],dtype=np.float128)\n",
        "          image_array = image_array/image_array.max()\n",
        "          label = np.array(f['cjdata']['label'], dtype=np.int)[0][0]\n",
        "          if image_array.shape[0] == 512:\n",
        "            image_list.append((list(image_array.reshape(-1)),label_transform[label]))\n",
        "          else:\n",
        "            image_array = np.pad(image_array,(512 - image_array.shape[0])//2,'constant',constant_values=0)\n",
        "            image_list.append((list(image_array.reshape(-1)),label_transform[label])) \n",
        "  return np.array(image_list)\n",
        "\n",
        "def myGenerator(batch_size):\n",
        "  files = os.listdir(\"download/mat\")\n",
        "  for i in itertools.cycle(range(0,len(files),batch_size)):\n",
        "    yield returnImageLabel(files[i:i+batch_size])\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0q7WmR80yb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myPca:\n",
        "  def doPCANN():\n",
        "    print(\"Start :\", datetime.datetime.now())\n",
        "    #df['label'] = pd.Categorical(df['label'])\n",
        "    #df['label'] = df.label.astype(np.int)\n",
        "    #tf.reset_default_graph()\n",
        "    #tf.Session().reset()\n",
        "\n",
        "    from sklearn.decomposition import PCA, IncrementalPCA\n",
        "    pca = PCA(n_components=1024,whiten=True)\n",
        "    #pca = IncrementalPCA(n_components=1024, batch_size=10)\n",
        "    pca.fit(df.sample(1025).image_array.to_list())\n",
        "    scaler = MinMaxScaler()\n",
        "    df[\"image_array_pca\"] = df.image_array.apply(lambda x: scaler.fit_transform(pca.transform(x.reshape(1, -1))[0].reshape(1, -1))[0])\n",
        "    print(\"Post PCA :\", datetime.datetime.now())\n",
        "    df_test = df.sample(frac=0.2)\n",
        "    df_train = df.drop(df_test.index)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "              #tf.keras.layers.Dense(4098, input_shape=[1024], activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(2048, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(1024, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(512, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              #,tf.keras.layers.Dense(128, activation=\"relu\") \n",
        "              #,tf.keras.layers.Dropout(0.5)\n",
        "              tf.keras.layers.Dense(3, input_shape=[1024], activation=\"softmax\") \n",
        "              ])\n",
        "    model.compile(loss=\"categorical_crossentropy\"\n",
        "                ,optimizer= \"adam\"\n",
        "                ,metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_train.image_array_pca, df_train.label))\n",
        "    \n",
        "    for feat, targ in dataset.take(5):\n",
        "      print ('Features: {}, Target: {}'.format(feat, targ))\n",
        "\n",
        "    train_dataset = dataset.batch(50)\n",
        "    #model.fit(dataset)\n",
        "    model.fit_generator(train_dataset,epochs=5)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df_test.image_array_pca, df_test.label))\n",
        "    test_dataset = dataset.batch(50)\n",
        "    return model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Result :\",myPca.doPCANN())\n",
        "print(\"End :\", datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}